{"cells":[{"cell_type":"markdown","metadata":{"id":"pSy-sfxOsclS"},"source":["# CS 447 Homework 2 $-$ Word Embeddings \\& Text Classification with Neural Networks\n","In this homework, you will first train word embeddings using the continuous-bag-of-words (CBOW) method. Then, you will build a convolutional neural network (CNN) classifier to detect the sentiment of movie reviews using the IMDb movie reviews dataset.\n","\n","In addition to the Pytorch tutorial we have provided online, we highly recommend that you take a look at the PyTorch tutorials before starting this assignment:\n","<ul>\n","<li><a href=\"https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\">https://pytorch.org/tutorials/beginner/pytorch_with_examples.html</a>\n","<li><a href=\"https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\">https://pytorch.org/tutorials/beginner/data_loading_tutorial.html</a>\n","<li><a href=\"https://github.com/yunjey/pytorch-tutorial\">https://github.com/yunjey/pytorch-tutorial</a>\n","</ul>\n","\n","<font color='green'><b>Hint:</b> While you work, we suggest that you keep your hardware accelerator set to \"CPU\" (the default for Colab). However, when you have finished debugging and are ready to train your models, you should select \"GPU\" as your runtime type. This will speed up the training of your models. You can find this by going to <TT>Runtime > Change Runtime Type</TT> and select \"GPU\" from the dropdown menu.</font>\n","\n","We have imported all the libraries you need to do this homework. <b>You should not import any extra libraries. Furthermore, you should not write any code outside of TODO sections.</b> If you do, the autograder will fail to run your code."]},{"cell_type":"markdown","source":["# Part 1: Continuous-Bag-of-Words (CBOW) Embeddings [50 points]\n","\n","In the first part of this assignment you will learn dense word embeddings based on the word2vec paradigm. In particular, you will use the continuous-bag-of-words approach, which trains a model to predict a word based on the embeddings of surrounding words. For example, in the sentence \"the man walks the dog in the park\", the embeddings for the words (\"man, \"walks\", \"dog\", \"in\") will be used to predict the word \"the\" (if your context size is 2 on each side of the target word)."],"metadata":{"id":"lCBhL7pR9E3N"}},{"cell_type":"markdown","source":["## Download \\& Preprocess the Data\n","First we will download the dataset using [torchtext](https://torchtext.readthedocs.io/en/latest/index.html), which is a package that supports NLP for PyTorch.\n","\n","Unfortunately, you have to install the <TT>torchdata</TT> package on the Colab machine in order to access the data. To do this, run the cell below (you may need to click the \"Restart Runtime\" button when it finishes). You will have to do this every time you return to work on the homework."],"metadata":{"id":"4hw9Zahp38Iv"}},{"cell_type":"code","source":["!pip install torchdata==0.5.1"],"metadata":{"id":"jvIRroBT9Hgf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678657119175,"user_tz":300,"elapsed":13255,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"39fc067c-9948-40d4-a109-f20523ec0dbc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchdata==0.5.1\n","  Downloading torchdata-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.9/dist-packages (from torchdata==0.5.1) (1.13.1+cu116)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchdata==0.5.1) (2.25.1)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.9/dist-packages (from torchdata==0.5.1) (1.26.14)\n","Collecting portalocker>=2.0.0\n","  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->torchdata==0.5.1) (4.5.0)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata==0.5.1) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata==0.5.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata==0.5.1) (2022.12.7)\n","Installing collected packages: portalocker, torchdata\n","Successfully installed portalocker-2.7.0 torchdata-0.5.1\n"]}]},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","if __name__=='__main__':\n","    print('Using device:', DEVICE)"],"metadata":{"id":"Q8Ak_A674YAT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678657122821,"user_tz":300,"elapsed":3655,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"53ebbedb-bb01-4efa-d421-925bb306530d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"markdown","source":["Now, we download the data. As with Homework 1, we will use WikiText-2, a corpus of high-quality Wikipedia articles. The dataset was originally introduced in the following paper: https://arxiv.org/pdf/1609.07843v1.pdf. A raw version of the data can easily be viewed here: https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2.preprocess \n","\n","After downloading the data, we preprocess the text as in Homework 1. <i>You do not need to edit this code.</i>\n","\n","* <b>Sentence splitting:</b>&nbsp;&nbsp;&nbsp;&nbsp;In this homework, we are interested in modeling individual sentences, rather than longer chunks of text such as paragraphs or documents. The WikiTest dataset provides paragraphs; thus, we provide a simple method to identify individual sentences by splitting paragraphs at punctuation tokens (\".\",  \"!\",  \"?\").\n","\n","* <b>Sentence markers:</b>&nbsp;&nbsp;&nbsp;&nbsp;For both training and testing corpora, each sentence must be surrounded by a start-of-sentence (`<s>`) and end-of-sentence marker (`/s`). These markers will allow your models to generate sentences that have realistic beginnings and endings.\n","\n","* <b>Unknown words:</b>&nbsp;&nbsp;&nbsp;&nbsp;In order to deal with unknown words, all words that do not appear in the vocabulary must be replaced with a special token for unknown words (`<UNK>`). The WikiText dataset has already done this, and you can read about the method in the paper above. When unknown words are encountered in the test corpus, they should be treated as that special token instead.\n","\n","We provide you with preprocessing code here, and you should not modify it."],"metadata":{"id":"nLbs3wht5weP"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","# Constants (feel free to use these in your code, but do not change them)\n","CBOW_START = \"<s>\"   # Start-of-sentence token\n","CBOW_END = \"</s>\"    # End-of-sentence-token\n","CBOW_UNK = \"<UNK>\"   # Unknown word token"],"metadata":{"id":"Lbkr6F2N9lvd","executionInfo":{"status":"ok","timestamp":1678657125085,"user_tz":300,"elapsed":6,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","import torchtext\n","import random\n","import sys\n","\n","def cbow_preprocess(data, vocab=None, do_lowercase=True):\n","    final_data = []\n","    lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n","    for paragraph in data:\n","        paragraph = [x if x != '<unk>' else CBOW_UNK for x in paragraph.split()]\n","        if vocab is not None:\n","            paragraph = [x if x in vocab else CBOW_UNK for x in paragraph]\n","        if paragraph == [] or paragraph.count('=') >= 2: continue\n","        sen = []\n","        prev_punct, prev_quot = False, False\n","        for word in paragraph:\n","            if prev_quot:\n","                if word[0] not in lowercase:\n","                    final_data.append(sen)\n","                    sen = []\n","                    prev_punct, prev_quot = False, False\n","            if prev_punct:\n","                if word == '\"':\n","                    prev_punct, prev_quot = False, True\n","                else:\n","                    if word[0] not in lowercase:\n","                        final_data.append(sen)\n","                        sen = []\n","                        prev_punct, prev_quot = False, False\n","            if word in {'.', '?', '!'}: prev_punct = True\n","            sen += [word]\n","        if sen[-1] not in {'.', '?', '!', '\"'}: continue # Prevent a lot of short sentences\n","        final_data.append(sen)\n","    vocab_was_none = vocab is None\n","    if vocab is None:\n","        vocab = {}\n","    for i in range(len(final_data)):\n","        # Make words lowercase for this assignment\n","        final_data[i] = [x.lower() if do_lowercase and x != CBOW_UNK else x for x in final_data[i]]\n","        final_data[i] = [CBOW_START] + final_data[i] + [CBOW_END]\n","        if vocab_was_none:\n","            for word in final_data[i]:\n","                vocab[word] = vocab.get(word, 0) + 1\n","    return final_data, vocab\n","\n","def getDataset():\n","    dataset = torchtext.datasets.WikiText2(root='.data', split=('train',))\n","    train_dataset, vocab = cbow_preprocess(dataset[0])\n","    return train_dataset, vocab\n","\n","if __name__=='__main__':\n","    sentences, vocab = getDataset()"],"metadata":{"id":"rnSyKLij9ocH","executionInfo":{"status":"ok","timestamp":1678657132138,"user_tz":300,"elapsed":6267,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Run the next cell to see 10 random sentences of the data."],"metadata":{"id":"sRDXzF956c6n"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    for x in random.sample(sentences, 10):\n","        print (x)"],"metadata":{"id":"HLSERYow7Cbt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678657132140,"user_tz":300,"elapsed":18,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"6caf7377-158b-4422-bfc9-b259032ce09d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['<s>', 'in', '2009', ',', 'after', 'playing', 'at', 'a', 'variety', 'of', 'festivals', 'including', 'glastonbury', 'in', 'the', 'summer', ',', 'she', 'ranked', 'in', 'second', 'place', 'in', 'the', 'bbc', \"'s\", 'sound', 'of', '2010', 'and', 'was', 'one', 'of', 'the', 'three', 'nominees', 'for', 'the', 'critics', \"'\", 'choice', 'award', 'at', 'the', '2010', 'brit', 'awards', '.', '</s>']\n","['<s>', 'for', 'example', ',', 'the', '2010', 'general', 'election', 'was', 'held', 'five', 'years', 'and', 'one', 'day', 'after', 'the', '2005', 'general', 'election', ',', 'whilst', 'the', '1992', 'general', 'election', 'was', 'held', 'on', '9', 'april', '1992', 'and', 'the', 'next', 'general', 'election', 'was', 'not', 'held', 'until', '1', 'may', '1997', '.', '</s>']\n","['<s>', 'the', 'third', 'storyline', 'is', 'concerned', 'with', 'the', 'thoughts', 'and', 'motives', 'of', 'rafael', '<UNK>', 'trujillo', 'molina', 'himself', '.', '</s>']\n","['<s>', '<UNK>', 'the', 'germans', 'were', 'the', 'old', 'russian', 'pre', '@-@', 'dreadnoughts', '<UNK>', 'and', '<UNK>', ',', 'the', 'armored', 'cruisers', 'bayan', ',', 'admiral', '<UNK>', ',', 'and', 'diana', ',', '26', 'destroyers', ',', 'and', 'several', 'torpedo', 'boats', 'and', 'gunboats', '.', '</s>']\n","['<s>', 'dylan', 'co', '@-@', 'founded', 'the', 'band', 'with', 'george', 'harrison', ',', 'jeff', '<UNK>', ',', 'roy', '<UNK>', ',', 'and', 'tom', 'petty', ',', 'and', 'in', 'late', '1988', 'their', 'multi', '@-@', 'platinum', 'traveling', 'wilburys', 'vol', '.', '</s>']\n","['<s>', '\"', 'i', 'am', 'unicorn', '\"', 'was', 'first', 'broadcast', 'on', 'september', '27', ',', '2011', 'in', 'the', 'united', 'states', 'on', 'fox', '.', '</s>']\n","['<s>', 'speaking', 'with', '<UNK>', ',', 'a', 'security', 'strategist', 'for', 'top', '<UNK>', 'networks', ',', 'ken', '<UNK>', 'said', 'that', 'he', 'thought', 'that', '<UNK>', 'were', 'involved', 'in', 'the', 'anonymous', 'operation', ':', '\"', 'there', 'are', 'circles', 'out', 'there', 'where', 'you', 'could', 'take', 'ownership', 'of', 'the', '<UNK>', 'machines', 'that', 'are', 'already', 'owned', 'and', 'launch', 'a', 'simultaneous', 'attack', 'against', '[', 'something', ']', 'like', 'the', 'church', 'from', '50', '@,@', '000', '<UNK>', ',', 'all', 'at', 'the', 'same', 'time', '\"', '.', '</s>']\n","['<s>', 'the', 'lesson', 'is', 'never', 'try', '\"', ',', 'was', 'added', 'to', 'the', 'oxford', 'dictionary', 'of', '<UNK>', 'in', 'august', '2007', '.', '</s>']\n","['<s>', 'by', 'early', 'on', 'august', '8', ',', 'the', 'storm', 'was', 'upgraded', 'to', 'a', 'hurricane', '.', '</s>']\n","['<s>', 'under', 'the', 'name', 'mogadishu', 'port', '<UNK>', 'terminal', ',', 'the', 'firm', 'is', 'slated', 'to', 'handle', 'all', 'of', 'the', 'port', \"'s\", 'technical', 'and', 'operational', 'functions', '.', '</s>']\n"]}]},{"cell_type":"markdown","source":["##<font color='red'>TODO:</font> Define the Dataset Class [15 points]\n","In the following cell, we will define the <b>dataset</b> class. The dataset contains input-output pairs for each training example we will provide to the model. You need to implement the following functions: \n","\n","*   <b>` make_training_examples(self)`:</b>  <b>[5 points]</b> Each training example will be a list of <em>context</em> words along with a <em>target</em> word. The context words consist of $c$ words on either side of the target word; hence, each list of context words has size $2c$. The goal will be to have your model predict the target word from the context words. Thus, you must convert each sentence into a series of context-target pairs, as follows:\n","<ul>\n","<li>For each sentence $s=[w_1,w_2,...,w_n]$ and a context size $c$, compute the following (context, target) pairs:<br>&emsp;&emsp;&emsp;&emsp;$([w_1,...,w_c,w_{c+2},...,w_{2c+1}]$, $w_{c+1}$)<br>&emsp;&emsp;&emsp;&emsp;$([w_2,...,w_{c+1},w_{c+3},...,w_{2c+2}]$, $w_{c+2}$)<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$\\vdots$<br>&emsp;&emsp;&emsp;&emsp;$([w_{n-2c},...,w_{n-c-1},w_{n-c+1},...,w_{n}]$, $w_{n-c}$)<br>For example, suppose your sentence is \"the man walks the dog in the park\" and the context size is $c=2$. Your method should find the following training pairs:<br>&emsp;&emsp;&emsp;&emsp;([\"the\", \"man\", \"the\", \"dog\"], \"walks\")<br>&emsp;&emsp;&emsp;&emsp;([\"man\", \"walks\", \"dog\", \"in\"], \"the\")<br>&emsp;&emsp;&emsp;&emsp;([\"walks\", \"the\", \"in\", \"the\"], \"dog\")<br>&emsp;&emsp;&emsp;&emsp;([\"the\", \"dog\", \"the\", \"park\"], \"in\")<br>Of course, the sentences in your dataset have start- and end-of-sentence tokens as well, which you should treat as any other word.\n","</ul>\n","This function should return a list of <b>all</b> such training pairs.\n","\n","*   <b>` build_dictionaries(self, vocab)`:</b>  <b>[4 points]</b> Creates the dictionaries `word2idx` and `idx2word`. You will represent each word in the vocabulary with a unique index, and keep track of this in these dictionaries. The input `vocab` is a list of words: you must assign indexes in the order the words appear in this list.\n","\n","* <b>`get_context_vector(self, idx)`:</b> Returns a vector representing the <em>context</em> of the `idx`th training example. Specifically, if the context size is $c$, this should be a tensor of $2c$ word indices corresponding to the context words of the `idx`th example. \n","\n","   <font color='green'><b>Hint:</b> You may want to pre-compute and save all context vectors (using word indices rather than the words themselves) in `__init__(...)`, and then access these in `get_context_vector(self, idx)`. This would give you a slight speedup at train time.</font>\n","\n","*   <b>`get_target_index(self, idx) `</b>: Return the target word index for the `idx`th training example.\n","\n","*  <b> ` __len__(self) `: [1 points]</b> Return the total number of training examples in the dataset as an `int`.\n","\n","*   <b>` __getitem__(self, idx)`:</b> <b>[5 points]</b> Return the `idx`th training example as a tuple of `(context_vector, target_word_index)`. You should use the ` get_context_vector(self, idx) ` and ` get_label(self, idx) ` functions here."],"metadata":{"id":"aZwVLt4n7ksc"}},{"cell_type":"code","source":["from torch.utils import data\n","from collections import defaultdict\n","\n","class CbowDataset(data.Dataset):\n","    def __init__(self, sentences, vocab, context_size):\n","        ##### DO NOT EDIT #####\n","\n","        assert CBOW_START in vocab and CBOW_END in vocab and CBOW_UNK in vocab\n","        self.sentences = sentences\n","        self.context_size = context_size\n","\n","        self.training_examples = []\n","        self.make_training_examples()\n","\n","        self.word2idx = {} # Mapping of word to index\n","        self.idx2word = {} # Mapping of index to word\n","        self.build_dictionaries(sorted(vocab.keys()))\n","        self.vocab_size = len(self.word2idx)\n","\n","    def make_training_examples(self):\n","        '''\n","        Builds a list of context-target_word pairs that will be used as training examples for the model and stores them in \n","        self.training_examples.\n","        Each example is a (context, target_word) tuple, where context is a list of strings of size 2*context_size and \n","        target_word is simply a string.\n","        Returns nothing.\n","        '''     \n","\n","        ##### TODO #####\n","        # For each sentence, loop over each word in the sentence. If there are c words before and c words after the word,\n","        # make a (context, word) pair, where context is a list made up of the c words before the word and the c words\n","        # after the word (in the same order they appear in the sentence). Append this (context, word) pair to self.training_examples.\n","        for ii in self.sentences:\n","          sentencesLen = len(ii)\n","          for i in range(self.context_size, sentencesLen - self.context_size):\n","            context = ii[i - self.context_size : i + self.context_size + 1]\n","            context = context[:self.context_size] + context[self.context_size + 1 :]\n","            self.training_examples.append((context, ii[i]))\n","        # pass\n","\n","    \n","    def build_dictionaries(self, vocab): \n","        '''\n","        Builds the dictionaries self.idx2word and self.word2idx. Make sure that you assign indices\n","        in the order the words appear in vocab (a list of words).\n","        Returns nothing.\n","        '''        \n","\n","        ##### TODO #####\n","        temp = []\n","        for i in self.sentences:\n","          for ii in i:\n","            if ii not in temp:\n","              temp.append(ii)\n","        temp.sort()\n","        indices = 0\n","        for i in temp:\n","          self.word2idx[i] = indices\n","          self.idx2word[indices] = i\n","          indices += 1\n","        \n","        # pass\n","    \n","    def get_context_vector(self, idx):\n","        '''\n","        Returns the context vector (as a torch.tensor) for the training example at index idx.\n","        This is is a tensor containing the indices of each word in the context. \n","        '''\n","        assert len(self.training_examples) > 0\n","\n","        ##### TODO #####\n","        context_vector = []\n","        for i in self.training_examples[idx][0]:\n","          context_vector.append(self.word2idx[i])\n","        # return None\n","        return torch.tensor(context_vector)\n","    \n","    def get_target_index(self, idx):\n","        '''\n","        Returns the index of the target word (as type int) of the training example at index idx.\n","        '''\n","        ##### TODO #####\n","        # return None\n","        res = self.word2idx[self.training_examples[idx][1]]\n","        return res\n","\n","    def __len__(self):\n","        '''\n","        Returns the number of training examples (as type int) in the dataset\n","        '''\n","        ##### TODO #####\n","\n","        return len(self.training_examples)\n","    \n","    def __getitem__(self, idx):\n","        '''\n","        Returns the context vector (as a torch.tensor) and target index (as type int) of the training example at index idx.\n","        '''\n","        ##### TODO #####\n","\n","        # return None, None\n","        return self.get_context_vector(idx), self.get_target_index(idx)"],"metadata":{"id":"beABjh82XyXA","executionInfo":{"status":"ok","timestamp":1678657132141,"user_tz":300,"elapsed":11,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["##Sanity Check: Dataset Class\n","\n","The code below runs a sanity check for your `CbowDataset` class. The tests are similar to the hidden ones in Gradescope. However, note that passing the sanity check does <b>not</b> guarantee that you will pass the autograder; it is intended to help you debug.\n","\n","You do <b>not</b> need to edit this cell."],"metadata":{"id":"hqmwBVMDkqEQ"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","def sanityCheckCbowDataset():\n","    #\tRead in the sample corpus\n","    test_sents = [['<s>', 'the', 'man', 'walks', 'the', 'dog', 'in', 'the', 'park', '</s>'],\n","            ['<s>', 'i', 'saw', 'the', 'man', 'with', 'the', 'telescope', 'on', 'the', CBOW_UNK, '</s>']]\n","    test_vocab = {'<s>':2, 'the':6, 'man':2, 'walks': 1, 'dog': 1, 'in': 1, 'park': 1, 'i': 1, 'saw': 1, 'with': 1, 'telescope':1, 'on': 1, CBOW_UNK: 1, '</s>': 2}\n","    print(\"Sample dataset:\")\n","    for x in test_sents: print(x)\n","\n","    context_sizes = [1,3,5]\n","\n","    print('\\n--- TEST: training_examples ---')\n","    training_examples_expected = [[(['<s>', 'man'], 'the'), (['the', 'walks'], 'man'), (['man', 'the'], 'walks'), (['walks', 'dog'], 'the'), (['the', 'in'], 'dog'), (['dog', 'the'], 'in'), (['in', 'park'], 'the'), (['the', '</s>'], 'park'), (['<s>', 'saw'], 'i'), (['i', 'the'], 'saw'), (['saw', 'man'], 'the'), (['the', 'with'], 'man'), (['man', 'the'], 'with'), (['with', 'telescope'], 'the'), (['the', 'on'], 'telescope'), (['telescope', 'the'], 'on'), (['on', '<UNK>'], 'the'), (['the', '</s>'], '<UNK>')],\n","                                  [(['<s>', 'the', 'man', 'the', 'dog', 'in'], 'walks'), (['the', 'man', 'walks', 'dog', 'in', 'the'], 'the'), (['man', 'walks', 'the', 'in', 'the', 'park'], 'dog'), (['walks', 'the', 'dog', 'the', 'park', '</s>'], 'in'), (['<s>', 'i', 'saw', 'man', 'with', 'the'], 'the'), (['i', 'saw', 'the', 'with', 'the', 'telescope'], 'man'), (['saw', 'the', 'man', 'the', 'telescope', 'on'], 'with'), (['the', 'man', 'with', 'telescope', 'on', 'the'], 'the'), (['man', 'with', 'the', 'on', 'the', '<UNK>'], 'telescope'), (['with', 'the', 'telescope', 'the', '<UNK>', '</s>'], 'on')],\n","                                  [(['<s>', 'i', 'saw', 'the', 'man', 'the', 'telescope', 'on', 'the', '<UNK>'], 'with'), (['i', 'saw', 'the', 'man', 'with', 'telescope', 'on', 'the', '<UNK>', '</s>'], 'the')]\n","                                 ]\n","    for i in range(len(context_sizes)):\n","        c=context_sizes[i]\n","        test_dataset = CbowDataset(test_sents, test_vocab, c)\n","        has_passed, message = True, ''\n","        training_examples = test_dataset.training_examples\n","        expected = training_examples_expected[i]\n","        if has_passed and len(training_examples) != len(expected):\n","            has_passed, message = False, 'len(training_examples) is incorrect. Expected: ' + str(len(expected)) + '\\tGot: ' + str(len(training_examples))\n","        if has_passed and set([(type(x), len(x)) for x in training_examples]) != {(tuple, 2)}:\n","            has_passed, message = False, 'Each item of training_examples must be a 2-tuple; at least one of your items is not a 2-tuple.'\n","        if has_passed and set([(type(x[0]), type(x[1])) for x in training_examples]) != {(list, str)}:\n","            has_passed, message = False, 'Each item must contain a list of context words and a target word as a string. At least one of your items does not meet this condition.'\n","        if has_passed and sorted(training_examples, key = lambda x: (' '.join(x[0]), x[1])) != sorted(expected, key = lambda x: (' '.join(x[0]), x[1])):\n","            has_passed, message = False, 'training_examples is incorrect (note that the order of the examples does not matter). Expected: '+str(sorted(expected, key = lambda x: (' '.join(x[0]), x[1]))) + '\\tGot: ' + str(sorted(training_examples, key = lambda x: (' '.join(x[0]), x[1])))\n","          \n","        status = 'PASSED' if has_passed else 'FAILED'\n","        print('\\tcontext_size:', c, '\\t'+status, '\\t'+message)\n","\n","    print('\\n--- TEST: idx2word and word2idx dictionaries ---')\n","    expected_word2idx = {'</s>': 0, '<UNK>': 1, '<s>': 2, 'dog': 3, 'i': 4, 'in': 5, 'man': 6, 'on': 7, 'park': 8, 'saw': 9, 'telescope': 10, 'the': 11, 'walks': 12, 'with': 13}\n","    expected_idx2word = {0: '</s>', 1: '<UNK>', 2: '<s>', 3: 'dog', 4: 'i', 5: 'in', 6: 'man', 7: 'on', 8: 'park', 9: 'saw', 10: 'telescope', 11: 'the', 12: 'walks', 13: 'with'}\n","    for i in range(len(context_sizes)):\n","        c=context_sizes[i]\n","        test_dataset = CbowDataset(test_sents, test_vocab, c)\n","        has_passed, message = True, ''\n","        word2idx = test_dataset.word2idx\n","        idx2word = test_dataset.idx2word\n","\n","        has_passed, message = True, ''\n","        if has_passed and (test_dataset.vocab_size != len(test_dataset.word2idx) or test_dataset.vocab_size != len(test_dataset.idx2word)):\n","            has_passed, message = False, 'dataset.vocab_size (' + str(test_dataset.vocab_size) + ') must be the same length as dataset.word2idx (' + str(len(test_dataset.word2idx)) + ') and dataset.idx2word ('+str(len(test_dataset.idx2word)) +').'\n","        if has_passed and (test_dataset.vocab_size != len(expected_word2idx)):\n","            has_passed, message = False, 'Your vocab size is incorrect. Expected: ' + str(len(expected_word2idx)) + '\\tGot: ' + str(test_dataset.vocab_size)\n","        if has_passed and sorted(list(test_dataset.idx2word.keys())) != list(range(0, test_dataset.vocab_size)):\n","            has_passed, message = False, 'dataset.idx2word must have keys ranging from 0 to dataset.vocab_size-1. Keys in your dataset.idx2word: ' + str(sorted(list(test_dataset.idx2word.keys())))\n","        if has_passed and sorted(list(test_dataset.word2idx.keys())) != sorted(list(expected_word2idx.keys())):\n","            has_passed, message = False, 'Your dataset.word2idx has incorrect keys. Expected: ' + str(sorted(list(expected_word2idx.keys()))) + '\\tGot: ' + str(sorted(list(test_dataset.word2idx.keys())))\n","        if has_passed: # Check that word2idx and idx2word are consistent\n","            widx = sorted(list(test_dataset.word2idx.items())) \n","            idxw = sorted(list([(v,k) for k,v in test_dataset.idx2word.items()]))\n","            if not (len(widx) == len(idxw) and all([widx[q] == idxw[q] for q in range(len(widx))])):\n","                has_passed, message = False, 'Your dataset.word2idx and dataset.idx2word are not consistent. dataset.idx2word: ' + str(dataset.idx2word) + '\\tdataset.word2idx: ' + str(dataset.word2idx)\n","        if has_passed and word2idx != expected_word2idx:\n","            has_passed, message = False, 'Your dataset.word2idx is incorrect. Expected: ' + str(expected_word2idx) + '\\tGot: ' + str(word2idx)\n","        if has_passed and idx2word != expected_idx2word:\n","            has_passed, message = False, 'Your dataset.word2idx is incorrect. Expected: ' + str(expected_idx2word) + '\\tGot: ' + str(idx2word)\n","\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        print('\\tcontext_size:', c, '\\t'+status, '\\t'+message)\n","    \n","    print('\\n--- TEST: len(dataset) ---')\n","    correct_lens = [18,10,2]\n","    for i in range(len(context_sizes)):\n","        c=context_sizes[i]\n","        test_dataset = CbowDataset(test_sents, test_vocab, c)\n","        has_passed = len(test_dataset) == correct_lens[i]\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        if has_passed: message = ''\n","        else: message = 'len(dataset) is incorrect. Expected: ' + str(correct_lens[i]) + '\\tGot: ' + str(len(test_dataset))\n","        print('\\tcontext_size:', c, '\\t'+status, '\\t'+message)\n","    \n","    print('\\n--- TEST: __getitem__(self, idx) ---')\n","    for i in range(len(context_sizes)):\n","        c=context_sizes[i]\n","        test_dataset = CbowDataset(test_sents, test_vocab, c)\n","        for j in range(correct_lens[i]):\n","            returned = test_dataset.__getitem__(j)\n","\n","            has_passed, message = True, ''\n","            if has_passed and len(returned) != 2:\n","                has_passed, message = False, 'dataset.__getitem__(idx) must return 2 items. Got ' + str(len(returned)) +' items instead.'\n","            if has_passed and (type(returned[0]) != torch.Tensor or type(returned[1]) != int):\n","                has_passed, message = False, 'The context vector must be a torch.Tensor and the target index must be an int. Got: (' + str(type(returned[0])) + ', ' + str(type(returned[1])) + ')'\n","            if has_passed and (returned[0].shape != torch.randint(0,100,(2*c,)).shape):\n","                has_passed, message = False, 'Shape of first return is incorrect. Expected: ' + str(correct_items[i][0].shape) + '.\\tGot: ' + str(returned[0].shape)\n","\n","            status = 'PASSED' if has_passed else 'FAILED'\n","            print('\\tcontext_size:', c, '\\tidx:',str(j),'\\t' if j<10 else '','\\t',status, '\\t'+message)\n","\n","if __name__ == '__main__':\n","    sanityCheckCbowDataset()"],"metadata":{"id":"qapAnsEMeoAq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678657132743,"user_tz":300,"elapsed":611,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"62a55448-5173-4136-f199-4c62ff0c3038"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample dataset:\n","['<s>', 'the', 'man', 'walks', 'the', 'dog', 'in', 'the', 'park', '</s>']\n","['<s>', 'i', 'saw', 'the', 'man', 'with', 'the', 'telescope', 'on', 'the', '<UNK>', '</s>']\n","\n","--- TEST: training_examples ---\n","\tcontext_size: 1 \tPASSED \t\n","\tcontext_size: 3 \tPASSED \t\n","\tcontext_size: 5 \tPASSED \t\n","\n","--- TEST: idx2word and word2idx dictionaries ---\n","\tcontext_size: 1 \tPASSED \t\n","\tcontext_size: 3 \tPASSED \t\n","\tcontext_size: 5 \tPASSED \t\n","\n","--- TEST: len(dataset) ---\n","\tcontext_size: 1 \tPASSED \t\n","\tcontext_size: 3 \tPASSED \t\n","\tcontext_size: 5 \tPASSED \t\n","\n","--- TEST: __getitem__(self, idx) ---\n","\tcontext_size: 1 \tidx: 0 \t \t PASSED \t\n","\tcontext_size: 1 \tidx: 1 \t \t PASSED \t\n","\tcontext_size: 1 \tidx: 2 \t \t PASSED \t\n","\tcontext_size: 1 \tidx: 3 \t \t PASSED \t\n","\tcontext_size: 1 \tidx: 4 \t \t PASSED \t\n","\tcontext_size: 1 \tidx: 5 \t \t PASSED \t\n","\tcontext_size: 1 \tidx: 6 \t \t PASSED \t\n","\tcontext_size: 1 \tidx: 7 \t \t PASSED \t\n","\tcontext_size: 1 \tidx: 8 \t \t PASSED \t\n","\tcontext_size: 1 \tidx: 9 \t \t PASSED \t\n","\tcontext_size: 1 \tidx: 10  \t PASSED \t\n","\tcontext_size: 1 \tidx: 11  \t PASSED \t\n","\tcontext_size: 1 \tidx: 12  \t PASSED \t\n","\tcontext_size: 1 \tidx: 13  \t PASSED \t\n","\tcontext_size: 1 \tidx: 14  \t PASSED \t\n","\tcontext_size: 1 \tidx: 15  \t PASSED \t\n","\tcontext_size: 1 \tidx: 16  \t PASSED \t\n","\tcontext_size: 1 \tidx: 17  \t PASSED \t\n","\tcontext_size: 3 \tidx: 0 \t \t PASSED \t\n","\tcontext_size: 3 \tidx: 1 \t \t PASSED \t\n","\tcontext_size: 3 \tidx: 2 \t \t PASSED \t\n","\tcontext_size: 3 \tidx: 3 \t \t PASSED \t\n","\tcontext_size: 3 \tidx: 4 \t \t PASSED \t\n","\tcontext_size: 3 \tidx: 5 \t \t PASSED \t\n","\tcontext_size: 3 \tidx: 6 \t \t PASSED \t\n","\tcontext_size: 3 \tidx: 7 \t \t PASSED \t\n","\tcontext_size: 3 \tidx: 8 \t \t PASSED \t\n","\tcontext_size: 3 \tidx: 9 \t \t PASSED \t\n","\tcontext_size: 5 \tidx: 0 \t \t PASSED \t\n","\tcontext_size: 5 \tidx: 1 \t \t PASSED \t\n"]}]},{"cell_type":"markdown","source":["##<font color='red'>TODO:</font> Define the CBOW Model [20 points]\n","\n","Here, you will define a simple feed-forward neural network that takes in a context vector and predicts the word that completes the context. We provide you with the `CbowModel` class, and you just need to fill in parts of the `__init__(...)` and `forward(...)` functions. Each of these functions is worth <b>10 points</b>.\n","\n","We have provided you with instructions and hints in the comments. In particular, pay attention to the desired shapes; you may find it helpful to print the shape of the tensors as you code. It may also help to keep the PyTorch documentation open for the modules & functions you are using, since they describe input and output dimensions."],"metadata":{"id":"EZhEl32wk5EM"}},{"cell_type":"code","source":["class CbowModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, context_size):\n","        '''\n","        vocab_size: Size of the vocabulary\n","        embed_size: Size of your embedding vectors\n","        hidden_size: Size of hidden layer of neural network\n","        context_size: The size of your context window used to generate training examples\n","        '''\n","        super(CbowModel, self).__init__()\n","\n","        self.context_size = context_size\n","\n","        ##### TODO #####\n","        # 1. Create an embedding layer using nn.Embedding, that will take an index in your vocabulary as input\n","        #    (referring to a word) and return a vector of size embed_size (i.e. your embedding vector).\n","        #    Note that providing a word index to nn.Embedding is the same (conceptually) as providing a one-hot\n","        #    vector to nn.Linear (however, nn.Embedding takes sparsity into account, so is more efficient)\n","\n","        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n","\n","        # 2. Create a linear layer that projects your embedding vector to a vector of size hidden_size.\n","        self.linear = nn.Linear(embed_size ,hidden_size)\n","        \n","        # 3. Create an output linear layer, that projects your hidden vector to a vector the size of your vocabulary.\n","        self.output = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, inputs):\n","        '''\n","        inputs: Tensor of size [batch_size, 2*context_size]\n","\n","        Returns output: Tensor of size [batch_size, vocab_size]\n","        '''\n","\n","        ##### TODO #####\n","        # 1. Feed the inputs through your embedding layer to get a tensor of size [batch_size, 2*context size, embed_size]\n","        # print(inputs)\n","        outputs = self.embedding(inputs)\n","        # 2. Average the embedding vectors of each of your context word embeddings (for each example in your batch). \n","        #    Expected size: [batch_size, embed_size]\n","        outputs = torch.mean(outputs, dim=1)\n","        # print(outputs)\n","        # 3. Feed this through your linear layer and then a ReLU activation. Expected size: [batch_size, hidden_size]\n","        outputs = self.linear(outputs)\n","        outputs = F.relu(outputs)\n","        # 4. Feed this through your output layer and return the result. Expected size [batch_size, vocab_size]\n","        #    Do NOT apply a softmax to the final output - this is done in the training method!\n","        outputs = self.output(outputs)\n","\n","\n","        return outputs"],"metadata":{"id":"osmSxlkFBsx0","executionInfo":{"status":"ok","timestamp":1678669198644,"user_tz":300,"elapsed":578,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"execution_count":151,"outputs":[]},{"cell_type":"markdown","source":["## Sanity Check: CBOW Model"],"metadata":{"id":"sLlw_pe2uNyJ"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","count_parameters = lambda model: sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def makeCbowSanityBatch(test_params):\n","    batch_size = test_params['batch_size']\n","    new_test_params = {k:v for k,v in test_params.items() if k != 'batch_size'}\n","    batch = torch.randint(0, new_test_params['vocab_size'], (batch_size,new_test_params['context_size']*2))\n","    return batch, new_test_params\n","\n","def sanityCheckModel(all_test_params, NN, expected_outputs, init_or_forward, make_batch_fxn=None):\n","    print('--- TEST: ' + ('Number of Model Parameters (tests __init__(...))' if init_or_forward=='init' else 'Output shape of forward(...)') + ' ---')\n","    \n","    for tp_idx, (test_params, expected_output) in enumerate(zip(all_test_params, expected_outputs)):       \n","        if init_or_forward == \"forward\":\n","            input, test_params = make_batch_fxn(test_params)\n","\n","        # Construct the student model\n","        tps = {k:v for k, v in test_params.items()}\n","        stu_nn = NN(**tps)\n","\n","        if init_or_forward == \"forward\":\n","            with torch.no_grad(): \n","                stu_out = stu_nn(input)\n","            ref_out_shape = expected_output\n","\n","            has_passed = torch.is_tensor(stu_out)\n","            if not has_passed: msg = 'Output must be a torch.Tensor; received ' + str(type(stu_out))\n","            else: \n","                has_passed = stu_out.shape == ref_out_shape\n","                msg = 'Your Output Shape: ' + str(stu_out.shape)\n","            \n","\n","            status = 'PASSED' if has_passed else 'FAILED'\n","            message = '\\t' + status + \"\\t Init Input: \" + str({k:v for k,v in tps.items()}) + '\\tForward Input Shape: ' + str(input.shape) + '\\tExpected Output Shape: ' + str(ref_out_shape) + '\\t' + msg\n","            print(message)\n","        else:\n","            stu_num_params = count_parameters(stu_nn)\n","            ref_num_params = expected_output\n","            comparison_result = (stu_num_params == ref_num_params)\n","\n","            status = 'PASSED' if comparison_result else 'FAILED'\n","            message = '\\t' + status + \"\\tInput: \" + str({k:v for k,v in test_params.items()}) + ('\\tExpected Num. Params: ' + str(ref_num_params) + '\\tYour Num. Params: '+ str(stu_num_params))\n","            print(message)\n","\n","        del stu_nn\n","\n","\n","if __name__ == '__main__':\n","    # Test init\n","    cbow_init_inputs = [{'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 128, 'context_size': 2}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 128, 'context_size': 4}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 128, 'context_size': 2}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 128, 'context_size': 4}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 128, 'context_size': 2}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 128, 'context_size': 4}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 128, 'context_size': 2}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 128, 'context_size': 4}]\n","    cbow_init_expected_outputs = [3082, 3082, 5834, 5834, 5450, 5450, 10250, 10250, 99112, 99112, 165224, 165224, 133160, 133160, 201320, 201320]\n","\n","    sanityCheckModel(cbow_init_inputs, CbowModel, cbow_init_expected_outputs, \"init\")\n","    print()\n","\n","    # Test forward\n","    cbow_forward_inputs = [{'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2, 'batch_size': 1}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2, 'batch_size': 5}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2, 'batch_size': 500}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4, 'batch_size': 1}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4, 'batch_size': 5}, {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4, 'batch_size': 500}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2, 'batch_size': 1}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2, 'batch_size': 5}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2, 'batch_size': 500}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4, 'batch_size': 1}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4, 'batch_size': 5}, {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4, 'batch_size': 500}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2, 'batch_size': 5}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2, 'batch_size': 500}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4, 'batch_size': 5}, {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4, 'batch_size': 500}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2, 'batch_size': 5}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2, 'batch_size': 500}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4, 'batch_size': 5}, {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4, 'batch_size': 500}]\n","    cbow_forward_expected_outputs = [torch.Size([1, 10]), torch.Size([5, 10]), torch.Size([500, 10]), torch.Size([1, 10]), torch.Size([5, 10]), torch.Size([500, 10]), torch.Size([1, 10]), torch.Size([5, 10]), torch.Size([500, 10]), torch.Size([1, 10]), torch.Size([5, 10]), torch.Size([500, 10]), torch.Size([1, 1000]), torch.Size([5, 1000]), torch.Size([500, 1000]), torch.Size([1, 1000]), torch.Size([5, 1000]), torch.Size([500, 1000]), torch.Size([1, 1000]), torch.Size([5, 1000]), torch.Size([500, 1000]), torch.Size([1, 1000]), torch.Size([5, 1000]), torch.Size([500, 1000])]\n","\n","    sanityCheckModel(cbow_forward_inputs, CbowModel, cbow_forward_expected_outputs, \"forward\", makeCbowSanityBatch)"],"metadata":{"id":"x7IJDXGkuQlz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678669201921,"user_tz":300,"elapsed":271,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"a39d9cf7-9aed-4091-d306-b41df7cff8da"},"execution_count":152,"outputs":[{"output_type":"stream","name":"stdout","text":["--- TEST: Number of Model Parameters (tests __init__(...)) ---\n","\tPASSED\tInput: {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2}\tExpected Num. Params: 3082\tYour Num. Params: 3082\n","\tPASSED\tInput: {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4}\tExpected Num. Params: 3082\tYour Num. Params: 3082\n","\tPASSED\tInput: {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 128, 'context_size': 2}\tExpected Num. Params: 5834\tYour Num. Params: 5834\n","\tPASSED\tInput: {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 128, 'context_size': 4}\tExpected Num. Params: 5834\tYour Num. Params: 5834\n","\tPASSED\tInput: {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2}\tExpected Num. Params: 5450\tYour Num. Params: 5450\n","\tPASSED\tInput: {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4}\tExpected Num. Params: 5450\tYour Num. Params: 5450\n","\tPASSED\tInput: {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 128, 'context_size': 2}\tExpected Num. Params: 10250\tYour Num. Params: 10250\n","\tPASSED\tInput: {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 128, 'context_size': 4}\tExpected Num. Params: 10250\tYour Num. Params: 10250\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2}\tExpected Num. Params: 99112\tYour Num. Params: 99112\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4}\tExpected Num. Params: 99112\tYour Num. Params: 99112\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 128, 'context_size': 2}\tExpected Num. Params: 165224\tYour Num. Params: 165224\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 128, 'context_size': 4}\tExpected Num. Params: 165224\tYour Num. Params: 165224\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2}\tExpected Num. Params: 133160\tYour Num. Params: 133160\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4}\tExpected Num. Params: 133160\tYour Num. Params: 133160\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 128, 'context_size': 2}\tExpected Num. Params: 201320\tYour Num. Params: 201320\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 128, 'context_size': 4}\tExpected Num. Params: 201320\tYour Num. Params: 201320\n","\n","--- TEST: Output shape of forward(...) ---\n","\tPASSED\t Init Input: {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2}\tForward Input Shape: torch.Size([1, 4])\tExpected Output Shape: torch.Size([1, 10])\tYour Output Shape: torch.Size([1, 10])\n","\tPASSED\t Init Input: {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2}\tForward Input Shape: torch.Size([5, 4])\tExpected Output Shape: torch.Size([5, 10])\tYour Output Shape: torch.Size([5, 10])\n","\tPASSED\t Init Input: {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2}\tForward Input Shape: torch.Size([500, 4])\tExpected Output Shape: torch.Size([500, 10])\tYour Output Shape: torch.Size([500, 10])\n","\tPASSED\t Init Input: {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4}\tForward Input Shape: torch.Size([1, 8])\tExpected Output Shape: torch.Size([1, 10])\tYour Output Shape: torch.Size([1, 10])\n","\tPASSED\t Init Input: {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4}\tForward Input Shape: torch.Size([5, 8])\tExpected Output Shape: torch.Size([5, 10])\tYour Output Shape: torch.Size([5, 10])\n","\tPASSED\t Init Input: {'vocab_size': 10, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4}\tForward Input Shape: torch.Size([500, 8])\tExpected Output Shape: torch.Size([500, 10])\tYour Output Shape: torch.Size([500, 10])\n","\tPASSED\t Init Input: {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2}\tForward Input Shape: torch.Size([1, 4])\tExpected Output Shape: torch.Size([1, 10])\tYour Output Shape: torch.Size([1, 10])\n","\tPASSED\t Init Input: {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2}\tForward Input Shape: torch.Size([5, 4])\tExpected Output Shape: torch.Size([5, 10])\tYour Output Shape: torch.Size([5, 10])\n","\tPASSED\t Init Input: {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2}\tForward Input Shape: torch.Size([500, 4])\tExpected Output Shape: torch.Size([500, 10])\tYour Output Shape: torch.Size([500, 10])\n","\tPASSED\t Init Input: {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4}\tForward Input Shape: torch.Size([1, 8])\tExpected Output Shape: torch.Size([1, 10])\tYour Output Shape: torch.Size([1, 10])\n","\tPASSED\t Init Input: {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4}\tForward Input Shape: torch.Size([5, 8])\tExpected Output Shape: torch.Size([5, 10])\tYour Output Shape: torch.Size([5, 10])\n","\tPASSED\t Init Input: {'vocab_size': 10, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4}\tForward Input Shape: torch.Size([500, 8])\tExpected Output Shape: torch.Size([500, 10])\tYour Output Shape: torch.Size([500, 10])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2}\tForward Input Shape: torch.Size([1, 4])\tExpected Output Shape: torch.Size([1, 1000])\tYour Output Shape: torch.Size([1, 1000])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2}\tForward Input Shape: torch.Size([5, 4])\tExpected Output Shape: torch.Size([5, 1000])\tYour Output Shape: torch.Size([5, 1000])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 2}\tForward Input Shape: torch.Size([500, 4])\tExpected Output Shape: torch.Size([500, 1000])\tYour Output Shape: torch.Size([500, 1000])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4}\tForward Input Shape: torch.Size([1, 8])\tExpected Output Shape: torch.Size([1, 1000])\tYour Output Shape: torch.Size([1, 1000])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4}\tForward Input Shape: torch.Size([5, 8])\tExpected Output Shape: torch.Size([5, 1000])\tYour Output Shape: torch.Size([5, 1000])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 32, 'hidden_size': 64, 'context_size': 4}\tForward Input Shape: torch.Size([500, 8])\tExpected Output Shape: torch.Size([500, 1000])\tYour Output Shape: torch.Size([500, 1000])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2}\tForward Input Shape: torch.Size([1, 4])\tExpected Output Shape: torch.Size([1, 1000])\tYour Output Shape: torch.Size([1, 1000])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2}\tForward Input Shape: torch.Size([5, 4])\tExpected Output Shape: torch.Size([5, 1000])\tYour Output Shape: torch.Size([5, 1000])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 2}\tForward Input Shape: torch.Size([500, 4])\tExpected Output Shape: torch.Size([500, 1000])\tYour Output Shape: torch.Size([500, 1000])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4}\tForward Input Shape: torch.Size([1, 8])\tExpected Output Shape: torch.Size([1, 1000])\tYour Output Shape: torch.Size([1, 1000])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4}\tForward Input Shape: torch.Size([5, 8])\tExpected Output Shape: torch.Size([5, 1000])\tYour Output Shape: torch.Size([5, 1000])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 64, 'hidden_size': 64, 'context_size': 4}\tForward Input Shape: torch.Size([500, 8])\tExpected Output Shape: torch.Size([500, 1000])\tYour Output Shape: torch.Size([500, 1000])\n"]}]},{"cell_type":"markdown","source":["##Train the CBOW Model [15 points]\n","\n","Now, we initialize the <b>dataloader</b>. A dataloader is responsible for providing batches of data to your model. Notice how we first instantiate dataset.\n","\n","You do not need to edit this cell."],"metadata":{"id":"QekcRZjslC93"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","BATCH_SIZE = 1000 # You may change the batch size if you'd like\n","CONTEXT_SIZE = 3  # You may change the context size if you'd like\n","\n","if __name__=='__main__':\n","    cbow_dataset = CbowDataset(sentences, vocab, CONTEXT_SIZE)\n","    cbow_dataloader = torch.utils.data.DataLoader(cbow_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True)"],"metadata":{"id":"q17j2AwNZ20J","executionInfo":{"status":"ok","timestamp":1678669316640,"user_tz":300,"elapsed":111643,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"execution_count":153,"outputs":[]},{"cell_type":"markdown","source":["Now we provide you with a function that takes your model and trains it on the data.\n","\n","You do not need to edit this cell. However, you may want to write code to save your model periodically, as Colab connections are not permanent. See the tutorial here if you wish to do this: https://pytorch.org/tutorials/beginner/saving_loading_models.html."],"metadata":{"id":"fVAnU3-L7Md5"}},{"cell_type":"code","source":["### DO NOT EDIT ###\n","\n","from tqdm.notebook import tqdm\n","from torch import optim\n","\n","def train_cbow_model(model, num_epochs, data_loader, optimizer, criterion):\n","    print(\"Training CBOW model....\")\n","    for epoch in range(num_epochs):\n","        epoch_loss, n = 0, 0\n","        for context, target in tqdm(data_loader):\n","            optimizer.zero_grad()\n","            log_probs = model(context.long().to(DEVICE)) # to(torch.float32)\n","            loss = criterion(log_probs, target.to(DEVICE))\n","            loss.backward()\n","            optimizer.step()\n","            n += context.shape[0]\n","            epoch_loss += (loss*context.shape[0])\n","\n","        epoch_loss = epoch_loss/n\n","        print('[TRAIN]\\t Epoch: {:2d}\\t Loss: {:.4f}'.format(epoch+1, epoch_loss))\n","    print('CBOW Model Trained!\\n')"],"metadata":{"id":"9Hl52H5T7MGV","executionInfo":{"status":"ok","timestamp":1678669316642,"user_tz":300,"elapsed":39,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"execution_count":154,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sogetmSb8z2I"},"source":["Now you can instantiate your model. We provide you with some recommended hyperparameters; you should be able to get the desired accuracy with these, but feel free to play around with them."]},{"cell_type":"code","execution_count":155,"metadata":{"id":"jHmJio5W8z2I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678669316643,"user_tz":300,"elapsed":39,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"17496333-b847-4a18-b855-5123d7aba67a"},"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 7,438,415 trainable parameters\n"]}],"source":["if __name__=='__main__':\n","    cbow_model = CbowModel(vocab_size = cbow_dataset.vocab_size, # Don't change this\n","                embed_size = 128, # Feel free to change\n","                hidden_size = 128, # Feel free to change \n","                context_size = CONTEXT_SIZE) # Don't change this (though you may change the value of CONTEXT_SIZE above if you wish)\n","\n","    # Put your model on the device (cuda or cpu)\n","    cbow_model = cbow_model.to(DEVICE)\n","    \n","    print('The model has {:,d} trainable parameters'.format(count_parameters(cbow_model)))"]},{"cell_type":"markdown","source":["Next, we create the **criterion**, which is our loss function: it is a measure of how well the model matches the empirical distribution of the data. We use cross-entropy loss (https://en.wikipedia.org/wiki/Cross_entropy).\n","\n","We also define the **optimizer**, which performs gradient descent. We use the Adam optimizer (https://arxiv.org/pdf/1412.6980.pdf), which has been shown to work well on these types of models."],"metadata":{"id":"GV2sajpX9mN-"}},{"cell_type":"code","execution_count":156,"metadata":{"id":"yTlIN_Nz9nsx","executionInfo":{"status":"ok","timestamp":1678669316645,"user_tz":300,"elapsed":36,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"outputs":[],"source":["import torch.optim as optim\n","\n","if __name__=='__main__':    \n","    LEARNING_RATE = 0.01 # Feel free to try other learning rates\n","\n","    # Define the loss function\n","    criterion = nn.CrossEntropyLoss().to(DEVICE)\n","\n","    # Define the optimizer\n","    optimizer = optim.Adam(cbow_model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"markdown","metadata":{"id":"e-WrKoEW9nsx"},"source":["Finally, we can train the model. If the model is implemented correctly and you're using the GPU, this cell should take around <b>3 minutes</b> (or less). Feel free to change the number of epochs."]},{"cell_type":"code","execution_count":157,"metadata":{"id":"e7BlulY89nsx","colab":{"base_uri":"https://localhost:8080/","height":365,"referenced_widgets":["f79bbb1717ac4b09a100b79ad4763dcf","d4dc93a37f0b4b76a16d1ef638a9031b","cc468caf032d440abf3e0ddadc7523d9","013d1454cf5c4e3787e0496aad0c3dec","9f562683655a46cdb19ab4af8e245541","f382565451294a5ea455988173dae32c","eac22a8b45ba49b1bd2fcadc4cb32774","46418646ceaa48328a70731e34115ccd","55afccadee634fa797536a02d9ea769e","867b1d4881ad4c3993997042e55d3565","5a3f7b2eb1994352bd3312019d668cf1","8894d23282624ca08414f84dd1566ab8","6068c1ab27304ea2832021d78496be08","0352f4bfcb7b4170b9abd07ee4ea53c7","5ed85d85085a48d89d36e3dbca0eaca9","d33979082de24790a4e01ae8f8e278c1","a4593d1435b3459c847f2f7c83401113","ead5b307918e4b589229c356e224ebe0","870eb60a329242ed80110532a85d59f8","2ceebc946ce74b848688d5ca23b443c9","e66e41079d454ebcb4204df03ab6da67","0821400d497f45fd8491b34f5111f973","6fac86ed3326448687b8d9c675761fa7","713777f59490431dbacb33c7ad15965d","73572656342c4c189da89a2a8be9b661","75b98b78f896407b9365234c48a57012","f554e5d42e084fdf99b899af7193f2a8","403fb67e79ac4856b5f002211c0f181f","026ab5def91f4b7c9cba03556cbbf735","bfb1ef291cf84b50acbad15f86e548de","30b33f0d81be4c54b9a4131e37e50489","4134d97d3daf4ce7965c1200a623d794","630959892dbb4162b7f49c7cf791fd0e","acb528e25c2f4fefaf12e341c7214e2a","dc73c52824074772b0ca2c6ec1e61075","40ed271d954e41a5a7c3a89bd49c094a","424f299132ed4380a69abd510c4acd3a","9044f7c481154fbab4e77e76426c4286","48cf07f45b894a02aa4ccbcdba395164","7395cc0c325e43958c02373cad5dc630","876ffcc2c30040719e88a031e5d35566","0f83f106fbbf407cb9f0ea2dda3e0ac2","218105a3a124413684f397eb46d722c6","75d3affeb7014a27871635a0c7adfe94","cbbeca40f08b4ea285c49b78a9fb9cb1","5dd31658026f45db849a1945719be98b","74c545d8e1284026857c8ebeda76f96e","41b0ebac551d4d9d97b8e7981cccbf48","154431ab1ace4ce59763929b09372e27","ea5a060aace746fabc9a5e1e82512dce","8e105495429249fdbf142feb04746667","e3c3b84467b648cfa6653ceac4daf93f","c495eaee5bf144d494c7805d8b9721f7","1177e719d93a47fca8ac581a8b6e6a8b","b21c71a65f61498b8b7c687d748cc3ab","8e813e7b0f5046229b59ec92bd2f9319","82dbd7f076e6476c9a359f2615ccc8ec","3df1bad06eb4402aaf9cac57f0c07222","56ec8bf1bb874f998b7ee641c9e4edcd","6a45e189376d4836ab2604fa303bf632","c3d74309e611462581644363a70460ce","c1267888d3fa439da37ff48b0ae6189c","9f66aeaadfab40d4b1fb3f474c463727","a59c852024a04ddea18fef86acbd7398","55ca83aef37d4f47ab9231af7c4a02df","a8bc0106b7914761b0484b46afb99b7e"]},"executionInfo":{"status":"ok","timestamp":1678669507659,"user_tz":300,"elapsed":191049,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"62a15784-902c-4fa9-8346-460fc6666324"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training CBOW model....\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1684 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f79bbb1717ac4b09a100b79ad4763dcf"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  1\t Loss: 6.8711\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1684 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8894d23282624ca08414f84dd1566ab8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  2\t Loss: 6.3142\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1684 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fac86ed3326448687b8d9c675761fa7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  3\t Loss: 6.0409\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1684 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acb528e25c2f4fefaf12e341c7214e2a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  4\t Loss: 5.8522\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1684 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbbeca40f08b4ea285c49b78a9fb9cb1"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  5\t Loss: 5.7082\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1684 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e813e7b0f5046229b59ec92bd2f9319"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  6\t Loss: 5.6024\n","CBOW Model Trained!\n","\n"]}],"source":["if __name__=='__main__':\n","    N_EPOCHS = 6 # Feel free to change this\n","    \n","    # Train model for N_EPOCHS epochs\n","    train_cbow_model(cbow_model, N_EPOCHS, cbow_dataloader, optimizer, criterion)"]},{"cell_type":"markdown","source":["To get full credit on the word embeddings, you must return the correct vectors when your model is instantiated with a particular random seed and called on the autograder. This is worth <b>15 points</b>."],"metadata":{"id":"xLNHHFgoB7Tb"}},{"cell_type":"markdown","source":["## Visualize Word Embeddings\n","\n","Now that you have a trained model, we can extract the word embeddings and visualize them. The word embeddings are basically the weight matrix of the embedding layer that you defined, as this maps each index of your vocab to a dense vector of size `embed_size`.\n","\n","Since we cannot easily visualize such high-dimensional vectors, we use a process called TSNE (t-distributed stochastic neighbor embedding). This reduces the vectors to a 2-dimensional space so that we can visualize them. For more information on TSNE, see https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding). Note that this method is not deterministic, so running this cell multiple times will give you a different visualization.\n","\n","The cell below will run TSNE and plot the word embeddings corresponding to thed 1,000 most frequent words on a 2-dimensional plot. You are welcome to increase this threshold if you'd like to see the vectors for more words."],"metadata":{"id":"iWea2Lgh-MBX"}},{"cell_type":"code","source":["if __name__=='__main__':    \n","    from sklearn.manifold import TSNE\n","    import numpy as np\n","    import plotly.express as px\n","    import pandas as pd\n","    import warnings\n","    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","    THRESHOLD = 1000\n","    words = [x[0] for x in sorted(vocab.items(), key = lambda x: -x[1])[:THRESHOLD]]\n","    idxes = [cbow_dataset.word2idx[word] for word in words]\n","    vectors = np.array([cbow_model.embedding.weight[i].tolist() for i in idxes])\n","\n","    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, verbose=False)\n","    new_vectors = tsne_model.fit_transform(vectors)\n","\n","    df = pd.DataFrame(data={'x': new_vectors[:,0], 'y': new_vectors[:,1], 'word':words})\n","\n","    fig = px.scatter(df, x='x', y='y', text='word')\n","    fig.update_traces(textposition='top center')\n","    fig.update_layout(height=600, title_text='Word Embedding 2D Visualization')\n","    fig.show()"],"metadata":{"id":"A5e4xjOIS3nS","colab":{"base_uri":"https://localhost:8080/","height":617},"executionInfo":{"status":"ok","timestamp":1678657896012,"user_tz":300,"elapsed":20721,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"ce10922b-cd39-4868-f2c4-91aaed3d3bcb"},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"127e7dba-8b4a-4154-ab5e-1ef1271c92d5\" class=\"plotly-graph-div\" style=\"height:600px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"127e7dba-8b4a-4154-ab5e-1ef1271c92d5\")) {                    Plotly.newPlot(                        \"127e7dba-8b4a-4154-ab5e-1ef1271c92d5\",                        [{\"hovertemplate\":\"x=%{x}<br>y=%{y}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[\"the\",\",\",\"<s>\",\"</s>\",\".\",\"of\",\"<UNK>\",\"and\",\"in\",\"to\",\"a\",\"\\\"\",\"was\",\"@-@\",\"on\",\"as\",\"that\",\"'s\",\"for\",\"with\",\"by\",\"is\",\"(\",\")\",\"it\",\"from\",\"at\",\"his\",\"he\",\"were\",\"an\",\"had\",\"which\",\"be\",\"are\",\"this\",\"their\",\"but\",\"first\",\"not\",\"one\",\"they\",\"its\",\";\",\"also\",\"after\",\"her\",\"or\",\"two\",\"have\",\"has\",\"been\",\"\\u2013\",\"@.@\",\":\",\"who\",\"she\",\"'\",\"new\",\"during\",\"@,@\",\"other\",\"when\",\"time\",\"all\",\"into\",\"more\",\"would\",\"1\",\"over\",\"while\",\"game\",\"only\",\"most\",\"i\",\"three\",\"up\",\"about\",\"later\",\"2\",\"between\",\"him\",\"may\",\"song\",\"there\",\"than\",\"some\",\"out\",\"year\",\"season\",\"made\",\"city\",\"such\",\"before\",\"where\",\"used\",\"3\",\"series\",\"them\",\"being\",\"both\",\"000\",\"second\",\"many\",\"these\",\"years\",\"world\",\"however\",\"film\",\"no\",\"album\",\"through\",\"south\",\"north\",\"then\",\"can\",\"part\",\"5\",\"war\",\"several\",\"number\",\"including\",\"against\",\"well\",\"became\",\"known\",\"state\",\"four\",\"/\",\"4\",\"\\u2014\",\"although\",\"united\",\"under\",\"early\",\"century\",\"day\",\"following\",\"began\",\"because\",\"so\",\"work\",\"called\",\"like\",\"music\",\"episode\",\"end\",\"until\",\"found\",\"could\",\"area\",\"said\",\"people\",\"states\",\"since\",\"american\",\"released\",\"each\",\"same\",\"british\",\"around\",\"along\",\"6\",\"team\",\"did\",\"long\",\"m\",\"church\",\"million\",\"five\",\"10\",\"back\",\"national\",\"john\",\"high\",\"0\",\"another\",\"company\",\"%\",\"best\",\"use\",\"if\",\"you\",\"final\",\"large\",\"river\",\"what\",\"september\",\"august\",\"off\",\"km\",\"west\",\"down\",\"due\",\"8\",\"games\",\"life\",\"line\",\"will\",\"name\",\"any\",\"now\",\"june\",\"7\",\"received\",\"much\",\"within\",\"group\",\"home\",\"described\",\"government\",\"six\",\"$\",\"storm\",\"9\",\"played\",\"set\",\"took\",\"october\",\"won\",\"single\",\"east\",\"system\",\"species\",\"family\",\"late\",\"road\",\"july\",\"us\",\"general\",\"]\",\"play\",\"[\",\"league\",\"based\",\"record\",\"according\",\"wrote\",\"times\",\"major\",\"...\",\"third\",\"included\",\"just\",\"man\",\"those\",\"even\",\"show\",\"named\",\"april\",\"very\",\"left\",\"march\",\"england\",\"men\",\"video\",\"main\",\"january\",\"white\",\"small\",\"book\",\"though\",\"school\",\"near\",\"york\",\"club\",\"way\",\"original\",\"last\",\"built\",\"water\",\"division\",\"place\",\"old\",\"20\",\"own\",\"november\",\"15\",\"we\",\"character\",\"december\",\"songs\",\"12\",\"top\",\"do\",\"30\",\"player\",\"death\",\"tropical\",\"form\",\"next\",\"black\",\"make\",\"king\",\"german\",\"still\",\"public\",\"island\",\"led\",\"de\",\"again\",\"moved\",\"role\",\"'t\",\"2009\",\"2010\",\"without\",\"history\",\"often\",\"university\",\"among\",\"further\",\"2008\",\"recorded\",\"considered\",\"military\",\"local\",\"period\",\"star\",\"army\",\"love\",\"side\",\"ii\",\"great\",\"came\",\"continued\",\"house\",\"100\",\"written\",\"power\",\"2007\",\"town\",\"run\",\"held\",\"days\",\"published\",\"story\",\"english\",\"french\",\"forces\",\"support\",\"few\",\"having\",\"force\",\"take\",\"throughout\",\"half\",\"14\",\"16\",\"point\",\"land\",\"25\",\"hurricane\",\"county\",\"become\",\"lost\",\"different\",\"18\",\"ship\",\"former\",\"children\",\"release\",\"despite\",\"11\",\"international\",\"order\",\"title\",\"2011\",\"route\",\"members\",\"light\",\"version\",\"common\",\"similar\",\"produced\",\"2006\",\"gave\",\"short\",\"production\",\"dylan\",\"field\",\"park\",\"once\",\"13\",\"given\",\"!\",\"site\",\"band\",\"live\",\"february\",\"little\",\"southern\",\"television\",\"country\",\"career\",\"total\",\"control\",\"position\",\"northern\",\"track\",\"young\",\"fire\",\"central\",\"making\",\"include\",\"away\",\"writing\",\"never\",\"seven\",\"2012\",\"reported\",\"seen\",\"development\",\"lead\",\"western\",\"good\",\"stated\",\"open\",\"air\",\"eastern\",\"returned\",\"red\",\"tour\",\"age\",\"body\",\"how\",\"established\",\"instead\",\"&\",\"across\",\"attack\",\"highway\",\"god\",\"ships\",\"using\",\"week\",\"match\",\"population\",\"2013\",\"service\",\"reached\",\"less\",\"developed\",\"battalion\",\"noted\",\"head\",\"america\",\"battle\",\"building\",\"ft\",\"eight\",\"thought\",\"rock\",\"result\",\"right\",\"ireland\",\"players\",\"miles\",\"himself\",\"president\",\"brigade\",\"modern\",\"my\",\"london\",\"royal\",\"per\",\"important\",\"father\",\"felt\",\"construction\",\"performed\",\"various\",\"areas\",\"feet\",\"win\",\"full\",\"low\",\"previous\",\"australia\",\"events\",\"died\",\"17\",\"originally\",\"went\",\"too\",\"level\",\"mm\",\"started\",\"upon\",\"project\",\"range\",\"kingdom\",\"others\",\"formed\",\"guitar\",\"u.s.\",\"women\",\"political\",\"style\",\"should\",\"caused\",\"art\",\"eventually\",\"located\",\"23\",\"james\",\"performance\",\"port\",\"human\",\"critics\",\"characters\",\"created\",\"sent\",\"football\",\"able\",\"50\",\"ground\",\"21\",\"works\",\"28\",\"stars\",\"19\",\"region\",\"born\",\"night\",\"director\",\"together\",\"strong\",\"center\",\"24\",\"street\",\"class\",\"\\u2019\",\"court\",\"2004\",\"completed\",\"remained\",\"cup\",\"every\",\"hero\",\"scored\",\"me\",\"announced\",\"novel\",\"2005\",\"australian\",\"award\",\"almost\",\"guns\",\"least\",\"damage\",\"chart\",\"22\",\"see\",\"popular\",\"ten\",\"son\",\"grand\",\"months\",\"s\",\"followed\",\"heavy\",\"behind\",\"appeared\",\"fourth\",\"addition\",\"added\",\"brown\",\"return\",\"present\",\"killed\",\"party\",\"does\",\"wife\",\"success\",\"coast\",\"taken\",\"26\",\"sea\",\"features\",\"playing\",\"re\",\"front\",\"aircraft\",\"action\",\"either\",\"list\",\"europe\",\"towards\",\"served\",\"decided\",\"leading\",\"put\",\"generally\",\"close\",\"elements\",\"design\",\"2003\",\"france\",\"believed\",\"david\",\"sold\",\"mother\",\"2015\",\"fleet\",\"operations\",\"soon\",\"female\",\"magazine\",\"significant\",\"goal\",\"ever\",\"weeks\",\"gold\",\"rather\",\"h\",\"points\",\"move\",\"campaign\",\"outside\",\"records\",\"opened\",\"example\",\"directed\",\"fort\",\"brought\",\"finished\",\"study\",\"carey\",\"non\",\"go\",\"featured\",\"help\",\"infantry\",\"earlier\",\"awards\",\"victory\",\"successful\",\"robert\",\"recording\",\"get\",\"wales\",\"case\",\"william\",\"poem\",\"provided\",\"working\",\"27\",\"manager\",\"law\",\"wanted\",\"village\",\"start\",\"gun\",\"mi\",\"act\",\"member\",\"stage\",\"appearance\",\"particularly\",\"association\",\"mid\",\"council\",\"federer\",\"initially\",\"jordan\",\"yard\",\"opening\",\"championship\",\"european\",\"post\",\"largest\",\"no.\",\"troops\",\"attempt\",\"40\",\"saw\",\"tech\",\"29\",\"roman\",\"far\",\"dam\",\"evidence\",\"possible\",\"atlantic\",\"yards\",\"find\",\"rest\",\"summer\",\"office\",\"above\",\"c\",\"blue\",\"allowed\",\"depression\",\"winds\",\"event\",\"type\",\"george\",\"month\",\"19th\",\"union\",\"florida\",\"hours\",\"designed\",\"whom\",\"nine\",\"special\",\"community\",\"a.\",\"saying\",\"worked\",\"previously\",\"police\",\"b\",\"plan\",\"forced\",\"cross\",\"missouri\",\"remains\",\"2014\",\"500\",\"increased\",\"free\",\"200\",\"praised\",\"review\",\"creek\",\"real\",\"race\",\"come\",\"process\",\"hall\",\"\\u00a3\",\"command\",\"lower\",\"middle\",\"replaced\",\"society\",\"studio\",\"taking\",\"radio\",\"parliament\",\"official\",\"base\",\"parts\",\"relationship\",\"research\",\"commander\",\"ball\",\"highest\",\"michael\",\"o\",\"operation\",\"beginning\",\"station\",\"estimated\",\"era\",\"writer\",\"claimed\",\"hill\",\"navy\",\"bay\",\"involved\",\"stone\",\"average\",\"joined\",\"don\",\"food\",\"units\",\"museum\",\"signed\",\"mph\",\"industry\",\"japanese\",\"placed\",\"social\",\"introduced\",\"met\",\"sometimes\",\"section\",\"oldham\",\"goals\",\"shot\",\"suggested\",\"co\",\"\\u00b0\",\"itself\",\"going\",\"complete\",\"olivier\",\"stories\",\"available\",\"face\",\"scene\",\"thus\",\"give\",\"training\",\"college\",\"date\",\"whose\",\"nearly\",\"paul\",\"today\",\"religious\",\"loss\",\"reviews\",\"approximately\",\"2001\",\"term\",\"business\",\"entire\",\"31\",\"henry\",\"indian\",\"irish\",\"capital\",\"damaged\",\"already\",\"must\",\"friends\",\"horse\",\"native\",\"turned\",\"prior\",\"fifth\",\"appointed\",\"told\",\"sound\",\"past\",\"lines\",\"commercial\",\"mark\",\"mexico\",\"probably\",\"language\",\"india\",\"additional\",\"issue\",\"thomas\",\"students\",\"male\",\"passed\",\"program\",\"forest\",\"winning\",\"shows\",\"especially\",\"size\",\"?\",\"length\",\"change\",\"enough\",\"failed\",\"britain\",\"woman\",\"chinese\",\"earth\",\"limited\",\"names\",\"forward\",\"better\",\"canada\",\"running\",\"ny\",\"ordered\",\"child\",\"spent\",\"mixed\",\"t\",\"arrived\",\"overall\",\"cast\",\"civil\",\"regiment\",\"larger\",\"christian\",\"usually\",\"might\",\"wheeler\",\"structure\",\"captain\",\"department\",\"our\",\"moving\",\"sun\",\"media\",\"remaining\",\"minor\",\"changes\",\"hit\",\"regular\",\"pacific\",\"hand\",\"canadian\",\"lake\",\"required\",\"space\",\"network\",\"wide\",\"decision\",\"future\",\"birds\",\"includes\",\"iii\",\"uk\",\"latter\",\"africa\",\"appear\",\"territory\",\"debut\",\"cathedral\",\"positive\",\"mounted\",\"st.\",\"san\",\"billboard\",\"critical\",\"ended\",\"feature\",\"round\",\"finally\",\"crew\",\"pressure\",\"supported\",\"rachel\",\"00\",\"nature\",\"score\",\"centre\",\"always\",\"notes\",\"films\",\"groups\",\"discovered\",\"jin\",\"smaller\",\"provide\",\"becoming\",\"related\",\"shortly\",\"particular\",\"money\",\"2002\",\"view\",\"defeated\",\"shown\",\"difficult\",\"person\",\"science\",\"temple\",\"musical\",\"big\",\"lack\",\"voice\",\"flight\",\"board\",\"saint\",\"staff\",\"subsequently\",\"mass\",\"upper\",\"nations\",\"material\",\"fact\",\"private\",\"peter\",\"surface\",\"minutes\",\"living\",\"idea\",\"intended\",\"teams\",\"trade\",\"cut\",\"charles\",\"gods\",\"zealand\",\"japan\",\"word\",\"greater\",\"bridge\",\"anti\",\"experience\",\"themselves\",\"room\",\"largely\",\"cm\",\"metres\",\"squadron\",\"2000\",\"problems\",\"education\",\"captured\",\"interest\",\"mostly\",\"yet\",\"mountain\",\"likely\",\"contract\"],\"x\":[1.7676842212677002,-2.303469657897949,-1.584359884262085,-1.869296669960022,-2.7075085639953613,-2.393157482147217,-2.0701398849487305,-1.1963697671890259,-2.4840962886810303,-1.5909316539764404,2.339552402496338,-2.121971607208252,-6.164423942565918,1.0903435945510864,-2.776212453842163,-1.6396642923355103,-1.3781083822250366,1.2584179639816284,-2.2034549713134766,-1.5978055000305176,-1.8144468069076538,-6.099334716796875,-2.496077537536621,-1.6832177639007568,-0.12753301858901978,-2.348641872406006,-2.8437297344207764,1.6849184036254883,0.21528083086013794,-6.341684341430664,2.460566282272339,-8.414353370666504,-0.08589855581521988,4.203345775604248,-6.317987442016602,1.1743099689483643,1.657836675643921,-0.2528815269470215,3.0176620483398438,-0.6963880062103271,2.153221845626831,1.528224229812622,1.6244256496429443,-2.17352557182312,-0.933598518371582,-3.140857219696045,1.5454362630844116,-1.2519556283950806,4.694891452789307,-8.43652057647705,-8.425925254821777,-6.369119644165039,1.6987836360931396,1.6471803188323975,-1.8761885166168213,-0.15634110569953918,0.14750196039676666,-3.057220220565796,-2.370140552520752,-2.539670944213867,1.6018662452697754,-2.3345448970794678,-1.2755712270736694,-1.1288437843322754,2.119429588317871,-2.2060935497283936,-10.511458396911621,-9.237704277038574,8.073272705078125,-2.5112435817718506,-0.8928092122077942,-4.756871223449707,-1.461129069328308,2.7612545490264893,1.810977816581726,4.804673194885254,-5.173336029052734,-2.381056547164917,-1.0715727806091309,7.843042850494385,-2.3473763465881348,0.38617420196533203,-9.193222045898438,-5.145812034606934,1.121665596961975,-1.2182143926620483,2.0742926597595215,-5.636407852172852,4.894748687744141,-2.4315176010131836,-1.5011451244354248,-4.622044563293457,8.366313934326172,-3.182772397994995,-0.7386289834976196,-1.291498064994812,7.8211493492126465,-2.485865592956543,0.6297991871833801,-1.0128408670425415,-0.4977206587791443,9.018580436706543,3.354792594909668,2.6863772869110107,1.111548662185669,8.828449249267578,-4.659261226654053,0.46625807881355286,-4.370560646057129,2.779663324356079,-5.310938835144043,-2.294368267059326,4.4451704025268555,4.4891357421875,-1.5694584846496582,-9.221691131591797,2.381481170654297,7.986210346221924,-9.260404586791992,4.545423984527588,2.0198252201080322,-0.5001790523529053,-2.3377647399902344,8.404154777526855,-5.80081844329834,-0.8457374572753906,-2.491647243499756,4.713835716247559,-4.5245466232299805,7.982260704040527,-2.265676259994507,-0.34666940569877625,5.235785007476807,-1.7895104885101318,5.315160751342773,-7.340808868408203,4.895236015319824,-3.6487462520599365,2.000216007232666,-1.2912492752075195,-0.265293151140213,-2.569896697998047,-3.1814656257629395,-0.9711148142814636,-0.5218465328216553,-5.2832536697387695,2.2722504138946533,-2.348222255706787,-2.36357045173645,-9.291743278503418,-5.517488956451416,-4.6701483726501465,0.8280349373817444,8.045074462890625,-2.3740394115448,-7.174273490905762,-1.5636866092681885,2.916473150253296,-4.444192886352539,-5.582945823669434,-2.5380539894104004,-3.6767287254333496,8.000054359436035,-5.455074310302734,-9.634810447692871,-9.746548652648926,5.108702659606934,-4.6690263748168945,-3.055654525756836,5.1190266609191895,8.3027982711792,-5.270603656768799,-3.576420783996582,0.07332444936037064,-9.136381149291992,7.617175579071045,2.1136207580566406,-6.445752143859863,4.909983158111572,3.244262933731079,0.04950190708041191,-1.0451494455337524,1.7338696718215942,3.1856162548065186,-8.861550331115723,-3.224950075149536,0.005223782267421484,9.118786811828613,9.114287376403809,-5.714996337890625,4.829005718231201,-3.3722455501556396,-5.612267971038818,4.319598197937012,8.269307136535645,6.677367687225342,-0.5552087426185608,-4.053530216217041,-9.226959228515625,-2.533529043197632,2.661802291870117,-0.8441470265388489,9.124337196350098,8.266603469848633,-2.832907199859619,2.7475686073303223,-2.668222188949585,-4.226073265075684,-2.2557952404022217,-0.8265976905822754,-8.488228797912598,5.063362121582031,10.085949897766113,-10.334181785583496,8.034436225891113,-1.6883801221847534,0.412551611661911,-3.134037733078003,9.102460861206055,-3.21026611328125,-3.738485813140869,4.5299811363220215,-7.195262908935547,-1.5034942626953125,-4.451292037963867,5.637596607208252,-8.202722549438477,9.123905181884766,-5.688543796539307,-2.969708204269409,-3.313297748565674,3.8318612575531006,-2.8018622398376465,-6.528023719787598,8.877473831176758,-2.651373863220215,4.280144214630127,-5.081441402435303,8.013103485107422,-6.099170207977295,-2.388434410095215,3.498225688934326,-2.828646659851074,-1.5395740270614624,-1.9456104040145874,2.5354528427124023,-1.0996813774108887,-3.6437087059020996,-2.7221732139587402,9.10806941986084,0.6521168947219849,-0.37293943762779236,9.109569549560547,6.218433856964111,0.7759556770324707,-2.992526054382324,-1.4554815292358398,9.132171630859375,-1.2090256214141846,-8.904339790344238,-4.8752570152282715,-0.2887492775917053,-1.627738356590271,-2.930898666381836,5.647315502166748,-6.26131534576416,-4.860374927520752,2.0623395442962646,2.9958667755126953,-1.553840160369873,-1.9684150218963623,-8.255825996398926,-4.871548175811768,5.971246719360352,8.447344779968262,0.07919918745756149,9.136852264404297,8.439260482788086,1.692718267440796,-3.6760525703430176,9.100261688232422,0.8430610299110413,8.527219772338867,1.955488920211792,1.3353791236877441,8.850680351257324,-5.685950756072998,-0.5497972369194031,1.3590635061264038,-3.4284231662750244,-4.1707444190979,-7.587940692901611,4.734033584594727,-5.007951736450195,-5.349637508392334,-0.9393686056137085,-4.163541316986084,-5.115385055541992,3.81908917427063,-4.476809024810791,-2.078258514404297,1.1744384765625,-4.529758453369141,-5.328821182250977,11.71625804901123,11.686049461364746,-2.943333387374878,0.6106966733932495,-0.6568737626075745,-1.490704894065857,-1.2261155843734741,-1.9100676774978638,11.697063446044922,-1.5361491441726685,-2.613288640975952,-3.494701385498047,-4.052108287811279,-1.319188117980957,0.8480302095413208,-8.941530227661133,-1.543117880821228,-3.735621929168701,3.511892318725586,-7.396989345550537,1.6716021299362183,2.0740649700164795,-3.7553467750549316,8.596529006958008,-1.1192702054977417,3.977809429168701,11.594276428222656,-4.722853660583496,2.7037806510925293,-2.253410816192627,8.745726585388184,-1.8785630464553833,-3.8467702865600586,-3.2710139751434326,-5.159470081329346,-9.456269264221191,-1.580209732055664,4.894418239593506,-8.469500541687012,-8.241950988769531,4.395270347595215,-2.782066583633423,5.106736660003662,8.495061874389648,8.728434562683105,-0.7941169738769531,-6.4884185791015625,8.859721183776855,-9.816699981689453,-2.0787241458892822,-5.949642181396484,-1.9228565692901611,-2.461301565170288,8.947357177734375,-6.935575485229492,-2.2892744541168213,0.701833188533783,-2.1422674655914307,-0.5167816877365112,8.47932243347168,-3.9920055866241455,-5.8661909103393555,-2.138417959213257,11.675233840942383,-3.9117305278778076,1.8958150148391724,-8.27543830871582,-1.737962007522583,-7.576088905334473,3.6006264686584473,-1.623861312866211,11.685528755187988,-3.504817008972168,-9.87049674987793,-0.7838782668113708,-0.4269317090511322,-1.5200964212417603,-1.861035704612732,-1.3151224851608276,8.322184562683105,-2.1421632766723633,-3.0267934799194336,-6.826748847961426,-5.562472343444824,2.737394094467163,9.12693977355957,-1.324964165687561,-4.743393421173096,3.556659698486328,-4.685428142547607,-0.6217541098594666,2.667023181915283,-0.14919790625572205,-4.565369129180908,-4.749387741088867,-3.6466801166534424,-3.0262832641601562,-8.02116584777832,-3.9112541675567627,-1.527685523033142,-0.7974057197570801,-6.010548114776611,-1.6945794820785522,-0.8549922704696655,4.9545159339904785,11.70042896270752,-3.3822760581970215,-1.2943286895751953,-0.36512911319732666,3.806232213973999,-4.690046310424805,-7.246982097625732,-4.942634105682373,7.610414981842041,-4.577818870544434,-4.673952579498291,1.7552552223205566,-7.092005729675293,-2.369852066040039,-0.7905210256576538,-5.88201379776001,-0.838142991065979,-1.831455111503601,0.3365451991558075,0.9067196249961853,-1.7464563846588135,-8.74023151397705,-8.342779159545898,0.7939801812171936,-10.45107650756836,-2.137617826461792,4.914651870727539,-4.173648834228516,-4.703335285186768,11.803201675415039,-2.2949562072753906,-3.0334396362304688,-10.538963317871094,-1.853494644165039,-8.05644416809082,-4.949673652648926,1.1327756643295288,-7.396498680114746,-3.852771520614624,-4.007435321807861,3.677504777908325,4.711341381072998,-4.051847457885742,2.851102828979492,2.482738733291626,4.928526878356934,7.81859016418457,1.1011788845062256,6.810330390930176,0.7134616374969482,2.9778542518615723,-8.225396156311035,-2.6620984077453613,1.8096404075622559,-0.7749800086021423,-6.1762003898620605,-5.59719705581665,-7.544609546661377,-0.8709440231323242,-4.732844829559326,-0.4343445301055908,-1.6948946714401245,3.9693119525909424,3.8487870693206787,6.41116189956665,-3.770080804824829,5.895512104034424,-9.100540161132812,2.343498706817627,6.842706203460693,1.7158221006393433,-5.826809883117676,8.518664360046387,2.567446708679199,1.9835714101791382,0.7996118068695068,-2.6486215591430664,5.872956275939941,0.1865004599094391,-3.3071069717407227,-7.097813606262207,-5.638965129852295,8.079312324523926,1.570812702178955,-1.7632246017456055,-1.401294231414795,-5.83988618850708,0.7371781468391418,-0.44121241569519043,1.175018548965454,-9.230076789855957,3.6196796894073486,-1.5438666343688965,-0.9372760653495789,-2.4915504455566406,8.828777313232422,0.4918779134750366,-1.5755705833435059,-6.979176044464111,1.6846177577972412,1.5485442876815796,0.7870381474494934,-1.4705266952514648,0.8263525366783142,-4.101902961730957,1.075334906578064,8.361717224121094,-2.7201666831970215,8.309483528137207,0.6662546992301941,9.305920600891113,2.2482964992523193,9.072072982788086,-5.578726768493652,-2.441967010498047,-1.469739556312561,1.2629162073135376,-4.137073516845703,-7.974618434906006,-8.0116605758667,8.633347511291504,-8.201319694519043,3.8988518714904785,-4.680680274963379,-10.715357780456543,11.500776290893555,-1.5214651823043823,-6.055395126342773,-9.373032569885254,2.989381790161133,-3.859788179397583,-3.5679149627685547,0.5608790516853333,-4.72778844833374,-4.753173351287842,11.843533515930176,-4.856422424316406,6.356634616851807,-4.012732028961182,7.9521260261535645,-0.5931192636489868,2.8614273071289062,-2.87217116355896,8.75001049041748,4.1062774658203125,-6.847158432006836,5.371145248413086,-1.1103814840316772,-3.262878894805908,8.829361915588379,8.392306327819824,-2.2207868099212646,-0.0651119202375412,-1.2715322971343994,-2.0791754722595215,3.7508037090301514,4.84168004989624,-4.240877628326416,-12.722352027893066,4.211780071258545,5.084161758422852,-0.8279645442962646,-2.9699673652648926,-9.689600944519043,-0.8953304886817932,-1.9762314558029175,-3.7538154125213623,-0.02215573750436306,8.500079154968262,-10.722953796386719,-0.5085470676422119,-3.307539224624634,6.220267295837402,-11.423477172851562,-1.2101240158081055,3.8503992557525635,4.169330596923828,-2.4059131145477295,7.596279144287109,-1.95694100856781,-0.6030829548835754,1.946441888809204,4.4947075843811035,1.2405035495758057,-0.19921840727329254,-5.446298599243164,1.9468830823898315,-0.822138249874115,11.484729766845703,6.8276214599609375,-4.262160778045654,0.5798432230949402,0.3646162450313568,-0.8782735466957092,12.008023262023926,-5.890636444091797,2.7138640880584717,-12.191509246826172,-9.360557556152344,2.0536320209503174,-8.094147682189941,-3.975032329559326,2.97834849357605,8.870870590209961,-2.8280162811279297,-10.61841106414795,2.313875198364258,5.5105414390563965,4.139986038208008,-2.5948948860168457,-2.661778688430786,2.2596938610076904,-1.517990231513977,0.8660569787025452,-1.3586962223052979,-4.342102527618408,1.1518163681030273,0.14209018647670746,-8.795343399047852,0.23001693189144135,6.3145670890808105,4.000472545623779,-2.75777006149292,-4.352302551269531,-11.484902381896973,-4.304385185241699,6.341317653656006,-3.7734179496765137,-6.8065714836120605,-0.21389856934547424,-2.523668050765991,4.552367210388184,6.353391647338867,-4.302056789398193,0.5383485555648804,-5.053538799285889,-0.9558840394020081,8.7257661819458,9.182999610900879,3.007859230041504,6.6376142501831055,2.1543071269989014,-4.857863426208496,2.0819108486175537,7.96019983291626,4.865355014801025,-4.6457061767578125,2.2050840854644775,-2.879777193069458,-3.442025899887085,0.6297718286514282,4.627148628234863,6.594508647918701,-8.945958137512207,0.6285909414291382,-0.5205214619636536,0.18136417865753174,-6.169328689575195,3.4242489337921143,-4.40484619140625,-6.308347702026367,5.66702127456665,3.181030511856079,-6.789857387542725,-9.528407096862793,-5.650934219360352,8.28450870513916,-4.022070407867432,-0.45523789525032043,9.52778148651123,-6.012589931488037,7.4461350440979,-6.681114673614502,-0.859312891960144,4.839720249176025,-4.539159297943115,9.315027236938477,4.52011251449585,2.544079542160034,-7.138871192932129,-7.6223320960998535,-3.6073577404022217,-2.1211488246917725,-3.478607416152954,1.2717567682266235,-10.367329597473145,3.02083158493042,-6.409002304077148,3.2224721908569336,0.41228917241096497,4.924582481384277,4.185982704162598,-12.242134094238281,1.9822152853012085,8.851606369018555,-0.5672282576560974,0.19222760200500488,5.107853412628174,-1.9797836542129517,-5.508571624755859,-10.763876914978027,0.1919766068458557,0.19832712411880493,-0.06748099625110626,-3.6505651473999023,-2.2776548862457275,-4.887368202209473,1.0358498096466064,4.64913272857666,1.5881038904190063,-2.0607683658599854,12.025309562683105,8.96865177154541,0.9036877155303955,-6.658824443817139,8.660423278808594,-3.749882936477661,-8.877508163452148,-9.340269088745117,-3.485374927520752,-8.404428482055664,2.373913288116455,-4.2648606300354,-1.88860023021698,10.136544227600098,-1.4005489349365234,3.0494754314422607,4.7888007164001465,-1.2018046379089355,-9.609373092651367,-11.008445739746094,-2.99945068359375,-2.037902355194092,-8.438236236572266,-3.093228340148926,-4.580877780914307,2.0590898990631104,-2.6578941345214844,-3.478971004486084,3.6881275177001953,-5.778960704803467,3.424921751022339,3.6137847900390625,2.068998098373413,-7.736021995544434,2.307659864425659,-8.445870399475098,-3.597040891647339,4.525319576263428,-10.031868934631348,-4.500532627105713,0.6632564067840576,-8.629348754882812,-7.738515377044678,-4.367879390716553,-2.5217745304107666,5.370466709136963,-1.4901202917099,3.8118538856506348,-3.0844717025756836,1.7303550243377686,-6.656184196472168,-0.9484825134277344,5.705399036407471,-6.659060955047607,-5.539057731628418,-2.153533697128296,-2.606682777404785,-2.161190986633301,-0.3543592691421509,-0.20060138404369354,2.181203842163086,3.0699450969696045,-7.102362155914307,-2.6056997776031494,-5.0005621910095215,5.983564853668213,2.8946449756622314,-1.8901957273483276,0.4465421140193939,-1.7491717338562012,0.7376632690429688,0.2390764206647873,-0.9836865067481995,1.2480038404464722,-3.8878746032714844,-1.4248862266540527,4.759372711181641,-0.8481777310371399,4.51942777633667,-5.163259506225586,0.058579277247190475,-2.8262009620666504,-0.41948390007019043,1.480208396911621,3.833503007888794,-0.9007890224456787,-12.478260040283203,-0.8180179595947266,11.722382545471191,-2.6280176639556885,-2.6696431636810303,-3.751556396484375,9.273414611816406,-1.0669158697128296,-5.6712188720703125,-6.559482097625732,-2.6065220832824707,-1.5576590299606323,-0.3894021809101105,-9.211471557617188,-5.627416610717773,-5.555209159851074,-12.287206649780273,1.142991065979004,4.470135688781738,3.937340497970581,-2.7701690196990967,-4.801505088806152,-5.610447406768799,-5.105650901794434,-0.6548107862472534,-4.261942386627197,-11.440140724182129,3.280750274658203,0.45155516266822815,-7.4505085945129395,-1.1176486015319824,1.3611255884170532,2.354883909225464,1.0037145614624023,0.8948920369148254,-9.947026252746582,-0.05564597249031067,-3.587664842605591,-10.851035118103027,-3.431868553161621,-0.22287867963314056,0.8719640374183655,0.025283150374889374,-2.690918445587158,-0.01795213110744953,-2.507383108139038,0.8810505867004395,2.2855384349823,6.253663539886475,-1.9681401252746582,-4.954132080078125,6.278255462646484,-1.056302547454834,1.6493659019470215,-5.838601112365723,-10.560138702392578,6.928956985473633,-1.5370728969573975,2.1526594161987305,1.0506619215011597,7.0931315422058105,-1.7773778438568115,-13.382908821105957,-4.087061405181885,-5.779794692993164,-3.8820407390594482,-7.0291361808776855,-4.617605209350586,1.6526782512664795,-9.249666213989258,-2.5169641971588135,-0.23637115955352783,-9.213479042053223,0.17852932214736938,-1.3530995845794678,4.021171569824219,-7.553241729736328,1.9106543064117432,1.8745934963226318,-7.142787456512451,-6.1115570068359375,-1.5287785530090332,-6.2740254402160645,-0.2211044728755951,-0.7459304332733154,-1.8421266078948975,-6.616365909576416,4.9069390296936035,-8.784363746643066,-7.083254814147949,0.12178019434213638,-1.4983071088790894,-5.805335521697998,-10.061752319335938,-5.006364345550537,-3.0676732063293457,1.7044652700424194,-5.1006178855896,-3.643907070159912,-5.145075798034668,2.1596925258636475,7.0686259269714355,5.141697883605957,-2.113833427429199,0.056315209716558456,-3.3454220294952393,-13.377532958984375,-9.937932014465332,4.376718521118164,11.07564926147461,-6.911593437194824,-7.0490193367004395,-0.23706674575805664,-3.2482125759124756,-8.0255126953125,-1.2755779027938843,-11.566030502319336,-1.4394423961639404,-2.1217546463012695,0.3341675102710724,10.155686378479004,0.5339418649673462,-2.7438485622406006,-9.828167915344238,-2.109600782394409,-5.97267484664917,0.14124327898025513,2.173452615737915,-2.710230588912964,-5.468534469604492,-9.263792991638184,4.9022536277771,-4.967474937438965,3.780186891555786,-12.242066383361816,0.24863354861736298,2.988360643386841,11.907854080200195,2.5503060817718506,-2.085425615310669,-1.5220489501953125,2.849414110183716,-2.6602160930633545,5.011010646820068,-6.148138046264648,-5.11898136138916,-3.666731357574463,-0.22851471602916718,-2.219360113143921,1.8785696029663086,-3.7796719074249268,7.540298938751221,-6.9758453369140625,-0.8110147714614868,6.426942825317383,3.983328104019165,7.86693811416626,4.679910659790039,-5.914199352264404,-3.7869083881378174,-0.9949315786361694,-2.7689731121063232,8.845516204833984,-2.570777177810669,-2.924426794052124,0.6346679925918579,1.3421770334243774,-5.909834861755371,1.5496764183044434,1.0641018152236938,1.2399591207504272,5.7007036209106445,9.418383598327637,-2.855591058731079,-10.504714012145996,-7.5393757820129395,6.380928039550781,-2.769282102584839,-6.919831275939941,-3.3045098781585693,-0.3462090492248535,6.088798522949219,7.089917182922363,-6.573338031768799,11.20661449432373,1.4677324295043945,-1.467950701713562,-0.8364464640617371,0.3656251132488251,1.3329331874847412,-0.7632710933685303,0.12695206701755524,-7.654107093811035,-9.341930389404297],\"xaxis\":\"x\",\"y\":[0.9548407793045044,-2.258601665496826,-2.564836025238037,-2.139122486114502,-1.174081563949585,-4.640413761138916,1.8002326488494873,-3.361767053604126,-4.891720294952393,-5.860991954803467,1.0061228275299072,0.8976958394050598,-8.218749046325684,-4.689001560211182,-5.315889358520508,-3.7358334064483643,-2.7388672828674316,-0.02319556288421154,-4.538567543029785,-4.394217491149902,-4.250598907470703,-8.229782104492188,-2.931323766708374,0.9352861642837524,2.6998846530914307,-5.023277759552002,-5.7976861000061035,0.7222147583961487,3.120424747467041,-8.146738052368164,0.6574223041534424,-8.015796661376953,-3.3131632804870605,-8.706372261047363,-8.14399528503418,0.7329906821250916,0.7316604852676392,-3.6647286415100098,-2.2086687088012695,-0.9092106819152832,5.447469234466553,3.2003443241119385,0.7140499949455261,-2.9913125038146973,-0.5314826965332031,-4.872270584106445,0.8147600889205933,-3.364614248275757,3.1013143062591553,-7.999606609344482,-8.00816535949707,-9.037593841552734,-5.726605415344238,-5.859544277191162,-3.2955613136291504,-2.9959115982055664,3.0734329223632812,-0.8914840221405029,-0.1288258582353592,-5.023432731628418,-5.892948627471924,0.25088030099868774,-3.2676966190338135,6.062994956970215,2.1624810695648193,-5.323755264282227,-5.510832786560059,-10.929577827453613,4.057819843292236,-4.13416051864624,-3.641484260559082,7.324497699737549,-1.7606332302093506,-3.0978145599365234,3.2072713375091553,3.2194576263427734,-2.4818015098571777,-3.914661169052124,-0.3856021761894226,4.006964206695557,-5.707007884979248,2.528926134109497,-10.9925537109375,6.291438579559326,-1.7807737588882446,-3.9013497829437256,2.1052916049957275,-3.026252031326294,10.683982849121094,7.256596088409424,-9.20932674407959,4.647910118103027,0.9229746460914612,-4.798619747161865,-3.585257053375244,-12.470890998840332,4.088336944580078,7.057419776916504,2.648458957672119,2.7967875003814697,0.8646863102912903,6.677741050720215,-1.738350749015808,2.5119478702545166,1.1091777086257935,-5.493096351623535,2.6239330768585205,-3.9758994579315186,6.686584949493408,0.17701105773448944,6.571948528289795,-4.977790355682373,5.601080417633057,5.592630863189697,0.479682594537735,-10.965202331542969,7.136904239654541,4.047550201416016,-0.22807885706424713,2.9493401050567627,9.550426483154297,-4.225532054901123,-4.429223537445068,0.9352272748947144,-8.399292945861816,-13.509519577026367,3.186944007873535,3.085714817047119,-6.123951435089111,4.062029838562012,-2.5250637531280518,-3.7450315952301025,-1.0307778120040894,-5.017937183380127,1.3672446012496948,9.340154647827148,10.647465705871582,-5.055176734924316,-11.729535102844238,-3.3746252059936523,-2.2790956497192383,5.130049228668213,-10.791584014892578,-3.913325309753418,2.1946051120758057,6.964274883270264,11.4862642288208,-4.753705024719238,-12.479727745056152,-10.874676704406738,4.740091323852539,-12.823023796081543,4.571864604949951,-3.216095209121704,-4.688343524932861,-0.37020617723464966,-12.775092124938965,0.5939908027648926,-5.297022342681885,0.8838678002357483,-4.334808349609375,-4.262730121612549,3.9731478691101074,7.6304450035095215,-10.461028099060059,-3.014875888824463,-5.117486000061035,4.873769283294678,-2.5661542415618896,3.2635769844055176,4.370508670806885,-2.2738513946533203,-0.7447899580001831,1.1164888143539429,-3.041205644607544,4.154348373413086,0.6129938364028931,4.414307117462158,-5.168715476989746,-3.192507266998291,6.881985664367676,-3.0928196907043457,3.1995394229888916,-2.283399820327759,-3.804687738418579,3.7857813835144043,-2.175474166870117,-8.452009201049805,-8.44864273071289,-3.259815216064453,-4.7724103927612305,1.0000407695770264,-3.135878324508667,-11.160686492919922,4.147891521453857,-3.7691798210144043,9.657783508300781,3.765085458755493,-10.912310600280762,10.050821304321289,0.9094883799552917,-0.4060005843639374,-8.466843605041504,4.07418155670166,-9.70650863647461,6.542381286621094,-5.222768783569336,5.5577497482299805,1.6662538051605225,-13.695462226867676,3.8733749389648438,3.1640448570251465,-4.154791355133057,8.324316024780273,3.859869956970215,-10.742537498474121,-7.140064239501953,-7.835461616516113,-8.43217945098877,-9.23510456085205,2.7639319896698,5.4176201820373535,3.578007221221924,6.721315383911133,5.380903720855713,1.449968695640564,1.5417158603668213,-8.462328910827637,1.561490535736084,1.8301807641983032,-2.8142709732055664,-7.745261192321777,-2.0993576049804688,8.638264656066895,-1.8072501420974731,6.100937843322754,-11.427143096923828,-12.81042766571045,-5.33732271194458,-1.5786432027816772,-2.2946248054504395,-1.7576818466186523,-11.073970794677734,-0.4553990066051483,9.019918441772461,2.3196890354156494,0.8605442047119141,5.28052282333374,-11.605170249938965,-8.455080032348633,-2.0138144493103027,-9.668561935424805,-8.447929382324219,0.4320690631866455,4.744880676269531,0.6152734756469727,-2.074641466140747,-8.486989974975586,1.3832998275756836,-3.8086578845977783,6.947531700134277,-3.8575551509857178,4.197482109069824,-5.9551544189453125,7.139284610748291,7.735820770263672,9.497535705566406,-2.113316774368286,-2.3558905124664307,-12.257288932800293,2.7788898944854736,8.82891845703125,9.862335205078125,1.7714592218399048,4.43221378326416,11.83376693725586,-8.484232902526855,4.231219291687012,3.226362466812134,6.941495895385742,-8.452841758728027,5.666530132293701,4.171300411224365,9.586718559265137,-7.352438926696777,4.82094669342041,7.917332649230957,10.175106048583984,-0.4172808825969696,5.991258144378662,-5.270714282989502,-1.2790623903274536,-8.653860092163086,3.067570924758911,1.0729589462280273,-0.598074197769165,2.034069299697876,3.697936534881592,-11.614359855651855,-3.8088080883026123,0.8500166535377502,-10.55312728881836,9.338232040405273,-4.639980792999268,0.2534131705760956,0.27126210927963257,-4.535597801208496,6.899601459503174,0.21699264645576477,4.346165180206299,-4.67294979095459,0.33759209513664246,0.2621667981147766,-11.602664947509766,-12.318868637084961,1.4737428426742554,0.6608487367630005,6.368446350097656,13.107348442077637,3.806523084640503,2.3940048217773438,11.127180099487305,-0.49710434675216675,-2.624624490737915,-10.694802284240723,-11.816024780273438,4.347222328186035,5.7946367263793945,-12.207539558410645,7.542713642120361,0.31239640712738037,4.30693244934082,-7.711435794830322,-11.813414573669434,-5.52406644821167,-12.973553657531738,6.926299571990967,0.1336868852376938,1.1822842359542847,3.496832847595215,3.2644569873809814,3.171126127243042,-7.9646172523498535,-0.8141132593154907,-8.750839233398438,-5.087818145751953,3.8854949474334717,4.1901421546936035,4.218614101409912,5.853015422821045,5.382352828979492,4.424774169921875,7.562236309051514,1.7368676662445068,-8.924080848693848,-8.218875885009766,-0.1019725427031517,4.258345603942871,7.072734355926514,-1.701486587524414,4.657958507537842,7.3580098152160645,-4.944082260131836,4.191744804382324,-0.8992822170257568,10.16279125213623,10.163872718811035,0.22768622636795044,3.8068878650665283,6.0650553703308105,-2.1063737869262695,7.545461177825928,-5.564264297485352,-10.631410598754883,-11.03695297241211,0.2530849277973175,-8.161510467529297,-3.0207438468933105,7.742852210998535,3.332571268081665,1.6166887283325195,1.7625141143798828,0.4285714030265808,4.068118572235107,-11.7808837890625,-1.442192792892456,6.270338535308838,6.139029502868652,-6.819334030151367,-8.473122596740723,-0.8941604495048523,-0.47892308235168457,4.00068473815918,4.972473621368408,9.721662521362305,2.495469093322754,5.11631441116333,9.382238388061523,-0.4291008412837982,8.742865562438965,-0.19144602119922638,11.19952392578125,0.013483590446412563,-8.898207664489746,-6.447037696838379,-3.663504123687744,3.6653621196746826,-0.8794764280319214,3.148228883743286,0.25513163208961487,-13.012978553771973,-13.100022315979004,7.329410076141357,-7.719311714172363,-0.25829315185546875,-2.4223711490631104,-12.884325981140137,8.356256484985352,0.9206965565681458,-0.3305116295814514,-10.77950668334961,-1.4361132383346558,7.174336910247803,6.345853328704834,6.019103050231934,-2.835908889770508,-11.803481101989746,0.1211046352982521,-5.242518901824951,-5.566905975341797,5.499109268188477,1.6089507341384888,9.83547306060791,4.384408950805664,-2.994499444961548,10.75485610961914,7.631719589233398,5.640099048614502,0.23443660140037537,4.270768642425537,-8.839157104492188,-5.531358242034912,-10.794366836547852,8.811638832092285,-12.890223503112793,8.671996116638184,-0.3701033592224121,7.957427024841309,3.369401693344116,-5.29974365234375,2.93367338180542,-12.96772289276123,-0.6012552976608276,7.338443756103516,8.541252136230469,-0.31970682740211487,4.43914794921875,-5.5312089920043945,1.8052890300750732,8.461230278015137,8.839583396911621,-0.7378454804420471,0.5544133186340332,1.7039635181427002,-0.2494954764842987,-5.329418182373047,-5.542665481567383,10.580286026000977,-12.77146053314209,7.643588066101074,-11.244222640991211,2.633833408355713,-0.065055713057518,-5.496298789978027,13.326510429382324,4.462240219116211,-3.068258285522461,-2.328658103942871,-0.23174543678760529,4.720950126647949,-6.439343452453613,4.213067531585693,-1.3619722127914429,-11.20694351196289,-2.147902011871338,3.726177930831909,-5.468095779418945,-9.196015357971191,-5.550631523132324,4.897401809692383,11.47941780090332,-3.2053909301757812,2.694814682006836,-10.87938117980957,0.9016885757446289,1.4829134941101074,4.696326732635498,1.06493079662323,8.693581581115723,-10.953171730041504,-11.882587432861328,2.176133155822754,-0.2401052713394165,-13.024292945861816,4.0179290771484375,0.9242237210273743,8.237340927124023,10.765897750854492,1.4821528196334839,2.558284044265747,5.6504435539245605,-11.385468482971191,-11.060382843017578,-1.0870381593704224,-13.07675552368164,5.17730712890625,2.337162494659424,4.664681434631348,5.830994129180908,4.241310119628906,4.505990505218506,4.282845497131348,4.733954906463623,-13.750609397888184,5.847322940826416,7.648956775665283,-2.877603054046631,-2.9706552028656006,5.633242607116699,3.4580180644989014,1.1904114484786987,9.887819290161133,-6.341618537902832,3.0165467262268066,0.355248361825943,-12.030486106872559,-8.04517650604248,-0.5388039946556091,0.5467373728752136,2.193983554840088,-8.974616050720215,2.2809500694274902,-13.007148742675781,7.322311878204346,0.2598292827606201,0.6281942129135132,-2.5737197399139404,-1.9485135078430176,10.36032485961914,5.750538349151611,4.125844955444336,1.9862909317016602,3.6598775386810303,-7.5224833488464355,-5.433855056762695,3.2886204719543457,10.879155158996582,-3.187498092651367,-5.533292293548584,2.348296880722046,-10.402976989746094,1.7735950946807861,-4.703953742980957,-14.214661598205566,-1.7320870161056519,-11.110033988952637,-12.679767608642578,1.2949481010437012,-9.132303237915039,-2.930339813232422,-11.757174491882324,7.222607612609863,-10.402894020080566,10.722016334533691,5.781713962554932,11.287917137145996,-10.454817771911621,3.747074842453003,1.251821517944336,-5.909972667694092,-1.622556209564209,4.638998508453369,6.931183815002441,5.147909641265869,4.250024318695068,0.5436420440673828,11.794107437133789,-0.4027268588542938,-5.889675617218018,-13.927131652832031,-12.111326217651367,-11.301926612854004,-10.058013916015625,-0.596177339553833,-2.266087055206299,6.1015167236328125,7.077193737030029,0.34157994389533997,-0.1272391974925995,-12.96596622467041,1.0103737115859985,-10.466322898864746,10.663297653198242,0.34461709856987,4.760470867156982,1.6541693210601807,-2.474759340286255,1.8824806213378906,-3.691248655319214,-3.722195625305176,8.813838958740234,3.172451972961426,-5.532802581787109,0.8689699172973633,-5.585319519042969,3.59195876121521,-1.9229724407196045,-8.967300415039062,7.506146430969238,-6.2210307121276855,-0.19029220938682556,-12.336780548095703,-4.0357489585876465,-10.9043607711792,3.2442500591278076,-10.362788200378418,-8.8092679977417,6.639909267425537,3.208420515060425,3.685336112976074,-8.951333999633789,-11.288496971130371,-2.4750115871429443,2.03535532951355,-4.736687660217285,-2.5810706615448,13.32302474975586,-5.255908489227295,1.0966427326202393,6.436834335327148,-8.51954460144043,-0.2837551534175873,7.565381050109863,-0.8902536034584045,6.936484336853027,-11.330541610717773,-1.880687952041626,4.240556716918945,8.342452049255371,6.328821659088135,-12.04195499420166,4.312014102935791,11.440629005432129,10.352326393127441,-5.427065372467041,-1.74737548828125,7.3722310066223145,4.753309726715088,9.742603302001953,-2.8820295333862305,6.078846454620361,2.362926483154297,3.953321695327759,2.5674455165863037,-0.390963613986969,3.637894868850708,-4.4041643142700195,-2.364311933517456,-3.3727641105651855,0.24712078273296356,4.944131851196289,-3.3316519260406494,-10.502970695495605,3.4736311435699463,10.030786514282227,5.283810138702393,-7.582183361053467,2.99960994720459,4.3987650871276855,0.7199674844741821,1.3635270595550537,5.558110237121582,4.588694095611572,-3.373952865600586,-1.1627413034439087,-5.024585723876953,-8.649080276489258,11.492815017700195,1.4157415628433228,5.64669132232666,-6.148761749267578,2.3847663402557373,-0.34295564889907837,-12.354703903198242,8.354180335998535,1.5485655069351196,6.792898654937744,6.790487289428711,0.724314272403717,10.716180801391602,-1.8604954481124878,3.026071071624756,1.264366626739502,-5.560704708099365,-12.535682678222656,-3.3933262825012207,3.031731367111206,-0.6725714802742004,3.645418882369995,-0.3965996205806732,-4.608015060424805,-14.723112106323242,-1.1177513599395752,1.7530916929244995,1.5506761074066162,8.254900932312012,-12.991128921508789,4.9285736083984375,-2.707469940185547,-1.0485007762908936,0.3286728858947754,6.57999324798584,-7.9707722663879395,-2.233252763748169,5.653957843780518,-10.889018058776855,6.745553493499756,10.040437698364258,-1.1484063863754272,0.13921090960502625,-9.281303405761719,6.178061485290527,2.2163472175598145,-4.113308906555176,12.702143669128418,-4.459558486938477,1.6103755235671997,-11.990344047546387,5.024230003356934,-1.491148829460144,-7.6535844802856445,2.104403257369995,4.164678573608398,-0.5147227644920349,10.514861106872559,6.187349319458008,5.464733600616455,2.3575689792633057,5.905447006225586,7.1844282150268555,-3.3052685260772705,1.4165537357330322,-0.6712546944618225,4.825313568115234,11.482007026672363,2.3423616886138916,-13.39587688446045,-4.935125350952148,0.43162864446640015,-12.759114265441895,-0.3575007915496826,3.6509180068969727,0.43425849080085754,-10.221531867980957,1.5678516626358032,1.687351107597351,-10.433137893676758,2.042098045349121,2.57627272605896,5.174406051635742,4.013251781463623,-11.281590461730957,-5.419140815734863,1.7283073663711548,0.7304088473320007,-12.627999305725098,0.9935781955718994,-12.441450119018555,-8.88326358795166,0.38254937529563904,8.068100929260254,2.225512981414795,-4.127049446105957,-0.9297055602073669,-13.221030235290527,4.494882106781006,-5.628513813018799,0.14415255188941956,-13.219837188720703,-1.515181303024292,3.426374673843384,5.9605231285095215,-14.905993461608887,10.729073524475098,6.628054141998291,0.43374115228652954,-8.680770874023438,2.5401504039764404,5.991894245147705,12.30744457244873,-3.467937469482422,-3.5088019371032715,1.6825097799301147,-3.5288546085357666,3.333322286605835,8.39924430847168,5.677886486053467,-2.428201913833618,0.0006248667486943305,10.691048622131348,1.2782361507415771,-3.664884567260742,3.7079684734344482,2.2474818229675293,-0.6050715446472168,-0.9154132604598999,2.6853039264678955,-9.362403869628906,-1.232638955116272,-11.011391639709473,-1.1072192192077637,0.036225128918886185,-0.5800527930259705,-10.435150146484375,-11.152740478515625,-1.7901520729064941,-12.15627384185791,-12.616455078125,2.6857473850250244,-4.20349645614624,4.366664886474609,0.6807230710983276,0.41745999455451965,4.866658687591553,-1.4602856636047363,2.59948468208313,3.605363607406616,-1.1430168151855469,-4.523717403411865,-0.7707664370536804,2.855860471725464,2.0146303176879883,-9.973336219787598,5.420752048492432,5.523881435394287,-2.217510461807251,-6.255970478057861,-3.31126070022583,8.77548885345459,-1.9503775835037231,9.095629692077637,4.759898662567139,-2.0965139865875244,-12.050580978393555,0.46801865100860596,9.121869087219238,1.7085673809051514,8.504958152770996,-1.5325199365615845,5.284420013427734,-3.174346923828125,-5.55305290222168,-1.0584062337875366,1.2279120683670044,-5.677499771118164,-12.940438270568848,7.319241046905518,-7.113219738006592,-4.56709623336792,1.4037868976593018,-6.7531561851501465,-3.3113646507263184,12.324127197265625,2.0586137771606445,-4.4412407875061035,-4.327524185180664,0.36652445793151855,-0.17733953893184662,-10.990785598754883,3.389220714569092,7.746275901794434,8.363906860351562,3.3112449645996094,0.601872444152832,-8.372268676757812,4.279906749725342,1.9982258081436157,-1.731374740600586,-1.6751993894577026,3.9947400093078613,-7.466057777404785,-0.7521204948425293,0.18263442814350128,0.14985424280166626,-1.5020798444747925,0.9205734133720398,-12.138845443725586,3.0829625129699707,8.685018539428711,-2.9675369262695312,8.620099067687988,1.3578078746795654,3.865649461746216,-9.01210880279541,0.8997402191162109,2.3364033699035645,-2.166187286376953,-0.3287121057510376,-7.358320713043213,3.4436495304107666,11.866243362426758,4.963801383972168,-4.5639448165893555,-1.477325201034546,0.877594530582428,-1.0623283386230469,7.932259559631348,-3.3708295822143555,-7.975892066955566,4.35214376449585,7.469531059265137,0.13014549016952515,4.3245768547058105,12.852410316467285,-10.494866371154785,2.7710015773773193,5.409967422485352,8.276089668273926,6.054072380065918,5.989628791809082,0.8337182402610779,-12.885903358459473,4.447499752044678,4.360081672668457,-12.714619636535645,6.080239772796631,-4.393963813781738,-8.881436347961426,-7.708948135375977,-10.780259132385254,-2.538412094116211,-0.2794233560562134,5.400967121124268,0.15929163992404938,5.809757709503174,-8.809534072875977,-13.278569221496582,-10.37930679321289,8.892694473266602,-0.373055100440979,2.7868399620056152,-1.303536057472229,-1.9830470085144043,7.824373722076416,5.030409336090088,-1.1992918252944946,11.446234703063965,2.8084452152252197,2.3257973194122314,0.011204365640878677,5.50791597366333,-1.557336688041687,-3.242377519607544,7.159465789794922,-11.152959823608398,0.32136934995651245,1.3318579196929932,3.555795431137085,-5.717823505401611,-1.590267300605774,8.22616195678711,-12.494340896606445,4.017299175262451,3.9674606323242188,-9.174430847167969,1.5156291723251343,6.645021438598633,7.2269463539123535,-0.47799253463745117,10.121796607971191,-5.477656841278076,6.809574604034424,3.6298303604125977,8.277430534362793,-7.593912601470947,3.050346851348877,-1.6500746011734009,-5.4688920974731445,-5.420396327972412,3.29300856590271,0.8326242566108704,2.1574442386627197,4.210001468658447,-10.137595176696777,7.466470241546631,-2.660728931427002,-2.08371901512146,-5.228647708892822,-5.665803909301758,0.9050819277763367],\"yaxis\":\"y\",\"type\":\"scatter\",\"textposition\":\"top center\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Word Embedding 2D Visualization\"},\"height\":600},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('127e7dba-8b4a-4154-ab5e-1ef1271c92d5');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"markdown","source":["At a high level, you should see words with similar meaning clustering together. You can use your mouse to zoom in and inspect the vector space closer.\n","\n","You should also see mini-clusters within this plot; you would need to zoom into examine these. Examples of mini-clusters you might see are:\n","* <b>Time words:</b> hours, minutes, seconds, months, weeks, years, etc.\n","* <b>Years:</b> 2000, 2002, 2004, etc.\n","* <b>Numbers:</b> 10, 15, 37, etc.\n","* <b>Months:</b> january, february, march, etc. <em>Question: does the word 'may', which is both a month and a modal verb, cluster with the other months? If not, can you see where it is in relation to other modal verbs ('can', 'will', 'would', 'might', etc.)?</em>\n","\n","Feel free to increase the number of vectors plotted if you want to investigate further."],"metadata":{"id":"8qNOeI2Wn76l"}},{"cell_type":"markdown","metadata":{"id":"zHbJ1-aDsWCG"},"source":["# Part 2: Train a Convolutional Neural Network (CNN) [50 points]\n","\n","The second part of this homework concerns text classification. You will train a CNN classifier to determine the sentiment of movie reviews."]},{"cell_type":"markdown","metadata":{"id":"bMVBA0ijAUgt"},"source":["## Download & Preprocess the Data\n","We will be using the IMDb movie reviews dataset, which is a corpus of movie reviews along with a <em>positive</em> or <em>negative</em> classification. This is again provided by torchtext.\n","\n","The following cell will produce `train_data` and `test_data`. It also does some basic tokenization.\n","\n","*   To access the list of textual tokens for the *i*th example, use `train_data[i][1]`\n","*   To access the label for the *i*th example, use `train_data[i][0]`"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"dfX3bNby8FYL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678658139770,"user_tz":300,"elapsed":23898,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"f229f548-0b24-4e0c-ceb2-0fd4f524ac6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Num. Train Examples: 20000\n","Num. Test Examples: 5000\n","\n","SAMPLE DATA:\n","Sample text: ['I', 'did', 'not', 'like', 'the', 'pretentious', 'and', 'overrated', 'Apocalypse', 'Now', '.', 'Probably', 'my', 'favorite', 'Vietnam', 'War', 'film', 'is', 'The', 'Deer', 'Hunter', '.', 'The', 'Deer', 'Hunter', 'focused', 'on', 'one', 'part', 'of', 'the', 'war', ',', 'and', 'then', 'focused', 'on', 'the', 'lives', 'before', 'the', 'war', '.', 'This', 'movie', 'is', 'essentially', 'Deer', 'Hunter', '2', '.', 'The', 'script', 'is', 'too', 'loose', 'compared', 'to', 'the', 'Deer', 'Hunter', '.', 'The', 'story', 'is', 'never', 'developed', 'to', 'the', 'point', 'that', 'the', 'audience', 'can', 'truly', 'understand', 'and', 'feel', 'for', 'the', 'characters', 'like', 'the', 'Deerhunter', 'did', '.', 'The', 'Vietnam', 'flashbacks', 'are', 'not', 'as', 'gripping', 'or', 'involved', 'as', 'the', 'ones', 'in', 'the', 'Deerhunter', '.', 'This', 'is', 'why', 'I', 'can', 'only', 'give', 'this', 'movie', '7', 'out', 'of', '10.<br', '/><br', '/>However', ',', 'I', 'think', 'that', 'the', 'acting', 'was', 'outstanding', '.', 'DeNiro', 'and', 'Harris', 'are', 'truly', 'amazing', 'actors', '.', 'They', 'totally', 'immersed', 'themselves', 'in', 'their', 'characters', 'and', 'expressed', 'the', 'great', 'anguish', 'of', 'two', 'former', 'friends', 'who', 'lost', 'their', 'best', 'friend', 'Bobby', 'in', 'combat', '.', 'Harris', \"'\", 'character', 'is', 'a', 'half-dead', 'alcoholic', ',', 'who', 'hides', 'the', 'guilt', 'that', 'he', 'has', 'in', 'Bobby', 'losing', 'his', 'life', 'trying', 'to', 'save', 'his.<br', '/><br', '/>I', 'also', 'like', 'the', 'supporting', 'cast', '.', 'Everyone', 'in', 'the', 'town', 'is', 'part', 'of', 'the', 'movie', '.', 'The', 'town', 'obviously', \"can't\", 'handle', 'Vietnam', 'vets', 'very', 'well', '.', 'Like', 'many', 'small', 'towns', ',', 'it', 'is', 'all', 'about', 'being', 'quiet', ',', 'humble', ',', 'and', 'minding', \"one's\", 'business', '.', 'Harris', \"'\", 'character', ',', 'however', ',', \"can't\", 'be', 'any', 'of', 'these', 'things', '.', 'It', 'is', 'interesting', 'how', 'wars', 'effect', 'people', '.', 'Some', 'people', 'rebound', 'quickly', ',', 'while', 'others', 'never', 'really', 'recover', '.']\n","Sample label: pos \n","\n","Sample text: ['This', 'is', ',', 'ostensibly', ',', 'a', 'movie', 'about', 'multiple', 'grief', '.', 'As', 'such', ',', 'it', 'ought', 'to', 'move', 'viewers', 'and', 'make', 'them', 'empathetic', 'with', 'the', 'plight', 'of', 'the', 'main', 'characters', '.', 'However', ',', 'its', 'irritatingly', 'postmodern', 'style', 'makes', 'it', 'almost', 'incomprehensible', '.', 'The', 'camera', 'continually', 'switches', 'from', 'one', 'scene', 'to', 'another', ',', 'from', 'one', 'personal', 'crisis', 'to', 'the', 'next', ',', 'creating', 'a', 'choppy', ',', 'disjointed', 'effect', '.', 'Most', 'characters', 'appear', 'to', 'live', 'aimless', ',', 'unstructured', 'lives', ',', 'held', 'together', 'by', 'their', 'professional', 'commitments', '.', '(', 'It', 'also', 'stretches', 'credibility', 'that', 'a', 'man', 'who', 'has', 'just', 'been', 'given', 'what', 'amounts', 'to', 'a', 'likely', 'death', 'sentence', ',', 'would', 'cheerfully', 'indulge', 'in', 'a', 'sex', 'romp', 'with', 'a', 'woman', 'he', 'has', 'just', 'met)', '.', 'The', 'storyline', '(', 'if', 'there', 'is', 'a', 'storyline', ')', 'is', 'difficult', 'to', 'follow', '.', 'In', 'sum', ',', 'the', 'overall', 'effect', 'is', 'rather', 'disappointing', '.', 'In', 'spite', 'of', 'all', 'that', ',', 'the', 'acting', 'is', 'generally', 'good', 'and', 'some', 'of', 'the', 'scenes', 'are', 'quite', 'powerful', '.']\n","Sample label: neg \n","\n","Sample text: ['I', 'know', 'that', \"you've\", 'already', 'entered', 'this', 'in', 'film', 'festivals', '(', 'or', 'at', 'least', 'I', 'think', 'you', 'have', ',', 'I', 'may', 'just', 'be', 'making', 'that', 'up', ')', 'but', 'I', 'think', 'this', 'should', 'get', '\"', 'best', 'animated', 'short', 'film', '\"', 'in', 'every', 'one', '.', 'Bravo', '.', 'I', \"can't\", 'wait', 'for', 'the', 'full', 'film', '.', 'I', 'realize', 'that', 'you', 'may', 'not', 'hear', 'this', 'often', 'enough', 'because', 'of', 'the', 'bizarre', 'nature', 'of', 'your', 'animations', ',', 'but', 'hear', 'it', 'now', 'and', 'accept', 'it', 'as', 'the', 'truth', '.', 'Kudos', ',', 'my', 'friend', '.', 'Okay', ',', 'now', \"I'm\", 'just', 'trying', 'to', 'get', 'ten', 'lines', 'of', 'text..', '.', 'Though', 'I', 'still', 'mean', 'it', '.', 'And', 'here', 'comes', 'yet', 'another', '-SHOE!-', 'and', 'I', 'cannot', 'stop', 'here', 'yet', '.', 'This', 'is', 'extremely', 'annoying', 'and', 'yet', 'at', 'the', 'same', 'time', 'I', 'have', 'nothing', 'better', 'to', 'do', '.', 'In', 'fact', ',', \"I'll\", 'probably', 'watch', 'all', 'of', 'your', 'movies', 'in', 'yet', 'another', 'spasmodic', '\"', 'Jason', 'Steele', 'Marathon.', '\"', 'I', 'do', 'have', 'a', 'lot', 'of', 'those.<br', '/><br', '/>-R']\n","Sample label: pos \n","\n","Sample text: ['Rumor', 'has', 'it', 'that', 'when', 'the', 'NASA', 'Technical', 'Advisors', 'to', 'this', 'film', 'were', 'asked', 'to', 'keep', 'the', 'picture', 'believable', ',', 'they', 'laughed', 'for', 'several', 'hours', '.', 'After', 'all', ',', 'unless', 'you', 'are', 'a', 'politician', 'or', 'work/crew', 'the', 'shuttle', ',', 'you', 'are', 'not', 'going', 'to', 'get', 'in', 'the', 'shuttle', '.', 'Furthermore', ',', 'Space', '(', 'Cadet', ')', 'Camp', 'is', 'in', 'Alabama', ',', 'not', 'Florida.<br', '/><br', '/>The', 'truth', 'is', 'everyone', 'on', 'Earth', 'will', 'win', 'multi-billion', 'dollar', 'lottery', 'prizes', 'before', 'the', 'events', 'depicted', 'in', 'this', 'film', 'ever', 'become', 'possible', '.', 'This', 'film', 'was', 'meant', 'for', 'kids', ',', 'and', 'had', 'to', 'have', 'been', 'written', 'by', 'one', ',', 'because', 'they', 'are', 'not', 'aware', 'of', 'the', 'myriad', 'restrictions', 'and', 'requirements', 'regarding', 'access', 'to', 'KSC/CCAFS.<br', '/><br', '/>This', 'is', 'the', 'most', 'useless', 'film', 'of', 'all', 'time', ',', 'and', 'it', 'was', 'a', 'well', 'deserved', 'flop', '.']\n","Sample label: neg \n","\n","Sample text: ['This', 'was', 'a', 'very', 'enjoyable', 'film', '.', 'A', 'humorous', ',', 'but', 'poignant', 'look', 'at', 'family', ',', 'and', 'the', 'obligations', 'that', 'come', 'with', 'it', '.', 'The', 'story', 'of', 'a', 'man', 'who', 'comes', 'home', 'from', 'his', 'life', 'in', 'the', 'city', 'to', 'his', 'fathers', 'bath', 'house', 'in', 'a', 'small', 'Chinese', 'village', '.', 'There', 'he', 'learns', 'to', 'appreciate', ',', 'even', 'cherish', 'the', 'very', 'things', 'he', 'left', 'home', 'to', 'get', 'away', 'from', '.', 'The', 'film', 'is', 'as', 'visually', 'beautiful', 'as', 'it', 'is', 'emotionally', 'beautiful', '.']\n","Sample label: pos \n","\n"]}],"source":["### DO NOT EDIT ###\n","\n","import torchtext\n","import random\n","\n","def cnn_preprocess(review):\n","    '''\n","    Simple preprocessing function.\n","    '''\n","    res = []\n","    for x in review.split(' '):\n","        remove_beg=True if x[0] in {'(', '\"', \"'\"} else False\n","        remove_end=True if x[-1] in {'.', ',', ';', ':', '?', '!', '\"', \"'\", ')'} else False\n","        if remove_beg and remove_end: res += [x[0], x[1:-1], x[-1]]\n","        elif remove_beg: res += [x[0], x[1:]]\n","        elif remove_end: res += [x[:-1], x[-1]]\n","        else: res += [x]\n","    return res\n","\n","if __name__=='__main__':\n","    train_data = torchtext.datasets.IMDB(root='.data', split='train')\n","    train_data = list(train_data)\n","    train_data = [(x[0], cnn_preprocess(x[1])) for x in train_data]\n","    train_data, test_data = train_data[0:10000] + train_data[12500:12500+10000], train_data[10000:12500] + train_data[12500+10000:], \n","\n","    print('Num. Train Examples:', len(train_data))\n","    print('Num. Test Examples:', len(test_data))\n","\n","    # Make pos/neg\n","    train_data = [('neg' if x[0] == 1 else 'pos', x[1]) for x in train_data]  \n","    test_data = [('neg' if x[0] == 1 else 'pos', x[1]) for x in test_data]\n","\n","    print(\"\\nSAMPLE DATA:\")\n","    for x in random.sample(train_data, 5):\n","        print('Sample text:', x[1])\n","        print('Sample label:', x[0], '\\n')"]},{"cell_type":"markdown","metadata":{"id":"lvFX-iX5oq7T"},"source":["## <font color='red'>TODO:</font> Define the Dataset Class [10 Points]\n","\n","In the following cell, we will define the <b>dataset</b> class. The dataset contains the tokenized data for your model. You need to implement the following functions: \n","\n","*   <b>` build_dictionary(self)`:</b>  <b>[5 points]</b> Creates the dictionaries `idx2word` and `word2idx`. You will represent each word in the dataset with a unique index, and keep track of this in these dictionaries. Use the hyperparameter `threshold` to control which words appear in the dictionary: a training word’s frequency should be `>= threshold` to be included in the dictionary.\n","\n","* <b>`convert_text(self)`:</b> Converts each review in the dataset to a list of indices, given by your `word2idx` dictionary. You should store this in the `textual_ids` variable, and the function does not return anything. If a word is not present in the  `word2idx` dictionary, you should use the `<UNK>` token for that word. Be sure to append the `<END>` token to the end of each review.\n","\n","*   <b>` get_text(self, idx) `:</b> Return the review at `idx` in the dataset as an array of indices corresponding to the words in the review. If the length of the review is less than `max_len`, you should pad the review with the `<PAD>` character up to the length of `max_len`. If the length is greater than `max_len`, then it should only return the first `max_len` words. The return type should be `torch.LongTensor`.\n","\n","*   <b>`get_label(self, idx) `</b>: Return the value `1` if the label for `idx` in the dataset is `positive`, and should return `0` if it is `negative`. The return type should be `torch.LongTensor`.\n","\n","*  <b> ` __len__(self) `:</b> Return the total number of reviews in the dataset as an `int`.\n","\n","*   <b>` __getitem__(self, idx)`:</b> <b>[5 points]</b> Return the (padded) text, and the label. The return type for both these items should be `torch.LongTensor`. You should use the ` get_label(self, idx) ` and ` get_text(self, idx) ` functions here.\n","\n","\n","<b>Note:</b> You should convert all words to lower case in your functions.\n","\n","<font color='green'><b>Hint:</b> Make sure that you use instance variables such as `self.threshold` throughout your code, rather than the global variable `THRESHOLD` (defined later on). The variable `THRESHOLD` will not be known to the autograder, and the use of it within the class will cause an autograder error.</font>\n","\n","<font color='green'><b>Hint:</b> Make sure that your dataset is deterministic $-$ that is, if it is instantiated multiple times, then the `word2idx` and `idx2word` mappings are the same. If they are not, the autograder will be unable to evaluate your CNN classifications.</font>"]},{"cell_type":"code","execution_count":93,"metadata":{"id":"1irMn3LX2YDB","executionInfo":{"status":"ok","timestamp":1678663315118,"user_tz":300,"elapsed":466,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"outputs":[],"source":["CNN_PAD = '<PAD>'\n","CNN_END = '<END>'\n","CNN_UNK = '<UNK>'\n","\n","from torch.utils import data\n","from collections import defaultdict\n","\n","class TextDataset(data.Dataset):\n","    def __init__(self, examples, split, threshold, max_len, idx2word=None, word2idx=None):\n","        ##### DO NOT EDIT #####\n","\n","        self.examples = examples\n","        assert split in {'train', 'val', 'test'}\n","        self.split = split\n","        self.threshold = threshold\n","        self.max_len = max_len\n","\n","        # Dictionaries\n","        self.word2idx = word2idx # Mapping of word to index\n","        self.idx2word = idx2word # Mapping of index to word\n","        if split == 'train':\n","            self.build_dictionary()\n","        self.vocab_size = len(self.word2idx)\n","        \n","        # Convert text to indices\n","        self.textual_ids = []\n","        self.convert_text()\n","\n","    \n","    def build_dictionary(self): \n","        '''\n","        Build the dictionaries idx2word and word2idx. This is only called when split='train', as these\n","        dictionaries are passed in to the __init__(...) function otherwise. Be sure to use self.threshold\n","        to control which words are assigned indices in the dictionaries.\n","        Returns nothing.\n","        '''\n","        assert self.split == 'train'\n","        \n","        # Don't change this\n","        self.idx2word = {0:CNN_PAD, 1:CNN_END, 2: CNN_UNK}\n","        self.word2idx = {CNN_PAD:0, CNN_END:1, CNN_UNK: 2}\n","\n","        ##### TODO #####\n","        # Count the frequencies of all words in the training data (self.examples)\n","        # Assign idx (starting from 3) to all words having word_freq >= self.threshold\n","        # Make sure you call word.lower() on each word to convert it to lowercase\n","        frequencies = {}\n","        for i in self.examples:\n","          for ii in i[1]:\n","            word = ii.lower()\n","            if word not in frequencies:\n","              frequencies[word] = 0\n","            frequencies[word] += 1\n","        \n","        indices=3\n","        frequencies = dict(sorted(frequencies.items(), key=lambda x: (x[0])))\n","        for i in frequencies:\n","          if frequencies[i] >= self.threshold:\n","            self.word2idx[i] = indices\n","            self.idx2word[indices] = i\n","            indices += 1\n","        # pass\n","    \n","    def convert_text(self):\n","        '''\n","        Convert each review in the dataset (self.examples) to a list of indices, given by self.word2idx.\n","        Store this in self.textual_ids; returns nothing.\n","        '''\n","        ##### TODO #####\n","        # Remember to replace a word with the <UNK> token if it does not exist in the word2idx dictionary.\n","        # Remember to append the <END> token to the end of each review.\n","        for i in self.examples:\n","          temp = []\n","          for ii in i[1]:\n","            word = ii.lower()\n","            if word in self.word2idx:\n","              temp.append(self.word2idx[word])\n","            else:\n","              temp.append(self.word2idx[CNN_UNK])\n","          temp.append(self.word2idx[CNN_END])\n","          self.textual_ids.append(temp)\n","          # print(self.textual_ids)\n","        \n","       \n","        # pass\n","\n","    def get_text(self, idx):\n","        '''\n","        Return the review at idx as a long tensor (torch.LongTensor) of integers corresponding to the words in the review.\n","        You may need to pad as necessary (see above).\n","        '''\n","\n","        ##### TODO #####\n","        # print(self.word2idx)\n","        # print(self.textual_ids)\n","        temp = self.textual_ids[idx]\n","        while(len(temp)<self.max_len):\n","          temp.append(self.word2idx[CNN_PAD])\n","        # if (len(temp)>self.max_len):\n","        temp = temp[:self.max_len]\n","        return torch.LongTensor(temp)\n","        # return None\n","    \n","    def get_label(self, idx):\n","        '''\n","        This function should return the value 1 if the label for idx in the dataset is 'positive', \n","        and 0 if it is 'negative'. The return type should be torch.LongTensor.\n","        '''\n","\n","        ##### TODO #####\n","        if self.examples[idx][0] == 'pos':\n","          return torch.tensor(1)\n","        elif self.examples[idx][0] == 'neg':\n","          return torch.tensor(0)\n","\n","    def __len__(self):\n","        '''\n","        Return the number of reviews (int value) in the dataset\n","        '''\n","\n","        ##### TODO #####\n","\n","        # return None\n","        return len(self.examples)\n","    \n","    def __getitem__(self, idx):\n","        '''\n","        Return the review, and label of the review specified by idx.\n","        '''\n","\n","        ##### TODO #####\n","\n","        # return None, None\n","        return self.get_text(idx), self.get_label(idx)"]},{"cell_type":"markdown","metadata":{"id":"HxVxiGGbFJAj"},"source":["##Sanity Check: Dataset Class\n","\n","The code below runs a sanity check for your `Dataset` class. The tests are similar to the hidden ones in Gradescope. However, note that passing the sanity check does <b>not</b> guarantee that you will pass the autograder; it is intended to help you debug."]},{"cell_type":"code","execution_count":94,"metadata":{"id":"bvHIZt8Z-RzK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678663317526,"user_tz":300,"elapsed":761,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"3cf909f5-5ea5-4599-87bb-41a190a68fcb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample dataset:\n","('pos', ['Your', 'life', 'is', 'good', 'when', 'you', 'have', 'money', ',', 'success', 'and', 'health'])\n","('neg', ['Life', 'is', 'bad', 'when', 'you', 'got', 'not', 'a', 'lot'])\n","\n","--- TEST: idx2word and word2idx dictionaries ---\n","\tthreshold: 1 \tmax_len: 3 \tPASSED \t\n","\tthreshold: 2 \tmax_len: 3 \tPASSED \t\n","\tthreshold: 3 \tmax_len: 3 \tPASSED \t\n","\n","--- TEST: len(dataset) ---\n","\tPASSED\n","\n","--- TEST: __getitem__(self, idx) ---\n","\tthreshold: 1 \tmax_len: 3 \tidx: 0 \tPASSED \t\n","\tthreshold: 1 \tmax_len: 3 \tidx: 1 \tPASSED \t\n","\tthreshold: 1 \tmax_len: 8 \tidx: 0 \tPASSED \t\n","\tthreshold: 1 \tmax_len: 8 \tidx: 1 \tPASSED \t\n","\tthreshold: 1 \tmax_len: 15 \tidx: 0 \tPASSED \t\n","\tthreshold: 1 \tmax_len: 15 \tidx: 1 \tPASSED \t\n","\tthreshold: 2 \tmax_len: 3 \tidx: 0 \tPASSED \t\n","\tthreshold: 2 \tmax_len: 3 \tidx: 1 \tPASSED \t\n","\tthreshold: 2 \tmax_len: 8 \tidx: 0 \tPASSED \t\n","\tthreshold: 2 \tmax_len: 8 \tidx: 1 \tPASSED \t\n","\tthreshold: 2 \tmax_len: 15 \tidx: 0 \tPASSED \t\n","\tthreshold: 2 \tmax_len: 15 \tidx: 1 \tPASSED \t\n","\tthreshold: 3 \tmax_len: 3 \tidx: 0 \tPASSED \t\n","\tthreshold: 3 \tmax_len: 3 \tidx: 1 \tPASSED \t\n","\tthreshold: 3 \tmax_len: 8 \tidx: 0 \tPASSED \t\n","\tthreshold: 3 \tmax_len: 8 \tidx: 1 \tPASSED \t\n","\tthreshold: 3 \tmax_len: 15 \tidx: 0 \tPASSED \t\n","\tthreshold: 3 \tmax_len: 15 \tidx: 1 \tPASSED \t\n"]}],"source":["### DO NOT EDIT ###\n","\n","def sanityCheckTextDataset():\n","    #\tRead in the sample corpus\n","    reviews = [('pos', 'Your life is good when you have money, success and health'),\n","               ('neg', 'Life is bad when you got not a lot')]\n","    data = [(x[0], cnn_preprocess(x[1])) for x in reviews]\n","    print(\"Sample dataset:\")\n","    for x in data: print(x)\n","\n","    thresholds = [1,2,3]\n","    print('\\n--- TEST: idx2word and word2idx dictionaries ---') # max_len does not matter for this test\n","    correct = [[',', '<END>', '<PAD>', '<UNK>', 'a', 'and', 'bad', 'good', 'got', 'have', 'health', 'is', 'life', 'lot', 'money', 'not', 'success', 'when', 'you', 'your'], ['<END>', '<PAD>', '<UNK>', 'is', 'life', 'when', 'you'], ['<END>', '<PAD>', '<UNK>']]\n","    for i in range(len(thresholds)):\n","        dataset = TextDataset(data, 'train', threshold=thresholds[i], max_len=3)\n","\n","        has_passed, message = True, ''\n","        if has_passed and (dataset.vocab_size != len(dataset.word2idx) or dataset.vocab_size != len(dataset.idx2word)):\n","            has_passed, message = False, 'dataset.vocab_size (' + str(dataset.vocab_size) + ') must be the same length as dataset.word2idx (' + str(len(dataset.word2idx)) + ') and dataset.idx2word ('+str(len(dataset.idx2word)) +').'\n","        if has_passed and (dataset.vocab_size != len(correct[i])):\n","            has_passed, message = False, 'Your vocab size is incorrect. Expected: ' + str(len(correct[i])) + '\\tGot: ' + str(dataset.vocab_size)\n","        if has_passed and sorted(list(dataset.idx2word.keys())) != list(range(0, dataset.vocab_size)):\n","            has_passed, message = False, 'dataset.idx2word must have keys ranging from 0 to dataset.vocab_size-1. Keys in your dataset.idx2word: ' + str(sorted(list(dataset.idx2word.keys())))\n","        if has_passed and sorted(list(dataset.word2idx.keys())) != correct[i]:\n","            has_passed, message = False, 'Your dataset.word2idx has incorrect keys. Expected: ' + str(correct[i]) + '\\tGot: ' + str(sorted(list(dataset.word2idx.keys())))\n","        if has_passed: # Check that word2idx and idx2word are consistent\n","            widx = sorted(list(dataset.word2idx.items())) \n","            idxw = sorted(list([(v,k) for k,v in dataset.idx2word.items()]))\n","            if not (len(widx) == len(idxw) and all([widx[q] == idxw[q] for q in range(len(widx))])):\n","                has_passed, message = False, 'Your dataset.word2idx and dataset.idx2word are not consistent. dataset.idx2word: ' + str(dataset.idx2word) + '\\tdataset.word2idx: ' + str(dataset.word2idx)\n","\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        print('\\tthreshold:', thresholds[i], '\\tmax_len:', 3, '\\t'+status, '\\t'+message)\n","    \n","    print('\\n--- TEST: len(dataset) ---')\n","    has_passed = len(dataset) == 2\n","    if has_passed: print('\\tPASSED')\n","    else: print('\\tlen(dataset) is incorrect. Expected: 2\\tGot: ' + str(len(dataset)))\n","\n","    print('\\n--- TEST: __getitem__(self, idx) ---')\n","    max_lens = [3,8,15]\n","    idxes = [0,1]\n","    combos = [{'threshold': t, 'max_len': m, 'idx': idx} for t in thresholds for m in max_lens for idx in idxes]\n","    correct = [(torch.tensor([3, 4, 5]), torch.tensor(1)), (torch.tensor([ 4,  5, 15]), torch.tensor(0)), (torch.tensor([ 3,  4,  5,  6,  7,  8,  9, 10]), torch.tensor(1)), (torch.tensor([ 4,  5, 15,  7,  8, 16, 17, 18]), torch.tensor(0)), (torch.tensor([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  1,  0,  0]), torch.tensor(1)), (torch.tensor([ 4,  5, 15,  7,  8, 16, 17, 18, 19,  1,  0,  0,  0,  0,  0]), torch.tensor(0)), (torch.tensor([2, 3, 4]), torch.tensor(1)), (torch.tensor([3, 4, 2]), torch.tensor(0)), (torch.tensor([2, 3, 4, 2, 5, 6, 2, 2]), torch.tensor(1)), (torch.tensor([3, 4, 2, 5, 6, 2, 2, 2]), torch.tensor(0)), (torch.tensor([2, 3, 4, 2, 5, 6, 2, 2, 2, 2, 2, 2, 1, 0, 0]), torch.tensor(1)), (torch.tensor([3, 4, 2, 5, 6, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0]), torch.tensor(0)), (torch.tensor([2, 2, 2]), torch.tensor(1)), (torch.tensor([2, 2, 2]), torch.tensor(0)), (torch.tensor([2, 2, 2, 2, 2, 2, 2, 2]), torch.tensor(1)), (torch.tensor([2, 2, 2, 2, 2, 2, 2, 2]), torch.tensor(0)), (torch.tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0]), torch.tensor(1)), (torch.tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0]), torch.tensor(0))]\n","    for i in range(len(combos)):\n","        combo = combos[i]\n","        dataset = TextDataset(data, 'train', threshold=combo['threshold'], max_len=combo['max_len'])\n","        returned = dataset.__getitem__(combo['idx'])\n","\n","        has_passed, message = True, ''\n","        if has_passed and len(returned) != 2:\n","            has_passed, message = False, 'dataset.__getitem__(idx) must return 2 items. Got ' + str(len(returned)) +' items instead.'\n","        if has_passed and (type(returned[0]) != torch.Tensor or type(returned[1]) != torch.Tensor):\n","            has_passed, message = False, 'Both returns must be of type torch.Tensor. Got: (' + str(type(returned[0])) + ', ' + str(type(returned[1])) + ')'\n","        if has_passed and (returned[0].shape != correct[i][0].shape):\n","            has_passed, message = False, 'Shape of first return is incorrect. Expected: ' + str(correct[i][0].shape) + '.\\tGot: ' + str(returned[0].shape)\n","        if has_passed and (returned[1].shape != correct[i][1].shape):\n","            has_passed, message = False, 'Shape of second return is incorrect. Expected: ' + str(correct[i][1].shape) + '.\\tGot: ' + str(returned[1].shape) + '\\n\\t\\tHint: torch.Size([]) means that the tensor should be dimensionless (just a number). Try squeezing your result.'\n","        if has_passed and (returned[1] != correct[i][1]):\n","            has_passed, message = False, 'Label (second return) is incorrect. Expected: ' + str(correct[i][1]) + '.\\tGot: ' + str(returned[1])\n","        if has_passed:\n","            correct_padding_idxes, your_padding_idxes = torch.where(correct[i][0] == 0)[0], torch.where(returned[0] == dataset.word2idx[CNN_PAD])[0]\n","            if not (correct_padding_idxes.shape == your_padding_idxes.shape and torch.all(correct_padding_idxes == your_padding_idxes)):\n","                has_passed, message = False, 'Padding is not correct. Expected padding indxes: ' + str(correct_padding_idxes) + '.\\tYour padding indexes: ' + str(your_padding_idxes)\n","\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        print('\\tthreshold:', combo['threshold'], '\\tmax_len:', combo['max_len'] , '\\tidx:', combo['idx'], '\\t'+status, '\\t'+message)\n","\n","if __name__ == '__main__':\n","    sanityCheckTextDataset()"]},{"cell_type":"markdown","metadata":{"id":"CR4VQbQCNZH6"},"source":["The following cell builds the dataset on the IMDb movie reviews and prints an example:"]},{"cell_type":"code","execution_count":95,"metadata":{"id":"HSxpGXj6ml9N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678663329392,"user_tz":300,"elapsed":4875,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"d1c4ef87-cf87-47fd-d5e2-c99e8c38e598"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab size: 19002 \n","\n","Example text:\n","[\"I've\", 'now', 'seen', 'this', 'one', 'about', '10', 'times', ',', 'so', 'there', 'must', 'be', 'something', 'about', 'it', 'I', 'like!<br', '/><br', \"/>50's\", 'US', 'sci-fi', 'movies', 'were', 'pretty', 'much', 'a', 'mixed', 'bunch', ':', 'they', 'were', 'either', 'intelligently', 'made', 'and/or', 'thought', 'provoking', 'or', 'cheap', 'and', 'laughable', 'cheese', '.', 'Forbidden', 'Planet', 'is', 'a', 'bit', 'of', 'both', ',', 'but', 'in', 'that', 'rarity', 'for', 'the', 'genre', ',', 'colour.<br', '/><br', '/>It', 'also', 'had', 'a', 'head', 'start', 'with', 'the', 'script', '-', 'although', 'Shakespeare', 'might', 'not', 'have', 'recognised', 'it', ',', 'it', 'was', 'based', 'on', 'his', 'timeless', 'play', 'and', 'thus', 'guaranteed', 'a', 'certain', 'amount', 'of', 'longevity', 'itself', 'if', 'made', 'well.<br', '/><br', \"/>It's\", 'the', 'story', 'of', 'one', 'mans', 'murderous', 'id', 'artificially', 'magnified', 'infinitely', 'by', 'machines', 'a', 'dead', 'race', 'left', 'switched', 'on', '200,000', 'years', 'before', '.', 'Along', 'the', 'way', 'the', 'plot', 'bristles', 'with', \"50's\", 'stereotypes', 'and', 'corn', 'so', 'pure', 'you', 'wonder', 'sometimes', 'why', \"you're\", 'watching', 'it', ',', 'but', 'always', 'do', '.', 'That', 'love', 'triangle', 'thing...yuk', '!', \"Disney's\", 'cartoonery', 'still', 'holds', 'up', 'well', ',', 'and', 'the', 'cartoon', 'backgrounds', 'straight', 'off', 'the', 'covers', 'of', 'Galaxy', 'magazine', 'etc', 'look', 'good', 'even', 'after', '50', 'years', '.', 'Robbie', 'driving', 'the', 'car', 'over', 'the', 'desert', 'in', 'the', 'far', 'distance', 'is', 'a', 'hoot', 'though!<br', '/><br', '/>All', 'in', 'all', ',', 'with', 'all', 'faults', ',', 'the', 'best', 'of', 'its', 'kind', 'and', 'we', 'should', 'be', 'grateful', 'that', 'such', 'a', 'pristine', 'print', 'survives', '.']\n","tensor([ 8620, 11788, 14980, 17089, 11987,   741,   419, 17212,    39, 15717,\n","        17043, 11407,  2038, 15789,   741,  9260,  8616,     2,    89,     2,\n","        17971, 14842, 11331, 18513, 13182, 11346,   701, 11122,  2794,   687,\n","        17060, 18513,  5782,  9066, 10454,  1276, 17112, 13394, 12058,  3310,\n","         1274,  9881,  3335,    47,  6965, 12806,  9232,   701,  2293, 11907,\n","         2524,    39,  2841,  8800, 16992, 13688,  6960, 17004,  7373,    39,\n","            2,    89,   220,  1177,  7882,   701,  8102, 16132, 18705, 17004,\n","        14894,    42,  1188, 15165, 10980, 11751,  8073, 13813,  9260,    39,\n","         9260, 18377,  1989, 11979,  8335, 17209, 12827,  1274, 17170,  7791,\n","          701,  3205,  1241, 11907, 10251,  9287,  8668, 10454, 18501,    89,\n","          221, 17004, 16274, 11907, 11987, 10580, 11383,  8636,  1580,     2,\n","         8927,  2859, 10441,   701,  4673, 13574,  9960, 16695, 11979,     2,\n","        18890,  2095,    47,  1167, 17004, 18416, 17004, 12857,     2, 18705,\n","          617, 16198,  1274,  4178, 15717, 13469, 18913, 18740, 15795, 18601,\n","        18919, 18402,  9260,    39,  2841,  1193,  5303,    47, 16992, 10316])\n","\n","Example label:\n","pos\n","tensor(1)\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    train_dataset = TextDataset(train_data, 'train', threshold=10, max_len=150)\n","    print('Vocab size:', train_dataset.vocab_size, '\\n')\n","\n","    randidx = random.randint(0, len(train_dataset)-1)\n","    text, label = train_dataset[randidx]\n","    print('Example text:')\n","    print(train_data[randidx][1])\n","    print(text)\n","    print('\\nExample label:')\n","    print(train_data[randidx][0])\n","    print(label)"]},{"cell_type":"markdown","metadata":{"id":"VcSKydlClwOC"},"source":["## <font color='red'>TODO:</font> Define the CNN Model [20 points]\n","Here you will define your convolutional neural network for text classification. We provide you with the CNN class, you need to fill in parts of the `__init__(...)` and `forward(...)` functions. Each of these functions is worth <b>10 points</b>.\n","\n","We have provided you with instructions and hints in the comments. In particular, pay attention to the desired shapes; you may find it helpful to print the shape of the tensors as you code. It may also help to keep PyTorch documentation open for the modules & functions you are using, since they describe input and output dimensions."]},{"cell_type":"code","execution_count":140,"metadata":{"id":"0ztuy2hUaAof","executionInfo":{"status":"ok","timestamp":1678666995902,"user_tz":300,"elapsed":308,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN(nn.Module):\n","    def __init__(self, vocab_size, embed_size, out_channels, filter_heights, stride, dropout, num_classes, pad_idx):\n","        super(CNN, self).__init__()\n","        \n","        ##### TODO #####\n","        # Create an embedding layer (https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n","        #   to represent the words in your vocabulary. Make sure to use vocab_size, embed_size, and pad_idx here.\n","        self.embedding = nn.Embedding(vocab_size, embed_size, pad_idx)\n","\n","        # Define multiple Convolution layers (nn.Conv2d) with filter (kernel) size [filter_height, embed_size] based on your \n","        #   different filter_heights.          \n","        # Input channels will be 1 and output channels will be out_channels (these many different filters will be trained \n","        #   for each convolution layer)\n","        # If you want, you can store a list of modules inside nn.ModuleList.\n","        # Note: even though your conv layers are nn.Conv2d, we are doing a 1d convolution since we are only moving the filter \n","        #   in one direction\n","        \n","        self.cnnLayers = []\n","        for i in filter_heights:\n","          self.cnnLayers.append(nn.Conv2d(1, out_channels, [i, embed_size]))\n","        self.cnnLayers = nn.ModuleList(self.cnnLayers)\n","\n","        # Create a dropout layer (nn.Dropout) using dropout\n","        self.dropout = nn.Dropout()\n","\n","        # Define a linear layer (nn.Linear) that consists of num_classes units \n","        #   and takes as input the concatenated output for all cnn layers (out_channels * num_of_cnn_layers units)\n","        self.linear = nn.Linear(out_channels*len(filter_heights), num_classes)\n","\n","\n","\n","    def forward(self, texts):\n","        \"\"\"\n","        texts: LongTensor [batch_size, max_len]\n","        \n","        Returns output: Tensor [batch_size, num_classes]\n","        \"\"\"\n","\n","        ##### TODO #####\n","\n","        # Pass texts through your embedding layer to convert from word ids to word embeddings\n","        #   Resulting: shape: [batch_size, max_len, embed_size]\n","        \n","        outputs = self.embedding(texts)\n","\n","        # Input to conv should have 1 channel. Take a look at torch's unsqueeze() function\n","        #   Resulting shape: [batch_size, 1, MAX_LEN, embed_size]\n","        outputs = torch.unsqueeze(outputs, 1)\n","        \n","        # Pass these texts to each of your conv layers and compute their output as follows:\n","        #   Your cnn output will have shape [batch_size, out_channels, *, 1] where * depends on filter_height and stride\n","        #   Convert to shape [batch_size, out_channels, *] (see torch's squeeze() function)\n","        #   Apply non-linearity on it (F.relu() is a commonly used one. Feel free to try others)\n","        #   Take the max value across last dimension to have shape [batch_size, out_channels]\n","        # Concatenate (torch.cat) outputs from all your cnns [batch_size, (out_channels*num_of_cnn_layers)]\n","        #\n","        temps = []\n","        # maxLastDim = -1\n","        for i in self.cnnLayers:\n","          o = i(outputs)\n","          o = torch.squeeze(o, 3)\n","          o = F.relu(o)\n","          o = torch.max(o, dim = -1).values\n","          temps.append(o)\n","        outputs = torch.cat(temps, dim = -1)\n","        \n","        \n","\n","        # Let's understand what you just did:\n","        #   Since each cnn is of different filter_height, it will look at different number of words at a time\n","        #     So, a filter_height of 3 means your cnn looks at 3 words (3-grams) at a time and tries to extract some information from it\n","        #   Each cnn will learn out_channels number of features from the words it sees at a time\n","        #   Then you applied a non-linearity and took the max value for all channels\n","        #     You are essentially trying to find important n-grams from the entire text\n","        # Everything happens on a batch simultaneously hence you have that additional batch_size as the first dimension\n","\n","        # Apply dropout\n","        outputs = self.dropout(outputs)\n","\n","        # Pass your output through the linear layer and return its output \n","        #   Resulting shape: [batch_size, num_classes]\n","        outputs = self.linear(outputs)\n","        # print(outputs.size())\n","\n","\n","        # NOTE: Do NOT apply a sigmoid or softmax to the final output - this is done in the training method!\n","\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"_mVE_ujfnh0w"},"source":["##Sanity Check: CNN Model\n","\n","The code below runs a sanity check for your `CNN` class. The tests are similar to the hidden ones in Gradescope. However, note that passing the sanity check does <b>not</b> guarantee that you will pass the autograder; it is intended to help you debug."]},{"cell_type":"code","execution_count":141,"metadata":{"id":"yy9oF6qUUHvV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678666998099,"user_tz":300,"elapsed":242,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"9ebe5ca5-207b-4189-f391-cdfc48d60268"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- TEST: Number of Model Parameters (tests __init__(...)) ---\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 22434\tYour Num. Params: 22434\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 22531\tYour Num. Params: 22531\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 22434\tYour Num. Params: 22434\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 22531\tYour Num. Params: 22531\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 23874\tYour Num. Params: 23874\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 23939\tYour Num. Params: 23939\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 23874\tYour Num. Params: 23874\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 23939\tYour Num. Params: 23939\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 41730\tYour Num. Params: 41730\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 42115\tYour Num. Params: 42115\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 41730\tYour Num. Params: 41730\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 42115\tYour Num. Params: 42115\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 47490\tYour Num. Params: 47490\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 47747\tYour Num. Params: 47747\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 47490\tYour Num. Params: 47490\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 47747\tYour Num. Params: 47747\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 44578\tYour Num. Params: 44578\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 44675\tYour Num. Params: 44675\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 44578\tYour Num. Params: 44578\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 44675\tYour Num. Params: 44675\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 47554\tYour Num. Params: 47554\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 47619\tYour Num. Params: 47619\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 47554\tYour Num. Params: 47554\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 47619\tYour Num. Params: 47619\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 82306\tYour Num. Params: 82306\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 82691\tYour Num. Params: 82691\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 82306\tYour Num. Params: 82306\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 82691\tYour Num. Params: 82691\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 94210\tYour Num. Params: 94210\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 94467\tYour Num. Params: 94467\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tExpected Num. Params: 94210\tYour Num. Params: 94210\n","\tPASSED\tInput: {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tExpected Num. Params: 94467\tYour Num. Params: 94467\n","\n","--- TEST: Output shape of forward(...) ---\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 10])\tExpected Output Shape: torch.Size([1, 2])\tYour Output Shape: torch.Size([1, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 10])\tExpected Output Shape: torch.Size([50, 2])\tYour Output Shape: torch.Size([50, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 100])\tExpected Output Shape: torch.Size([1, 2])\tYour Output Shape: torch.Size([1, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 100])\tExpected Output Shape: torch.Size([50, 2])\tYour Output Shape: torch.Size([50, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 10])\tExpected Output Shape: torch.Size([1, 3])\tYour Output Shape: torch.Size([1, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 10])\tExpected Output Shape: torch.Size([50, 3])\tYour Output Shape: torch.Size([50, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 100])\tExpected Output Shape: torch.Size([1, 3])\tYour Output Shape: torch.Size([1, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 100])\tExpected Output Shape: torch.Size([50, 3])\tYour Output Shape: torch.Size([50, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 10])\tExpected Output Shape: torch.Size([1, 2])\tYour Output Shape: torch.Size([1, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 10])\tExpected Output Shape: torch.Size([50, 2])\tYour Output Shape: torch.Size([50, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 100])\tExpected Output Shape: torch.Size([1, 2])\tYour Output Shape: torch.Size([1, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 100])\tExpected Output Shape: torch.Size([50, 2])\tYour Output Shape: torch.Size([50, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 10])\tExpected Output Shape: torch.Size([1, 3])\tYour Output Shape: torch.Size([1, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 10])\tExpected Output Shape: torch.Size([50, 3])\tYour Output Shape: torch.Size([50, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 100])\tExpected Output Shape: torch.Size([1, 3])\tYour Output Shape: torch.Size([1, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 100])\tExpected Output Shape: torch.Size([50, 3])\tYour Output Shape: torch.Size([50, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 10])\tExpected Output Shape: torch.Size([1, 2])\tYour Output Shape: torch.Size([1, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 10])\tExpected Output Shape: torch.Size([50, 2])\tYour Output Shape: torch.Size([50, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 100])\tExpected Output Shape: torch.Size([1, 2])\tYour Output Shape: torch.Size([1, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 100])\tExpected Output Shape: torch.Size([50, 2])\tYour Output Shape: torch.Size([50, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 10])\tExpected Output Shape: torch.Size([1, 3])\tYour Output Shape: torch.Size([1, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 10])\tExpected Output Shape: torch.Size([50, 3])\tYour Output Shape: torch.Size([50, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 100])\tExpected Output Shape: torch.Size([1, 3])\tYour Output Shape: torch.Size([1, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 100])\tExpected Output Shape: torch.Size([50, 3])\tYour Output Shape: torch.Size([50, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 10])\tExpected Output Shape: torch.Size([1, 2])\tYour Output Shape: torch.Size([1, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 10])\tExpected Output Shape: torch.Size([50, 2])\tYour Output Shape: torch.Size([50, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 100])\tExpected Output Shape: torch.Size([1, 2])\tYour Output Shape: torch.Size([1, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 100])\tExpected Output Shape: torch.Size([50, 2])\tYour Output Shape: torch.Size([50, 2])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 10])\tExpected Output Shape: torch.Size([1, 3])\tYour Output Shape: torch.Size([1, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 10])\tExpected Output Shape: torch.Size([50, 3])\tYour Output Shape: torch.Size([50, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([1, 100])\tExpected Output Shape: torch.Size([1, 3])\tYour Output Shape: torch.Size([1, 3])\n","\tPASSED\t Init Input: {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}\tForward Input Shape: torch.Size([50, 100])\tExpected Output Shape: torch.Size([50, 3])\tYour Output Shape: torch.Size([50, 3])\n"]}],"source":["### DO NOT EDIT ###\n","\n","def makeCnnSanityBatch(test_params):\n","    batch_size = test_params['batch_size']\n","    max_len = test_params['max_len']\n","    new_test_params = {k:v for k,v in test_params.items() if k not in {'batch_size', 'max_len'}}\n","    batch = torch.randint(0, new_test_params['vocab_size'], (batch_size,max_len))\n","    return batch, new_test_params\n","\n","if __name__ == '__main__':\n","    # Test init\n","    cnn_init_inputs = [{'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 32, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0}, {'vocab_size': 1000, 'embed_size': 32, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0}]\n","    cnn_init_expected_outputs = [22434, 22531, 22434, 22531, 23874, 23939, 23874, 23939, 41730, 42115, 41730, 42115, 47490, 47747, 47490, 47747, 44578, 44675, 44578, 44675, 47554, 47619, 47554, 47619, 82306, 82691, 82306, 82691, 94210, 94467, 94210, 94467]\n","\n","    sanityCheckModel(cnn_init_inputs, CNN, cnn_init_expected_outputs, \"init\")\n","    print()\n","\n","    # Test forward\n","    cnn_forward_inputs = [{'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [3, 4, 5], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 1, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 2, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 10, 'batch_size': 50}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 1}, {'vocab_size': 1000, 'embed_size': 16, 'out_channels': 128, 'filter_heights': [5, 10], 'stride': 3, 'dropout': 0, 'num_classes': 3, 'pad_idx': 0, 'max_len': 100, 'batch_size': 50}]\n","    cnn_forward_expected_outputs = [torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 2]), torch.Size([50, 2]), torch.Size([1, 3]), torch.Size([50, 3]), torch.Size([1, 3]), torch.Size([50, 3])]\n","\n","    sanityCheckModel(cnn_forward_inputs, CNN, cnn_forward_expected_outputs, \"forward\", makeCnnSanityBatch)"]},{"cell_type":"markdown","metadata":{"id":"FupiBIfasCu_"},"source":["## Train CNN Model\n","\n","First, we initialize the train and test <b>dataloaders</b>. A dataloader is responsible for providing batches of data to your model. Notice how we first instantiate datasets for the train and test data, and that we use the training vocabulary for both.\n","\n","You do not need to edit this cell."]},{"cell_type":"code","execution_count":142,"metadata":{"id":"J2QYl334n9ON","executionInfo":{"status":"ok","timestamp":1678667013842,"user_tz":300,"elapsed":4327,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    THRESHOLD = 5 # Don't change this\n","    MAX_LEN = 200 # Don't change this\n","    BATCH_SIZE = 32 # Feel free to try other batch sizes\n","\n","    train_dataset = TextDataset(train_data, 'train', THRESHOLD, MAX_LEN)\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n","\n","    test_dataset = TextDataset(test_data, 'test', THRESHOLD, MAX_LEN, train_dataset.idx2word, train_dataset.word2idx)\n","    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1, drop_last=False)"]},{"cell_type":"markdown","metadata":{"id":"AvsctopWmeoY"},"source":["Now we provide you with a function that takes your model and trains it on the data.\n","\n","You do not need to edit this cell. However, you may want to write code to save your model periodically, as Colab connections are not permanent. See the tutorial here if you wish to do this: https://pytorch.org/tutorials/beginner/saving_loading_models.html."]},{"cell_type":"code","execution_count":143,"metadata":{"id":"LD-Jj2rUFOzr","executionInfo":{"status":"ok","timestamp":1678667013843,"user_tz":300,"elapsed":11,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","from tqdm.notebook import tqdm\n","\n","def train_cnn_model(model, num_epochs, data_loader, optimizer, criterion):\n","    print('Training Model...')\n","    model.train()\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        epoch_acc = 0\n","        for texts, labels in tqdm(data_loader):\n","            texts = texts.to(DEVICE) # shape: [batch_size, MAX_LEN]\n","            labels = labels.to(DEVICE) # shape: [batch_size]\n","\n","            optimizer.zero_grad()\n","\n","            output = model(texts)\n","            acc = accuracy(output, labels)\n","            \n","            loss = criterion(output, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        print('[TRAIN]\\t Epoch: {:2d}\\t Loss: {:.4f}\\t Train Accuracy: {:.2f}%'.format(epoch+1, epoch_loss/len(data_loader), 100*epoch_acc/len(data_loader)))\n","    print('Model Trained!\\n')"]},{"cell_type":"markdown","metadata":{"id":"FyIZS0WUhFA6"},"source":["Here are some other helper functions we will need."]},{"cell_type":"code","execution_count":144,"metadata":{"id":"zVP2scuyhG5f","executionInfo":{"status":"ok","timestamp":1678667013844,"user_tz":300,"elapsed":10,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","def accuracy(output, labels):\n","    \"\"\"\n","    Returns accuracy per batch\n","    output: Tensor [batch_size, n_classes]\n","    labels: LongTensor [batch_size]\n","    \"\"\"\n","    preds = output.argmax(dim=1) # find predicted class\n","    correct = (preds == labels).sum().float() # convert into float for division \n","    acc = correct / len(labels)\n","    return acc"]},{"cell_type":"markdown","metadata":{"id":"YjvX5c6Isw9e"},"source":["Now you can instantiate your model. We provide you with some recommended hyperparameters; you should be able to get the desired accuracy with these, but feel free to play around with them."]},{"cell_type":"code","execution_count":145,"metadata":{"id":"M5UtdjGDuBty","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678667015857,"user_tz":300,"elapsed":215,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"b93531ce-6d2f-490d-82f1-49782943eb13"},"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 3,879,746 trainable parameters\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    cnn_model = CNN(vocab_size = train_dataset.vocab_size, # Don't change this\n","                embed_size = 128, \n","                out_channels = 64, \n","                filter_heights = [2, 3, 4], \n","                stride = 1, \n","                dropout = 0.5, \n","                num_classes = 2, # Don't change this\n","                pad_idx = train_dataset.word2idx[CNN_PAD]) # Don't change this\n","\n","    # Put your model on the device (cuda or cpu)\n","    cnn_model = cnn_model.to(DEVICE)\n","    \n","    print('The model has {:,d} trainable parameters'.format(count_parameters(cnn_model)))"]},{"cell_type":"markdown","metadata":{"id":"SeHpqw6zvkhI"},"source":["Next, we create the **criterion**, which is our loss function: it is a measure of how well the model matches the empirical distribution of the data. We use cross-entropy loss (https://en.wikipedia.org/wiki/Cross_entropy).\n","\n","We also define the **optimizer**, which performs gradient descent. We use the Adam optimizer (https://arxiv.org/pdf/1412.6980.pdf), which has been shown to work well on these types of models."]},{"cell_type":"code","execution_count":146,"metadata":{"id":"FoeyQL4PoNoH","executionInfo":{"status":"ok","timestamp":1678667018748,"user_tz":300,"elapsed":791,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","import torch.optim as optim\n","\n","if __name__=='__main__':    \n","    LEARNING_RATE = 5e-4 # Feel free to try other learning rates\n","\n","    # Define the loss function\n","    criterion = nn.CrossEntropyLoss().to(DEVICE)\n","\n","    # Define the optimizer\n","    optimizer = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"markdown","metadata":{"id":"RopLfAJ9wOHN"},"source":["Finally, we can train the model. If the model is implemented correctly and you're using the GPU, this cell should take around <b>4 minutes</b> (or less). Feel free to change the number of epochs."]},{"cell_type":"code","execution_count":147,"metadata":{"id":"lPOs1FifoNoN","colab":{"base_uri":"https://localhost:8080/","height":563,"referenced_widgets":["6a31470913cd4867908d27f523219cf9","7116bcc36c014d3cb627ad15f121d9b6","e65b311f35814ba783786fd21d6b3f6f","cbb23551f13e47c5aec5b9f861115f96","569a38ef8a854eca8a08cf05aa1fb3ea","f530fbb53cdf4587bfd227ca2d8e71e1","b335241211f74ccaa9be3619b8bc18c2","a9970e894eb04abe8c2d7362341845af","c9c53e46af9d4643bf1946f2269522d2","455b39db517b4df4b9b6403b85217217","7ad6ac8675a246488243e0ad24e95dc7","9c429465ff9e46ac86b84138313b0426","fd0c1f6c44bf421591034b99ea84b78c","6905a4b9d25746cba4b250b3b49c4dec","c6a37e2a4c3b4204a07de8a4efd2d8cd","c7f16348a267479d874668161a0c6725","b8436cef53a741c29ae99621ee589d6e","29dd9cd352284030aeca4246716ff725","d90768a4e2e14e5aa17ce91c2c7c54c4","7a65d341e7934e8b8308a5158df0dedb","2a50d1f94a004b37b1e38e8ca7d1f941","4f493860c81645b4a615c7730e8b9283","c909b8546b2e4d9cae3874729a0c9a99","9568f87bd1e049c0997a086bc87b340c","a57723e34aa24080b53f154a23529842","062f7ae71529412baacf686670a902a0","3d7eec73e3894e12b30929ebf8c611cc","a4bd2b2935f64adeb92b97e462e6c4f7","79db8802ad6243ea8a87fffbedd70cb2","d83690857e3e4013abc16657b969a65f","705e873e7f004a728b019e9685ebb353","c44d89c6230447a391a59a85b4e7415d","fb08e7919db644c1ba80d32123c0b6c9","f6e92bc339c149fcad3286b21f6229b3","07262706c9a346cfa93d766868ba688d","7fa350023e504f79b1408ab422255bb4","6ea1dd64b6704a66a3dd7aad9dd3aa6e","16f2dfd48eff400bb3940c80cf4a38e3","ba40ea04f49147c0864eea2d149c66ee","74d97f18b2964b4386e598f3b44d723a","a021aeec005446d2ac0244ee85810af4","d3b87fe75e744cb99dfa4e120c4a5113","9f4ccfe17528435b8a7c3e45ed2b7973","d1965cd81e874620ac13b0519d68808f","cfb9aa40c89f4c36a48c0947a4702df4","337660f9157a4707a9400d802272953c","2d27140ffd8a45558cafae5d59db14b0","401a771565cb44cc823a466e4d5bb17a","88113ab3ae1c465aad47cc85a591f5e5","5a78255fba384295abfbc9003c544c67","ad66cb8a13124405a85c66c45ec78ff1","2b67113f89b147239849a6a4302ed0c2","9f371a117a094c8da38cf2e5bd103d2a","d5aba3bc6ed14fa4ae15cba9a96886a3","1d76bbc0618b424c8c1bb6e19df4a421","10354edf1b3848e9b8dcf9847942d286","e60969b4871f4361a9f4f3e85ce66943","c270afcd68d14e3cba5cdae6fe8d93ca","745eb4c1db3247c8b2aa5e9c1c2e0a49","8dcbb02c04884615b02349e77963cd70","307d4e5fc7874ff59f426cea7c3afada","881977ad917b45ee97246616a462f7bf","da89f56e4834484fa15c8b9354cd6357","51a34b0bbc8e4e6b8a379b54377e030c","cf4cc6cc1fe44bc589c1556dc53e7591","4eef3ef6e9de48a1877e8abd165208ed","0f027b20d5e748acb738bae3ef0c592e","a58cb5e49f314da7bf01d0b5bc92315a","32d46d611ee447cea20a86e020d83cc5","ec603ed6366d4c228c067ec9e44f346f","cdd9abbd3bfa40089f0062931b94d789","2c6bad8ea50c4d13b8fd29dee3875789","63daedc83554419a9b58c641791cda1f","c0f3d343231c433786b6e4a045163a2a","e95668fd87c24139be7310586af06c9a","46a6ea3e9a9b480eaa6bddf1c4143660","aa2decf242c54520a0162ca429b53154","e46fbab001874c10b5f9f56c664876e3","70a3aaa1c0f34b79b2fb42cfafb58868","9f83ae67fcf344018e041a2910144d4d","feeaa2dd200c40f7b6fb8d76e1beb8bc","fe8762005099450bb78c2896203e60b1","40a7df3fd4ac4867bb7888315ddcbc36","ca9b952e0adb488096852ddbc41d90d1","7d4f54e8e5ec48afae56afd8a7455130","b545457a86a44565af05c1d24062eed6","7ded3619e872440a8cb061a35690abc4","d3ac5fe7f13e49c690cc9f5b32ac16ae","57974af5bb29412f9135a86c64c1530e","56a579a960e54053b8a4e6561f3e9962","1ecaaf9c574c49cba36bc0a852e180d9","22ea6aa1c98c4cf5b1dc1796378f3987","748746350c914591a56d6a805da9d445","b5468bdad0714ce19edf6aa548c67e4b","238e80ba21144faea0d9c40ca71f2df3","2daf1d9b6f924dbfb3c42179343f13d5","185d435cdcfc48d5b62746c561dd1c3a","1c20280796c74fa1aef386d044055f34","592eca8428e54db2bf388c1573d2401b","7a5e842084ea44a09c01103248b3910a","89ce7a6968744b44ac377e14399ca5e1","ab369329814f4509b24c6d42783955d9","f1bba8eff638447ba79ccddb848feec4","8540b9797e474a74ae1b7c9c68c23916","02aa66ef598a4cc1bac1df4ef5e133d0","1fe6d3cd5eac44c0a340bca9bf83770a","8976513619494847a915a4a1d6a8fcb0","328e0be144544d079f4e7138b139a5b1","81b99b9b92ed47b287746ec6babac7a7","6297e10ec7c94e5494eee1490cac1112"]},"executionInfo":{"status":"ok","timestamp":1678667278062,"user_tz":300,"elapsed":258183,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"430a1b47-31b9-4897-b179-4974634fc660"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Model...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/625 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a31470913cd4867908d27f523219cf9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  1\t Loss: 0.6959\t Train Accuracy: 59.83%\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/625 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c429465ff9e46ac86b84138313b0426"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  2\t Loss: 0.5583\t Train Accuracy: 70.94%\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/625 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c909b8546b2e4d9cae3874729a0c9a99"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  3\t Loss: 0.4988\t Train Accuracy: 75.16%\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/625 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6e92bc339c149fcad3286b21f6229b3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  4\t Loss: 0.4561\t Train Accuracy: 78.44%\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/625 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfb9aa40c89f4c36a48c0947a4702df4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  5\t Loss: 0.4200\t Train Accuracy: 80.56%\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/625 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10354edf1b3848e9b8dcf9847942d286"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  6\t Loss: 0.3849\t Train Accuracy: 82.64%\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/625 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f027b20d5e748acb738bae3ef0c592e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  7\t Loss: 0.3519\t Train Accuracy: 84.44%\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/625 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e46fbab001874c10b5f9f56c664876e3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  8\t Loss: 0.3122\t Train Accuracy: 86.59%\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/625 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57974af5bb29412f9135a86c64c1530e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch:  9\t Loss: 0.2727\t Train Accuracy: 88.51%\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/625 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a5e842084ea44a09c01103248b3910a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[TRAIN]\t Epoch: 10\t Loss: 0.2407\t Train Accuracy: 90.17%\n","Model Trained!\n","\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':    \n","    N_EPOCHS = 10 # Feel free to change this\n","    \n","    # train model for N_EPOCHS epochs\n","    train_cnn_model(cnn_model, N_EPOCHS, train_loader, optimizer, criterion)"]},{"cell_type":"markdown","metadata":{"id":"Q-OJbZ72t6Yq"},"source":["## Evaluate CNN Model [20 points]\n","\n","Now that we have trained a model for text classification, it is time to evaluate it. We have provided you with a function to do this; you do not need to modify anything.\n","\n","To pass the autograder for the CNN, you will need to achieve **82% accuracy** on the hidden test set on Gradescope. Note that the Gradescope test set is very similar, and the accuracies between the two datasets should be comparable.\n","\n","<font color='green'><b>Hint:</b> If you receive close to 82% accuracy in the notebook but close to 50% accuracy in the autograder, then the most likely causes are:\n","1. You uploaded an untrained model checkpoint. Make sure you save the model after it is trained.\n","2. Your `TextDataset` class is not deterministic in that the `word2idx` and `idx2word` mappings are not necessarily in the same order when the class is instantiated multiple times. This is a problem as your trained CNN will expect the words in the order seen in this notebook, but the autograder will be using a different ordering. If this is your issue, reimplement the `TextDataset` class so that it is deterministic, and then retrain and upload your model.</font>"]},{"cell_type":"code","execution_count":148,"metadata":{"id":"vTiiYDZIF--7","executionInfo":{"status":"ok","timestamp":1678667428152,"user_tz":300,"elapsed":190,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}}},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","import random\n","\n","def evaluate(model, data_loader, criterion, use_tqdm=False):\n","    print('Evaluating performance on the test dataset...')\n","    has_printed=False\n","    model.eval()\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    all_predictions = []\n","    iterator = tqdm(data_loader) if use_tqdm else data_loader\n","    total = 0\n","    for texts, labels in iterator:\n","        bs = texts.shape[0]\n","        total += bs\n","        texts = texts.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","        \n","        output = model(texts)\n","        acc = accuracy(output, labels) * len(labels)\n","        pred = output.argmax(dim=1)\n","        all_predictions.append(pred)\n","        \n","        loss = criterion(output, labels) * len(labels)\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","        if random.random() < 0.0015 and bs == 1:\n","            if not has_printed: print(\"\\nSOME PREDICTIONS FROM THE MODEL:\")\n","            print(\"Input: \"+' '.join([data_loader.dataset.idx2word[idx] for idx in texts[0].tolist() if idx not in {data_loader.dataset.word2idx[CNN_PAD], data_loader.dataset.word2idx[CNN_END]}]))\n","            print(\"Prediction:\", pred.item(), '\\tCorrect Output:', labels.item(), '\\n')\n","            has_printed=True\n","\n","    full_acc = 100*epoch_acc/total\n","    full_loss = epoch_loss/total\n","    print('[TEST]\\t Loss: {:.4f}\\t Accuracy: {:.2f}%'.format(full_loss, full_acc))\n","    predictions = torch.cat(all_predictions)\n","    return predictions, full_acc, full_loss"]},{"cell_type":"code","execution_count":149,"metadata":{"id":"Z718w8e0oNoS","colab":{"base_uri":"https://localhost:8080/","height":555,"referenced_widgets":["9f71662c4e98461d89fdbd9c743c35fc","61ed9679c9d2427fb0b6bdfd7af82542","eb9803973f624aab820bb4e13a4cbbc5","3633b82cc3364fc2b71d5a8073f25308","b35294fb1e1c4ccaa9b6f3ea7b0e4bc0","1811a2851be54e0284526905f5af5a92","cb686bee389544b988a3c5e64bab2d0a","bf85d62916ed4ae7ad83f3ff284fc42a","ebbccd314b3b4a2da42bc716747dc7a0","9d4cbc352da04b058544ea8e0baf6bfb","59bbe3036ffb423886c61a396202bf35"]},"executionInfo":{"status":"ok","timestamp":1678667447630,"user_tz":300,"elapsed":17636,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"23c9aff1-a152-4ae2-dc73-cbae0b04e259"},"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating performance on the test dataset...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f71662c4e98461d89fdbd9c743c35fc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","SOME PREDICTIONS FROM THE MODEL:\n","Input: i'd been following this films progress for quite some time so perhaps expected a little too much . i consider both gillian anderson and danny dyer to be good at what they do and was interested to see what dan reed could come up with but unfortunately it just didn't work for me.<br /><br />the problem lies in the fact that the film doesn't really seem to understand which genre it's falling into and as such it fails to impress on drama , horror and thriller elements because rather than focusing on one of them and doing it well it's a bit of a jack of all trades and master of none.<br /><br />the premise ( as with most revenge films ) is simple , couple meet and go out , something bad happens and they get their revenge it's a simple formula and one that many directors have handled expertly over the years . <UNK> in this case it's as if dan reed thought , \" it'd be great to do one of those revenge films that goes a little deeper by showing a more human side to all the characters and delving into their mental state in more\n","Prediction: 0 \tCorrect Output: 0 \n","\n","Input: my wife and i rented this movie because some people had drawn parallels between it and \" office space\" . blockbuster and imdb even had it as an \" also recommended \" selection if you liked \" office <UNK> /><br />now , i've seen office space probably 15 or 20 times . i love it . it's probably one of my 10 favorite movies . witty , humorous , and featuring characters that remind me of people i've worked with over the years . \" <UNK> tunnel \" is similar to \" office space \" in that they are both films . that's where the similarity ends . we sat through probably the first 50 minutes of <UNK> , giving it the benefit of the doubt , hoping , nay , <UNK> that it would get better . it didn't . we couldn't take it any more , and stopped the tape . thank god it was a free rental . i'd have been <UNK> if we'd actually paid for it . we should be <UNK> for having to sit through it . now , since we didn't see the end , perhaps it miraculously comes together and redeems itself\n","Prediction: 1 \tCorrect Output: 0 \n","\n","Input: movie about a small town with equal numbers of mormons and <UNK> . new family moves in , cue the <UNK> dialog , mediocre acting , green <UNK> salad with <UNK> <UNK> , and every other ' inside mormon joke ' known to man . anyone outside the mormon culture will have a hard time <UNK> this movie . anyone inside the mormon culture will be slightly amused with a chuckle here and there . you'll be much better off watching <UNK> other movies ( napoleon dynamite , etc.. ) than trying to sit through this one . the acting is mediocre . jared hess has had his hands on much more quality films like \" saints and <UNK> , and \" napoleon <UNK> . i would recommend both movies over this <UNK> .\n","Prediction: 0 \tCorrect Output: 0 \n","\n","Input: the night listener ( 2006 ) <UNK> robin williams , toni collette , bobby <UNK> , rory culkin , joe morton , sandra oh , john <UNK> , lisa <UNK> , becky ann baker . ( dir : patrick <UNK> ) <br /><br <UNK> <UNK> gives williams a stand-out low-key performance.<br /><br />what is it about celebrities and fans ? what is the near paranoia one associates with the other and why is it almost the norm ? <br /><br />in the latest <UNK> fan scenario , based on true events no less , williams stars as a <UNK> personality named gabriel no one , who reads stories he's penned over the airwaves and has accumulated an interesting fan in the form of a young boy named pete <UNK> ( culkin ) who has submitted a manuscript about the travails of his troubled youth to no one's editor <UNK> ( morton ) who gives it to no one to read for himself . <br /><br />no one is naturally disturbed but ultimately intrigued about the nightmarish existence of pete being abducted and sexually abused for years until he was finally rescued by a nurse named donna ( collette giving an\n","Prediction: 0 \tCorrect Output: 1 \n","\n","Input: this is a wonderful film . the non-stop patter takes several <UNK> to fully appreciate . the musical productions of busby berkeley will never be duplicated . i think this movie easily outdoes all of his other efforts . joan blondell and james cagney are incredible together . some of the humor would almost push the boundaries of today's movies . put rational explanation of how they did it aside and enjoy it for the spectacle that it is .\n","Prediction: 1 \tCorrect Output: 1 \n","\n","Input: this film takes you to another time when there was a different pace to everyday life . we get an idea how families had to deal with the war and how quickly we sent young men off to fight . a very touching look at the past and a reminder that casualties of war don't just happen on the <UNK> /><br />luckily many of us have never had to go through what our <UNK> , grandparents or parents went through during a war . this film , i think , is a small thank you . peter <UNK> looks amazingly like a young peter o'toole and russell crowe is absolutely charming and as australian as he can be . it's definitely worth listening to him recite \" high flight \" and makes me wonder what he might accomplish with shakespeare .\n","Prediction: 1 \tCorrect Output: 1 \n","\n","Input: though not in the whole film , andy griffith again plays his role best in this cbs tv-movie . the plot is <UNK> character dies and his last wish is that his wife and kids <UNK> his ashes is the place he named ( mountains <UNK> . though it will never be seen on tv and never be released on video , if you do get the chance to watch <UNK> it .\n","Prediction: 1 \tCorrect Output: 1 \n","\n","Input: ok clara bow silent film from 1927 , it's a spin-off of rain , with bow playing the <UNK> wild daughter of the local pineapple king who falls in love with the staid english <UNK> brook . bow <UNK> with the local widow ( <UNK> <UNK> ) for his attentions , but both women get a big surprise when his wife shows up ( patricia <UNK> . the predatory wife is ready for a divorce until she discovers he might be on the verge of a fortune . bow settles her hash fast.<br /><br <UNK> has personality to spare and has a few great scenes : her opening nude bath , her <UNK> in a grass skirt , and the dog rescue scene with bow and brook doing their own <UNK> /><br />note : the imdb credit list is wrong . the film credits ( from the dvd i have ) list patricia <UNK> as playing mrs . <UNK> margaret <UNK> as listed on imdb .\n","Prediction: 1 \tCorrect Output: 1 \n","\n","[TEST]\t Loss: 0.3967\t Accuracy: 82.34%\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    evaluate(cnn_model, test_loader, criterion, use_tqdm=True) # Compute test data accuracy"]},{"cell_type":"markdown","metadata":{"id":"8WQAV6O2xHvS"},"source":["# What to Submit\n","\n","To submit the assignment, download this notebook as a <TT>.py</TT> file. You can do this by going to <TT>File > Download > Download .py</TT>. Then (optionally) rename it to `hwk2.py`.\n","\n","You will also need to save the `cnn_model` (you do not need to save anything additional for your word embeddings). You can run the cell below to do this. After you save the files to your Google Drive, you need to manually download the files to your computer, and then submit them to the autograder.\n","\n","You will submit the following files to the autograder:\n","1.   `hwk2.py`, the download of this notebook as a `.py` file (**not** a `.ipynb` file)\n","1.   `cnn.pt`, the saved version of your `cnn_model`"]},{"cell_type":"code","execution_count":150,"metadata":{"id":"abbbMNi8X_ai","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678667494081,"user_tz":300,"elapsed":22892,"user":{"displayName":"Pulkit Thakar","userId":"00623592257716571616"}},"outputId":"bfb37f66-587d-4bb4-e954-5a43e20d4d83"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","Saving CNN model....\n","\n","Done!\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    print()\n","\n","    try:\n","        cnn_model is None\n","        cnn_exists = True\n","    except:\n","        cnn_exists = False\n","\n","    if cnn_exists:\n","        print(\"Saving CNN model....\") \n","        torch.save(cnn_model, \"drive/My Drive/cnn.pt\")\n","    \n","    print(\"\\nDone!\")"]},{"cell_type":"code","source":[],"metadata":{"id":"OsJGdQ7iYNfo"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1kynw8BLvBjQhzK5L4lalDS0wOFYrPDZr","timestamp":1678176998109}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f79bbb1717ac4b09a100b79ad4763dcf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d4dc93a37f0b4b76a16d1ef638a9031b","IPY_MODEL_cc468caf032d440abf3e0ddadc7523d9","IPY_MODEL_013d1454cf5c4e3787e0496aad0c3dec"],"layout":"IPY_MODEL_9f562683655a46cdb19ab4af8e245541"}},"d4dc93a37f0b4b76a16d1ef638a9031b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f382565451294a5ea455988173dae32c","placeholder":"​","style":"IPY_MODEL_eac22a8b45ba49b1bd2fcadc4cb32774","value":"100%"}},"cc468caf032d440abf3e0ddadc7523d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46418646ceaa48328a70731e34115ccd","max":1684,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55afccadee634fa797536a02d9ea769e","value":1684}},"013d1454cf5c4e3787e0496aad0c3dec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_867b1d4881ad4c3993997042e55d3565","placeholder":"​","style":"IPY_MODEL_5a3f7b2eb1994352bd3312019d668cf1","value":" 1684/1684 [00:31&lt;00:00, 43.56it/s]"}},"9f562683655a46cdb19ab4af8e245541":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f382565451294a5ea455988173dae32c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eac22a8b45ba49b1bd2fcadc4cb32774":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46418646ceaa48328a70731e34115ccd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55afccadee634fa797536a02d9ea769e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"867b1d4881ad4c3993997042e55d3565":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a3f7b2eb1994352bd3312019d668cf1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8894d23282624ca08414f84dd1566ab8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6068c1ab27304ea2832021d78496be08","IPY_MODEL_0352f4bfcb7b4170b9abd07ee4ea53c7","IPY_MODEL_5ed85d85085a48d89d36e3dbca0eaca9"],"layout":"IPY_MODEL_d33979082de24790a4e01ae8f8e278c1"}},"6068c1ab27304ea2832021d78496be08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4593d1435b3459c847f2f7c83401113","placeholder":"​","style":"IPY_MODEL_ead5b307918e4b589229c356e224ebe0","value":"100%"}},"0352f4bfcb7b4170b9abd07ee4ea53c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_870eb60a329242ed80110532a85d59f8","max":1684,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ceebc946ce74b848688d5ca23b443c9","value":1684}},"5ed85d85085a48d89d36e3dbca0eaca9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e66e41079d454ebcb4204df03ab6da67","placeholder":"​","style":"IPY_MODEL_0821400d497f45fd8491b34f5111f973","value":" 1684/1684 [00:33&lt;00:00, 47.58it/s]"}},"d33979082de24790a4e01ae8f8e278c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4593d1435b3459c847f2f7c83401113":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ead5b307918e4b589229c356e224ebe0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"870eb60a329242ed80110532a85d59f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ceebc946ce74b848688d5ca23b443c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e66e41079d454ebcb4204df03ab6da67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0821400d497f45fd8491b34f5111f973":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fac86ed3326448687b8d9c675761fa7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_713777f59490431dbacb33c7ad15965d","IPY_MODEL_73572656342c4c189da89a2a8be9b661","IPY_MODEL_75b98b78f896407b9365234c48a57012"],"layout":"IPY_MODEL_f554e5d42e084fdf99b899af7193f2a8"}},"713777f59490431dbacb33c7ad15965d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_403fb67e79ac4856b5f002211c0f181f","placeholder":"​","style":"IPY_MODEL_026ab5def91f4b7c9cba03556cbbf735","value":"100%"}},"73572656342c4c189da89a2a8be9b661":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfb1ef291cf84b50acbad15f86e548de","max":1684,"min":0,"orientation":"horizontal","style":"IPY_MODEL_30b33f0d81be4c54b9a4131e37e50489","value":1684}},"75b98b78f896407b9365234c48a57012":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4134d97d3daf4ce7965c1200a623d794","placeholder":"​","style":"IPY_MODEL_630959892dbb4162b7f49c7cf791fd0e","value":" 1684/1684 [00:31&lt;00:00, 40.01it/s]"}},"f554e5d42e084fdf99b899af7193f2a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"403fb67e79ac4856b5f002211c0f181f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"026ab5def91f4b7c9cba03556cbbf735":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bfb1ef291cf84b50acbad15f86e548de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30b33f0d81be4c54b9a4131e37e50489":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4134d97d3daf4ce7965c1200a623d794":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"630959892dbb4162b7f49c7cf791fd0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acb528e25c2f4fefaf12e341c7214e2a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc73c52824074772b0ca2c6ec1e61075","IPY_MODEL_40ed271d954e41a5a7c3a89bd49c094a","IPY_MODEL_424f299132ed4380a69abd510c4acd3a"],"layout":"IPY_MODEL_9044f7c481154fbab4e77e76426c4286"}},"dc73c52824074772b0ca2c6ec1e61075":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48cf07f45b894a02aa4ccbcdba395164","placeholder":"​","style":"IPY_MODEL_7395cc0c325e43958c02373cad5dc630","value":"100%"}},"40ed271d954e41a5a7c3a89bd49c094a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_876ffcc2c30040719e88a031e5d35566","max":1684,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f83f106fbbf407cb9f0ea2dda3e0ac2","value":1684}},"424f299132ed4380a69abd510c4acd3a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_218105a3a124413684f397eb46d722c6","placeholder":"​","style":"IPY_MODEL_75d3affeb7014a27871635a0c7adfe94","value":" 1684/1684 [00:30&lt;00:00, 44.96it/s]"}},"9044f7c481154fbab4e77e76426c4286":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48cf07f45b894a02aa4ccbcdba395164":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7395cc0c325e43958c02373cad5dc630":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"876ffcc2c30040719e88a031e5d35566":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f83f106fbbf407cb9f0ea2dda3e0ac2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"218105a3a124413684f397eb46d722c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75d3affeb7014a27871635a0c7adfe94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbbeca40f08b4ea285c49b78a9fb9cb1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5dd31658026f45db849a1945719be98b","IPY_MODEL_74c545d8e1284026857c8ebeda76f96e","IPY_MODEL_41b0ebac551d4d9d97b8e7981cccbf48"],"layout":"IPY_MODEL_154431ab1ace4ce59763929b09372e27"}},"5dd31658026f45db849a1945719be98b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea5a060aace746fabc9a5e1e82512dce","placeholder":"​","style":"IPY_MODEL_8e105495429249fdbf142feb04746667","value":"100%"}},"74c545d8e1284026857c8ebeda76f96e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3c3b84467b648cfa6653ceac4daf93f","max":1684,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c495eaee5bf144d494c7805d8b9721f7","value":1684}},"41b0ebac551d4d9d97b8e7981cccbf48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1177e719d93a47fca8ac581a8b6e6a8b","placeholder":"​","style":"IPY_MODEL_b21c71a65f61498b8b7c687d748cc3ab","value":" 1684/1684 [00:31&lt;00:00, 64.33it/s]"}},"154431ab1ace4ce59763929b09372e27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea5a060aace746fabc9a5e1e82512dce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e105495429249fdbf142feb04746667":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3c3b84467b648cfa6653ceac4daf93f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c495eaee5bf144d494c7805d8b9721f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1177e719d93a47fca8ac581a8b6e6a8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b21c71a65f61498b8b7c687d748cc3ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e813e7b0f5046229b59ec92bd2f9319":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_82dbd7f076e6476c9a359f2615ccc8ec","IPY_MODEL_3df1bad06eb4402aaf9cac57f0c07222","IPY_MODEL_56ec8bf1bb874f998b7ee641c9e4edcd"],"layout":"IPY_MODEL_6a45e189376d4836ab2604fa303bf632"}},"82dbd7f076e6476c9a359f2615ccc8ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3d74309e611462581644363a70460ce","placeholder":"​","style":"IPY_MODEL_c1267888d3fa439da37ff48b0ae6189c","value":"100%"}},"3df1bad06eb4402aaf9cac57f0c07222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f66aeaadfab40d4b1fb3f474c463727","max":1684,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a59c852024a04ddea18fef86acbd7398","value":1684}},"56ec8bf1bb874f998b7ee641c9e4edcd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55ca83aef37d4f47ab9231af7c4a02df","placeholder":"​","style":"IPY_MODEL_a8bc0106b7914761b0484b46afb99b7e","value":" 1684/1684 [00:32&lt;00:00, 39.35it/s]"}},"6a45e189376d4836ab2604fa303bf632":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3d74309e611462581644363a70460ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1267888d3fa439da37ff48b0ae6189c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f66aeaadfab40d4b1fb3f474c463727":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a59c852024a04ddea18fef86acbd7398":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"55ca83aef37d4f47ab9231af7c4a02df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8bc0106b7914761b0484b46afb99b7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a31470913cd4867908d27f523219cf9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7116bcc36c014d3cb627ad15f121d9b6","IPY_MODEL_e65b311f35814ba783786fd21d6b3f6f","IPY_MODEL_cbb23551f13e47c5aec5b9f861115f96"],"layout":"IPY_MODEL_569a38ef8a854eca8a08cf05aa1fb3ea"}},"7116bcc36c014d3cb627ad15f121d9b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f530fbb53cdf4587bfd227ca2d8e71e1","placeholder":"​","style":"IPY_MODEL_b335241211f74ccaa9be3619b8bc18c2","value":"100%"}},"e65b311f35814ba783786fd21d6b3f6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9970e894eb04abe8c2d7362341845af","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c9c53e46af9d4643bf1946f2269522d2","value":625}},"cbb23551f13e47c5aec5b9f861115f96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_455b39db517b4df4b9b6403b85217217","placeholder":"​","style":"IPY_MODEL_7ad6ac8675a246488243e0ad24e95dc7","value":" 625/625 [00:29&lt;00:00, 25.31it/s]"}},"569a38ef8a854eca8a08cf05aa1fb3ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f530fbb53cdf4587bfd227ca2d8e71e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b335241211f74ccaa9be3619b8bc18c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9970e894eb04abe8c2d7362341845af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9c53e46af9d4643bf1946f2269522d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"455b39db517b4df4b9b6403b85217217":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ad6ac8675a246488243e0ad24e95dc7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c429465ff9e46ac86b84138313b0426":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd0c1f6c44bf421591034b99ea84b78c","IPY_MODEL_6905a4b9d25746cba4b250b3b49c4dec","IPY_MODEL_c6a37e2a4c3b4204a07de8a4efd2d8cd"],"layout":"IPY_MODEL_c7f16348a267479d874668161a0c6725"}},"fd0c1f6c44bf421591034b99ea84b78c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8436cef53a741c29ae99621ee589d6e","placeholder":"​","style":"IPY_MODEL_29dd9cd352284030aeca4246716ff725","value":"100%"}},"6905a4b9d25746cba4b250b3b49c4dec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d90768a4e2e14e5aa17ce91c2c7c54c4","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7a65d341e7934e8b8308a5158df0dedb","value":625}},"c6a37e2a4c3b4204a07de8a4efd2d8cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a50d1f94a004b37b1e38e8ca7d1f941","placeholder":"​","style":"IPY_MODEL_4f493860c81645b4a615c7730e8b9283","value":" 625/625 [00:25&lt;00:00, 25.33it/s]"}},"c7f16348a267479d874668161a0c6725":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8436cef53a741c29ae99621ee589d6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29dd9cd352284030aeca4246716ff725":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d90768a4e2e14e5aa17ce91c2c7c54c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a65d341e7934e8b8308a5158df0dedb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a50d1f94a004b37b1e38e8ca7d1f941":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f493860c81645b4a615c7730e8b9283":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c909b8546b2e4d9cae3874729a0c9a99":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9568f87bd1e049c0997a086bc87b340c","IPY_MODEL_a57723e34aa24080b53f154a23529842","IPY_MODEL_062f7ae71529412baacf686670a902a0"],"layout":"IPY_MODEL_3d7eec73e3894e12b30929ebf8c611cc"}},"9568f87bd1e049c0997a086bc87b340c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4bd2b2935f64adeb92b97e462e6c4f7","placeholder":"​","style":"IPY_MODEL_79db8802ad6243ea8a87fffbedd70cb2","value":"100%"}},"a57723e34aa24080b53f154a23529842":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d83690857e3e4013abc16657b969a65f","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_705e873e7f004a728b019e9685ebb353","value":625}},"062f7ae71529412baacf686670a902a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c44d89c6230447a391a59a85b4e7415d","placeholder":"​","style":"IPY_MODEL_fb08e7919db644c1ba80d32123c0b6c9","value":" 625/625 [00:25&lt;00:00, 25.49it/s]"}},"3d7eec73e3894e12b30929ebf8c611cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4bd2b2935f64adeb92b97e462e6c4f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79db8802ad6243ea8a87fffbedd70cb2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d83690857e3e4013abc16657b969a65f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"705e873e7f004a728b019e9685ebb353":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c44d89c6230447a391a59a85b4e7415d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb08e7919db644c1ba80d32123c0b6c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6e92bc339c149fcad3286b21f6229b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07262706c9a346cfa93d766868ba688d","IPY_MODEL_7fa350023e504f79b1408ab422255bb4","IPY_MODEL_6ea1dd64b6704a66a3dd7aad9dd3aa6e"],"layout":"IPY_MODEL_16f2dfd48eff400bb3940c80cf4a38e3"}},"07262706c9a346cfa93d766868ba688d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba40ea04f49147c0864eea2d149c66ee","placeholder":"​","style":"IPY_MODEL_74d97f18b2964b4386e598f3b44d723a","value":"100%"}},"7fa350023e504f79b1408ab422255bb4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a021aeec005446d2ac0244ee85810af4","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3b87fe75e744cb99dfa4e120c4a5113","value":625}},"6ea1dd64b6704a66a3dd7aad9dd3aa6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f4ccfe17528435b8a7c3e45ed2b7973","placeholder":"​","style":"IPY_MODEL_d1965cd81e874620ac13b0519d68808f","value":" 625/625 [00:25&lt;00:00, 24.49it/s]"}},"16f2dfd48eff400bb3940c80cf4a38e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba40ea04f49147c0864eea2d149c66ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74d97f18b2964b4386e598f3b44d723a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a021aeec005446d2ac0244ee85810af4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3b87fe75e744cb99dfa4e120c4a5113":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f4ccfe17528435b8a7c3e45ed2b7973":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1965cd81e874620ac13b0519d68808f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfb9aa40c89f4c36a48c0947a4702df4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_337660f9157a4707a9400d802272953c","IPY_MODEL_2d27140ffd8a45558cafae5d59db14b0","IPY_MODEL_401a771565cb44cc823a466e4d5bb17a"],"layout":"IPY_MODEL_88113ab3ae1c465aad47cc85a591f5e5"}},"337660f9157a4707a9400d802272953c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a78255fba384295abfbc9003c544c67","placeholder":"​","style":"IPY_MODEL_ad66cb8a13124405a85c66c45ec78ff1","value":"100%"}},"2d27140ffd8a45558cafae5d59db14b0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b67113f89b147239849a6a4302ed0c2","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f371a117a094c8da38cf2e5bd103d2a","value":625}},"401a771565cb44cc823a466e4d5bb17a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5aba3bc6ed14fa4ae15cba9a96886a3","placeholder":"​","style":"IPY_MODEL_1d76bbc0618b424c8c1bb6e19df4a421","value":" 625/625 [00:25&lt;00:00, 25.35it/s]"}},"88113ab3ae1c465aad47cc85a591f5e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a78255fba384295abfbc9003c544c67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad66cb8a13124405a85c66c45ec78ff1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b67113f89b147239849a6a4302ed0c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f371a117a094c8da38cf2e5bd103d2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5aba3bc6ed14fa4ae15cba9a96886a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d76bbc0618b424c8c1bb6e19df4a421":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10354edf1b3848e9b8dcf9847942d286":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e60969b4871f4361a9f4f3e85ce66943","IPY_MODEL_c270afcd68d14e3cba5cdae6fe8d93ca","IPY_MODEL_745eb4c1db3247c8b2aa5e9c1c2e0a49"],"layout":"IPY_MODEL_8dcbb02c04884615b02349e77963cd70"}},"e60969b4871f4361a9f4f3e85ce66943":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_307d4e5fc7874ff59f426cea7c3afada","placeholder":"​","style":"IPY_MODEL_881977ad917b45ee97246616a462f7bf","value":"100%"}},"c270afcd68d14e3cba5cdae6fe8d93ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_da89f56e4834484fa15c8b9354cd6357","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_51a34b0bbc8e4e6b8a379b54377e030c","value":625}},"745eb4c1db3247c8b2aa5e9c1c2e0a49":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf4cc6cc1fe44bc589c1556dc53e7591","placeholder":"​","style":"IPY_MODEL_4eef3ef6e9de48a1877e8abd165208ed","value":" 625/625 [00:25&lt;00:00, 25.19it/s]"}},"8dcbb02c04884615b02349e77963cd70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"307d4e5fc7874ff59f426cea7c3afada":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"881977ad917b45ee97246616a462f7bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da89f56e4834484fa15c8b9354cd6357":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51a34b0bbc8e4e6b8a379b54377e030c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf4cc6cc1fe44bc589c1556dc53e7591":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4eef3ef6e9de48a1877e8abd165208ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f027b20d5e748acb738bae3ef0c592e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a58cb5e49f314da7bf01d0b5bc92315a","IPY_MODEL_32d46d611ee447cea20a86e020d83cc5","IPY_MODEL_ec603ed6366d4c228c067ec9e44f346f"],"layout":"IPY_MODEL_cdd9abbd3bfa40089f0062931b94d789"}},"a58cb5e49f314da7bf01d0b5bc92315a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c6bad8ea50c4d13b8fd29dee3875789","placeholder":"​","style":"IPY_MODEL_63daedc83554419a9b58c641791cda1f","value":"100%"}},"32d46d611ee447cea20a86e020d83cc5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0f3d343231c433786b6e4a045163a2a","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e95668fd87c24139be7310586af06c9a","value":625}},"ec603ed6366d4c228c067ec9e44f346f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46a6ea3e9a9b480eaa6bddf1c4143660","placeholder":"​","style":"IPY_MODEL_aa2decf242c54520a0162ca429b53154","value":" 625/625 [00:25&lt;00:00, 25.43it/s]"}},"cdd9abbd3bfa40089f0062931b94d789":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c6bad8ea50c4d13b8fd29dee3875789":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63daedc83554419a9b58c641791cda1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0f3d343231c433786b6e4a045163a2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e95668fd87c24139be7310586af06c9a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"46a6ea3e9a9b480eaa6bddf1c4143660":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa2decf242c54520a0162ca429b53154":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e46fbab001874c10b5f9f56c664876e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_70a3aaa1c0f34b79b2fb42cfafb58868","IPY_MODEL_9f83ae67fcf344018e041a2910144d4d","IPY_MODEL_feeaa2dd200c40f7b6fb8d76e1beb8bc"],"layout":"IPY_MODEL_fe8762005099450bb78c2896203e60b1"}},"70a3aaa1c0f34b79b2fb42cfafb58868":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40a7df3fd4ac4867bb7888315ddcbc36","placeholder":"​","style":"IPY_MODEL_ca9b952e0adb488096852ddbc41d90d1","value":"100%"}},"9f83ae67fcf344018e041a2910144d4d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d4f54e8e5ec48afae56afd8a7455130","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b545457a86a44565af05c1d24062eed6","value":625}},"feeaa2dd200c40f7b6fb8d76e1beb8bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ded3619e872440a8cb061a35690abc4","placeholder":"​","style":"IPY_MODEL_d3ac5fe7f13e49c690cc9f5b32ac16ae","value":" 625/625 [00:25&lt;00:00, 25.58it/s]"}},"fe8762005099450bb78c2896203e60b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40a7df3fd4ac4867bb7888315ddcbc36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca9b952e0adb488096852ddbc41d90d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d4f54e8e5ec48afae56afd8a7455130":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b545457a86a44565af05c1d24062eed6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7ded3619e872440a8cb061a35690abc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3ac5fe7f13e49c690cc9f5b32ac16ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57974af5bb29412f9135a86c64c1530e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_56a579a960e54053b8a4e6561f3e9962","IPY_MODEL_1ecaaf9c574c49cba36bc0a852e180d9","IPY_MODEL_22ea6aa1c98c4cf5b1dc1796378f3987"],"layout":"IPY_MODEL_748746350c914591a56d6a805da9d445"}},"56a579a960e54053b8a4e6561f3e9962":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5468bdad0714ce19edf6aa548c67e4b","placeholder":"​","style":"IPY_MODEL_238e80ba21144faea0d9c40ca71f2df3","value":"100%"}},"1ecaaf9c574c49cba36bc0a852e180d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2daf1d9b6f924dbfb3c42179343f13d5","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_185d435cdcfc48d5b62746c561dd1c3a","value":625}},"22ea6aa1c98c4cf5b1dc1796378f3987":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c20280796c74fa1aef386d044055f34","placeholder":"​","style":"IPY_MODEL_592eca8428e54db2bf388c1573d2401b","value":" 625/625 [00:25&lt;00:00, 25.47it/s]"}},"748746350c914591a56d6a805da9d445":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5468bdad0714ce19edf6aa548c67e4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"238e80ba21144faea0d9c40ca71f2df3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2daf1d9b6f924dbfb3c42179343f13d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"185d435cdcfc48d5b62746c561dd1c3a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1c20280796c74fa1aef386d044055f34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"592eca8428e54db2bf388c1573d2401b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a5e842084ea44a09c01103248b3910a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89ce7a6968744b44ac377e14399ca5e1","IPY_MODEL_ab369329814f4509b24c6d42783955d9","IPY_MODEL_f1bba8eff638447ba79ccddb848feec4"],"layout":"IPY_MODEL_8540b9797e474a74ae1b7c9c68c23916"}},"89ce7a6968744b44ac377e14399ca5e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02aa66ef598a4cc1bac1df4ef5e133d0","placeholder":"​","style":"IPY_MODEL_1fe6d3cd5eac44c0a340bca9bf83770a","value":"100%"}},"ab369329814f4509b24c6d42783955d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8976513619494847a915a4a1d6a8fcb0","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_328e0be144544d079f4e7138b139a5b1","value":625}},"f1bba8eff638447ba79ccddb848feec4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81b99b9b92ed47b287746ec6babac7a7","placeholder":"​","style":"IPY_MODEL_6297e10ec7c94e5494eee1490cac1112","value":" 625/625 [00:25&lt;00:00, 25.29it/s]"}},"8540b9797e474a74ae1b7c9c68c23916":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02aa66ef598a4cc1bac1df4ef5e133d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fe6d3cd5eac44c0a340bca9bf83770a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8976513619494847a915a4a1d6a8fcb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"328e0be144544d079f4e7138b139a5b1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"81b99b9b92ed47b287746ec6babac7a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6297e10ec7c94e5494eee1490cac1112":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f71662c4e98461d89fdbd9c743c35fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_61ed9679c9d2427fb0b6bdfd7af82542","IPY_MODEL_eb9803973f624aab820bb4e13a4cbbc5","IPY_MODEL_3633b82cc3364fc2b71d5a8073f25308"],"layout":"IPY_MODEL_b35294fb1e1c4ccaa9b6f3ea7b0e4bc0"}},"61ed9679c9d2427fb0b6bdfd7af82542":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1811a2851be54e0284526905f5af5a92","placeholder":"​","style":"IPY_MODEL_cb686bee389544b988a3c5e64bab2d0a","value":"100%"}},"eb9803973f624aab820bb4e13a4cbbc5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf85d62916ed4ae7ad83f3ff284fc42a","max":5000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ebbccd314b3b4a2da42bc716747dc7a0","value":5000}},"3633b82cc3364fc2b71d5a8073f25308":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d4cbc352da04b058544ea8e0baf6bfb","placeholder":"​","style":"IPY_MODEL_59bbe3036ffb423886c61a396202bf35","value":" 5000/5000 [00:17&lt;00:00, 255.10it/s]"}},"b35294fb1e1c4ccaa9b6f3ea7b0e4bc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1811a2851be54e0284526905f5af5a92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb686bee389544b988a3c5e64bab2d0a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf85d62916ed4ae7ad83f3ff284fc42a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebbccd314b3b4a2da42bc716747dc7a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9d4cbc352da04b058544ea8e0baf6bfb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59bbe3036ffb423886c61a396202bf35":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}